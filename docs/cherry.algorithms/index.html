<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Seb Arnold">
        <link rel="canonical" href="http://cherry-rl.net/docs/cherry.algorithms/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>cherry.algorithms - cherry</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-68693545-3', 'seba-1511.github.com');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">cherry</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../..">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Tutorials <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../tutorials/getting_started/">Getting Started with Cherry</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Documentation <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../cherry/">cherry</a>
</li>
                                    
<li class="active">
    <a href="./">cherry.algorithms</a>
</li>
                                    
<li >
    <a href="../cherry.debug/">cherry.debug</a>
</li>
                                    
<li >
    <a href="../cherry.distributions/">cherry.distributions</a>
</li>
                                    
<li >
    <a href="../cherry.envs/">cherry.envs</a>
</li>
                                    
<li >
    <a href="../cherry.models/">cherry.models</a>
</li>
                                    
<li >
    <a href="../cherry.nn/">cherry.nn</a>
</li>
                                    
<li >
    <a href="../cherry.nn.init/">cherry.nn.init</a>
</li>
                                    
<li >
    <a href="../cherry.optim/">cherry.optim</a>
</li>
                                    
<li >
    <a href="../cherry.pg/">cherry.pg</a>
</li>
                                    
<li >
    <a href="../cherry.plot/">cherry.plot</a>
</li>
                                    
<li >
    <a href="../cherry.td/">cherry.td</a>
</li>
                                </ul>
                            </li>
                            <li >
                                <a href="https://github.com/seba-1511/cherry/tree/master/examples">Examples</a>
                            </li>
                            <li >
                                <a href="https://github.com/seba-1511/cherry/">GitHub</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../cherry/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../cherry.debug/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#cherryalgorithmsa2c">cherry.algorithms.a2c</a></li>
            <li><a href="#policy_loss">policy_loss</a></li>
            <li><a href="#state_value_loss">state_value_loss</a></li>
        <li class="main "><a href="#cherryalgorithmsppo">cherry.algorithms.ppo</a></li>
            <li><a href="#policy_loss_1">policy_loss</a></li>
            <li><a href="#state_value_loss_1">state_value_loss</a></li>
        <li class="main "><a href="#cherryalgorithmstrpo">cherry.algorithms.trpo</a></li>
            <li><a href="#policy_loss_2">policy_loss</a></li>
            <li><a href="#hessian_vector_product">hessian_vector_product</a></li>
            <li><a href="#conjugate_gradient">conjugate_gradient</a></li>
        <li class="main "><a href="#cherryalgorithmssac">cherry.algorithms.sac</a></li>
            <li><a href="#policy_loss_3">policy_loss</a></li>
            <li><a href="#action_value_loss">action_value_loss</a></li>
            <li><a href="#state_value_loss_2">state_value_loss</a></li>
            <li><a href="#entropy_weight_loss">entropy_weight_loss</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="cherryalgorithmsa2c">cherry.algorithms.a2c</h1>
<p><strong>Description</strong></p>
<p>Helper functions for implementing A2C.</p>
<p>A2C simply computes the gradient of the policy as follows:</p>
<p>
<script type="math/tex; mode=display">
\mathbb{E} \left[ (Q(s, a) - V(s)) \cdot \nabla_\theta \log \pi_\theta (a \vert s) \right].
</script>
</p>
<h2 id="policy_loss">policy_loss</h2>
<pre><code class="python">policy_loss(log_probs, advantages)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/a2c.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The policy loss of the Advantage Actor-Critic.</p>
<p>This function simply performs an element-wise multiplication and a mean reduction.</p>
<p><strong>References</strong></p>
<ol>
<li>Mnih et al. 2016. “Asynchronous Methods for Deep Reinforcement Learning.” arXiv [cs.LG].</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>log_probs</strong> (tensor) - Log-density of the selected actions.</li>
<li><strong>advantages</strong> (tensor) - Advantage of the action-state pairs.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The policy loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">advantages = replay.advantage()
log_probs = replay.log_prob()
loss = a2c.policy_loss(log_probs, advantages)
</code></pre>

<h2 id="state_value_loss">state_value_loss</h2>
<pre><code class="python">state_value_loss(values, rewards)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/a2c.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The state-value loss of the Advantage Actor-Critic.</p>
<p>This function is equivalent to a MSELoss.</p>
<p><strong>References</strong></p>
<ol>
<li>A3C paper</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>values</strong> (tensor) - Predicted values for some states.</li>
<li><strong>rewards</strong> (tensor) - Observed rewards for those states.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The value loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">values = replay.value()
rewards = replay.reward()
loss = a2c.state_value_loss(values, rewards)
</code></pre>

<h1 id="cherryalgorithmsppo">cherry.algorithms.ppo</h1>
<p><strong>Description</strong></p>
<p>Helper functions for implementing PPO.</p>
<h2 id="policy_loss_1">policy_loss</h2>
<pre><code class="python">policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/ppo.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The clipped policy loss of Proximal Policy Optimization.</p>
<p><strong>References</strong></p>
<ol>
<li>Schulman et al. 2017. “Proximal Policy Optimization Algorithms.” arXiv [cs.LG].</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>new_log_probs</strong> (tensor) - The log-density of actions from the target policy.</li>
<li><strong>old_log_probs</strong> (tensor) - The log-density of actions from the behaviour policy.</li>
<li><strong>advantages</strong> (tensor) - Advantage of the actions.</li>
<li><strong>clip</strong> (float, <em>optional</em>, default=0.1) - The clipping coefficient.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The clipped policy loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">advantage = ch.pg.generalized_advantage(GAMMA,
                                        TAU,
                                        replay.reward(),
                                        replay.done(),
                                        replay.value(),
                                        next_state_value)
new_densities = policy(replay.state())
new_logprobs = new_densities.log_prob(replay.action())
loss = policy_loss(new_logprobs,
                   replay.logprob().detach(),
                   advantage.detach(),
                   clip=0.2)
</code></pre>

<h2 id="state_value_loss_1">state_value_loss</h2>
<pre><code class="python">state_value_loss(new_values, old_values, rewards, clip=0.1)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/ppo.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The clipped state-value loss of Proximal Policy Optimization.</p>
<p><strong>References</strong></p>
<ol>
<li>PPO paper</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>new_values</strong> (tensor) - State values from the optimized value function.</li>
<li><strong>old_values</strong> (tensor) - State values from the reference value function.</li>
<li><strong>rewards</strong> (tensor) -  Observed rewards.</li>
<li><strong>clip</strong> (float, <em>optional</em>, default=0.1) - The clipping coefficient.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The clipped value loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">values = v_function(batch.state())
value_loss = ppo.state_value_loss(values,
                                  batch.value().detach(),
                                  batch.reward(),
                                  clip=0.2)
</code></pre>

<h1 id="cherryalgorithmstrpo">cherry.algorithms.trpo</h1>
<p><strong>Description</strong></p>
<p>Helper functions for implementing Trust-Region Policy Optimization.</p>
<p>Recall that TRPO strives to solve the following objective:</p>
<p>
<script type="math/tex; mode=display">
\max_\theta \quad \mathbb{E}\left[ \frac{\pi_\theta}{\pi_\text{old}} \cdot A  \right] \\
\text{subject to} \quad \mathbb{E}\left[ \text{KL}(\pi_\text{old} \vert \vert \pi_\theta) \leq \delta \right].
</script>
</p>
<h2 id="policy_loss_2">policy_loss</h2>
<pre><code class="python">policy_loss(new_log_probs, old_log_probs, advantages)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/a2c.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The policy loss for Trust-Region Policy Optimization.</p>
<p>This is also known as the surrogate loss.</p>
<p><strong>References</strong></p>
<ol>
<li>Schulman et al. 2015. “Trust Region Policy Optimization.” ICML 2015.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>new_log_probs</strong> (tensor) - The log-density of actions from the target policy.</li>
<li><strong>old_log_probs</strong> (tensor) - The log-density of actions from the behaviour policy.</li>
<li><strong>advantages</strong> (tensor) - Advantage of the actions.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The policy loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">advantage = ch.pg.generalized_advantage(GAMMA,
                                        TAU,
                                        replay.reward(),
                                        replay.done(),
                                        replay.value(),
                                        next_state_value)
new_densities = policy(replay.state())
new_logprobs = new_densities.log_prob(replay.action())
loss = policy_loss(new_logprobs,
                   replay.logprob().detach(),
                   advantage.detach())
</code></pre>

<h2 id="hessian_vector_product">hessian_vector_product</h2>
<pre><code class="python">hessian_vector_product(loss, parameters, damping=1e-05)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/trpo.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>Returns a callable that computes the product of the Hessian of loss
(w.r.t. parameters) with another vector, using Pearlmutter's trick.</p>
<p>Note that parameters and the argument of the callable can be tensors
or list of tensors.</p>
<p>TODO: The reshaping (if arguments are lists) is not efficiently implemented.
      (It requires a copy) A good idea would be to have
      vector_to_shapes(vec, shapes) or similar.</p>
<p>NOTE: We can not compute the grads with a vector version of the parameters,
      since that vector (created within the function) will be a different
      tree that is was not used in the computation of the loss.
      (i.e. you get 'One of the differentiated tensors was not used.')</p>
<p><strong>References</strong></p>
<ol>
<li>Pearlmutter, B. A. 1994. “Fast Exact Multiplication by the Hessian.” Neural Computation.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>loss</strong> (tensor) - The loss of which to compute the Hessian.</li>
<li><strong>parameters</strong> (tensor or list) - The tensors to take the gradient with respect to.</li>
<li><strong>damping</strong> (float, <em>optional</em>, default=1e-5) - Damping of the Hessian-vector product.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><strong>hvp(other)</strong> (callable) - A function to compute the Hessian-vector product,
    given a vector or list <code>other</code>.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">pass
</code></pre>

<h2 id="conjugate_gradient">conjugate_gradient</h2>
<pre><code class="python">conjugate_gradient(Ax, b, num_iterations=10, tol=1e-10, eps=1e-08)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/trpo.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>Computes x = A^{-1}b using the conjugate gradient algorithm.</p>
<p><strong>References</strong></p>
<ol>
<li>Nocedal and Wright. 2006. Numerical Optimization, 2nd edition. Springer.</li>
<li><a href="https://github.com/Kaixhin/spinning-up-basic/blob/master/trpo.py">https://github.com/Kaixhin/spinning-up-basic/blob/master/trpo.py</a></li>
<li><a href="https://github.com/tristandeleu/pytorch-maml-rl">https://github.com/tristandeleu/pytorch-maml-rl</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>Ax</strong> (callable) - Given a vector x, computes A@x.</li>
<li><strong>b</strong> (tensor or list) - The reference vector.</li>
<li><strong>num_iterations</strong> (int, <em>optional</em>, default=10) - Number of conjugate gradient iterations.</li>
<li><strong>tol</strong> (float, <em>optional</em>, default=1e-10) - Tolerance for proposed solution.</li>
<li><strong>eps</strong> (float, <em>optional</em>, default=1e-8) - Numerical stability constant.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><strong>x</strong> (tensor or list) - The solution to Ax = b, as a list if b is a list else a tensor.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">pass
</code></pre>

<h1 id="cherryalgorithmssac">cherry.algorithms.sac</h1>
<p><strong>Description</strong></p>
<p>Helper functions for implementing Soft-Actor Critic.</p>
<p>You should update the function approximators according to the following order.</p>
<ol>
<li>Entropy weight update.</li>
<li>Action-value update.</li>
<li>State-value update. (Optional, c.f. below)</li>
<li>Policy update.</li>
</ol>
<p>Note that most recent implementations of SAC omit step 3. above by using
the Bellman residual instead of modelling a state-value function.
For an example of such implementation refer to
<a href="https://github.com/seba-1511/cherry/blob/master/examples/pybullet/delayed_tsac_pybullet.py">this link</a>.</p>
<h2 id="policy_loss_3">policy_loss</h2>
<pre><code class="python">policy_loss(log_probs, q_curr, alpha=1.0)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/sac.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The policy loss of the Soft Actor-Critic.</p>
<p>New actions are sampled from the target policy, and those are used to compute the Q-values.
While we should back-propagate through the Q-values to the policy parameters, we shouldn't
use that gradient to optimize the Q parameters.
This is often avoided by either using a target Q function, or by zero-ing out the gradients
of the Q function parameters.</p>
<p><strong>References</strong></p>
<ol>
<li>Haarnoja et al. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” arXiv [cs.LG].</li>
<li>Haarnoja et al. 2018. “Soft Actor-Critic Algorithms and Applications.” arXiv [cs.LG].</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>log_probs</strong> (tensor) - Log-density of the selected actions.</li>
<li><strong>q_curr</strong> (tensor) - Q-values of state-action pairs.</li>
<li><strong>alpha</strong> (float, <em>optional</em>, default=1.0) - Entropy weight.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The policy loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">densities = policy(batch.state())
actions = densities.sample()
log_probs = densities.log_prob(actions)
q_curr = q_function(batch.state(), actions)
loss = policy_loss(log_probs, q_curr, alpha=0.1)
</code></pre>

<h2 id="action_value_loss">action_value_loss</h2>
<pre><code class="python">action_value_loss(value, next_value, rewards, dones, gamma)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/sac.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The action-value loss of the Soft Actor-Critic.</p>
<p><code>value</code> should be the value of the current state-action pair, estimated via the Q-function.
<code>next_value</code> is the expected value of the next state; it can be estimated via a V-function,
or alternatively by computing the Q-value of the next observed state-action pair.
In the latter case, make sure that the action is sampled according to the current policy,
not the one used to gather the data.</p>
<p><strong>References</strong></p>
<ol>
<li>Haarnoja et al. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” arXiv [cs.LG].</li>
<li>Haarnoja et al. 2018. “Soft Actor-Critic Algorithms and Applications.” arXiv [cs.LG].</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>value</strong> (tensor) - Action values of the actual transition.</li>
<li><strong>next_value</strong> (tensor) - State values of the resulting state.</li>
<li><strong>rewards</strong> (tensor) - Observed rewards of the transition.</li>
<li><strong>dones</strong> (tensor) - Which states were terminal.</li>
<li><strong>gamma</strong> (float) - Discount factor.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The policy loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">value = qf(batch.state(), batch.action().detach())
next_value = targe_vf(batch.next_state())
loss = action_value_loss(value,
                         next_value,
                         batch.reward(),
                         batch.done(),
                         gamma=0.99)
</code></pre>

<h2 id="state_value_loss_2">state_value_loss</h2>
<pre><code class="python">state_value_loss(v_value, log_probs, q_value, alpha=1.0)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/sac.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The state-value loss of the Soft Actor-Critic.</p>
<p>This update is computed "on-policy": states are sampled from a replay but the state values,
action values, and log-densities are computed using the current value functions and policy.</p>
<p><strong>References</strong></p>
<ol>
<li>Haarnoja et al. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” arXiv [cs.LG].</li>
<li>Haarnoja et al. 2018. “Soft Actor-Critic Algorithms and Applications.” arXiv [cs.LG].</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>v_value</strong> (tensor) - State values for some observed states.</li>
<li><strong>log_probs</strong> (tensor) - Log-density of actions sampled from the current policy.</li>
<li><strong>q_value</strong> (tensor) - Action values of the actions for the current policy.</li>
<li><strong>alpha</strong> (float, <em>optional</em>, default=1.0) - Entropy weight.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The state value loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">densities = policy(batch.state())
actions = densities.sample()
log_probs = densities.log_prob(actions)
q_value = qf(batch.state(), actions)
v_value = vf(batch.state())
loss = state_value_loss(v_value,
                        log_probs,
                        q_value,
                        alpha=0.1)
</code></pre>

<h2 id="entropy_weight_loss">entropy_weight_loss</h2>
<pre><code class="python">entropy_weight_loss(log_alpha, log_probs, target_entropy)
</code></pre>

<p><a href="https://github.com/seba-1511/cherry/blob/master/cherry/algorithms/sac.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>Loss of the entropy weight, to automatically tune it.</p>
<p>The target entropy needs to be manually tuned.
However, a popular heuristic for TanhNormal policies is to use the negative of the action-space
dimensionality. (e.g. -4 when operating the voltage of a quad-rotor.)</p>
<p><strong>References</strong></p>
<ol>
<li>Haarnoja et al. 2018. “Soft Actor-Critic Algorithms and Applications.” arXiv [cs.LG].</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>log_alpha</strong> (tensor) - Log of the entropy weight.</li>
<li><strong>log_probs</strong> (tensor) - Log-density of policy actions.</li>
<li><strong>target_entropy</strong> (float) - Target of the entropy value.</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>(tensor) - The state value loss for the given arguments.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">densities = policy(batch.state())
actions = densities.sample()
log_probs = densities.log_prob(actions)
target_entropy = -np.prod(env.action_space.shape).item()
loss = entropy_weight_loss(alpha.log(),
                           log_probs,
                           target_entropy)
</code></pre></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js" defer></script>
        <script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/mathtex-script-type.min.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
