{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cherry is a reinforcement learning framework for researchers built on top of PyTorch. Unlike other reinforcement learning implementations, cherry doesn't implement a single monolithic interface to existing algorithms. Instead, it provides you with low-level, common tools to write your own algorithms. Drawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible. So if you don't like a specific tool, you don\u2019t need to use it. Features Pythonic and low-level interface \u00e0 la Pytorch. Support for tabular (!) and function approximation algorithms. Various OpenAI Gym environment wrappers. Helper functions for popular algorithms. (e.g. A2C, DDPG, TRPO, PPO, SAC) Logging, visualization, and debugging tools. Painless and efficient distributed training on CPUs and GPUs. Unit, integration, and regression tested, continuously integrated. To learn more about the tools and philosophy behind cherry, check out our Getting Started tutorial . Example The following snippet showcases some of the tools offered by cherry. import cherry as ch # Wrap environments env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) policy = PolicyNet() optimizer = optim.Adam(policy.parameters(), lr=1e-2) replay = ch.ExperienceReplay() # Manage transitions for step in range(1000): state = env.reset() while True: mass = Categorical(policy(state)) action = mass.sample() log_prob = mass.log_prob(action) next_state, reward, done, _ = env.step(action) # Build the ExperienceReplay replay.append(state, action, reward, next_state, done, log_prob=log_prob) if done: break else: state = next_state # Discounting and normalizing rewards rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() replay.empty() Many more high-quality examples are available in the examples/ folder. Installation Note Cherry is considered in early alpha release. Stuff might break. pip install cherry-rl Changelog A human-readable changelog is available in the CHANGELOG.md file. Documentation Documentation and tutorials are available on cherry\u2019s website: http://cherry-rl.net . Contributing First, thanks for your consideration in contributing to cherry. Here are a couple of guidelines we strive to follow. It's always a good idea to open an issue first, where we can discuss how to best proceed. If you want to contribute a new example using cherry, it would preferably stand in a single file. If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: it allows for automatic testing, it ensures that the functionality is correctly implemented, it shows users how to use your functionality, and it gives a concrete example when discussing the best way to merge your implementation. We don't have forums, but are happy to discuss with you on slack. Make sure to send an email to smr.arnold@gmail.com to get an invite. Acknowledgements Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , Kai Arulkumaran's implementations , RLLab / Garage . Why 'cherry' ? Because it's the sweetest part of the cake .","title":"Home"},{"location":"#example","text":"The following snippet showcases some of the tools offered by cherry. import cherry as ch # Wrap environments env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) policy = PolicyNet() optimizer = optim.Adam(policy.parameters(), lr=1e-2) replay = ch.ExperienceReplay() # Manage transitions for step in range(1000): state = env.reset() while True: mass = Categorical(policy(state)) action = mass.sample() log_prob = mass.log_prob(action) next_state, reward, done, _ = env.step(action) # Build the ExperienceReplay replay.append(state, action, reward, next_state, done, log_prob=log_prob) if done: break else: state = next_state # Discounting and normalizing rewards rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() replay.empty() Many more high-quality examples are available in the examples/ folder.","title":"Example"},{"location":"#installation","text":"Note Cherry is considered in early alpha release. Stuff might break. pip install cherry-rl","title":"Installation"},{"location":"#changelog","text":"A human-readable changelog is available in the CHANGELOG.md file.","title":"Changelog"},{"location":"#documentation","text":"Documentation and tutorials are available on cherry\u2019s website: http://cherry-rl.net .","title":"Documentation"},{"location":"#contributing","text":"First, thanks for your consideration in contributing to cherry. Here are a couple of guidelines we strive to follow. It's always a good idea to open an issue first, where we can discuss how to best proceed. If you want to contribute a new example using cherry, it would preferably stand in a single file. If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: it allows for automatic testing, it ensures that the functionality is correctly implemented, it shows users how to use your functionality, and it gives a concrete example when discussing the best way to merge your implementation. We don't have forums, but are happy to discuss with you on slack. Make sure to send an email to smr.arnold@gmail.com to get an invite.","title":"Contributing"},{"location":"#acknowledgements","text":"Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , Kai Arulkumaran's implementations , RLLab / Garage .","title":"Acknowledgements"},{"location":"#why-cherry","text":"Because it's the sweetest part of the cake .","title":"Why 'cherry' ?"},{"location":"docs/cherry.algorithms/","text":"cherry.algorithms.a2c Description Helper functions for implementing A2C. A2C simply computes the gradient of the policy as follows: \\mathbb{E} \\left[ (Q(s, a) - V(s)) \\cdot \\nabla_\\theta \\log \\pi_\\theta (a \\vert s) \\right]. policy_loss policy_loss(log_probs, advantages) [Source] Description The policy loss of the Advantage Actor-Critic. This function simply performs an element-wise multiplication and a mean reduction. References Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments log_probs (tensor) - Log-density of the selected actions. advantages (tensor) - Advantage of the action-state pairs. Returns (tensor) - The policy loss for the given arguments. Example advantages = replay.advantage() log_probs = replay.log_prob() loss = a2c.policy_loss(log_probs, advantages) state_value_loss state_value_loss(values, rewards) [Source] Description The state-value loss of the Advantage Actor-Critic. This function is equivalent to a MSELoss. References Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments values (tensor) - Predicted values for some states. rewards (tensor) - Observed rewards for those states. Returns (tensor) - The value loss for the given arguments. Example values = replay.value() rewards = replay.reward() loss = a2c.state_value_loss(values, rewards) cherry.algorithms.ppo Description Helper functions for implementing PPO. policy_loss policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) [Source] Description The clipped policy loss of Proximal Policy Optimization. References Schulman et al. 2017. \u201cProximal Policy Optimization Algorithms.\u201d arXiv [cs.LG]. Arguments new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. clip (float, optional , default=0.1) - The clipping coefficient. Returns (tensor) - The clipped policy loss for the given arguments. Example advantage = ch.pg.generalized_advantage(GAMMA, TAU, replay.reward(), replay.done(), replay.value(), next_state_value) new_densities = policy(replay.state()) new_logprobs = new_densities.log_prob(replay.action()) loss = policy_loss(new_logprobs, replay.logprob().detach(), advantage.detach(), clip=0.2) state_value_loss state_value_loss(new_values, old_values, rewards, clip=0.1) [Source] Description The clipped state-value loss of Proximal Policy Optimization. References PPO paper Arguments new_values (tensor) - State values from the optimized value function. old_values (tensor) - State values from the reference value function. rewards (tensor) - Observed rewards. clip (float, optional , default=0.1) - The clipping coefficient. Returns (tensor) - The clipped value loss for the given arguments. Example values = v_function(batch.state()) value_loss = ppo.state_value_loss(values, batch.value().detach(), batch.reward(), clip=0.2) cherry.algorithms.trpo Description Helper functions for implementing Trust-Region Policy Optimization. Recall that TRPO strives to solve the following objective: \\max_\\theta \\quad \\mathbb{E}\\left[ \\frac{\\pi_\\theta}{\\pi_\\text{old}} \\cdot A \\right] \\\\ \\text{subject to} \\quad D_\\text{KL}(\\pi_\\text{old} \\vert \\vert \\pi_\\theta) \\leq \\delta. policy_loss policy_loss(new_log_probs, old_log_probs, advantages) [Source] Description The policy loss for Trust-Region Policy Optimization. This is also known as the surrogate loss. References Schulman et al. 2015. \u201cTrust Region Policy Optimization.\u201d ICML 2015. Arguments new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. Returns (tensor) - The policy loss for the given arguments. Example advantage = ch.pg.generalized_advantage(GAMMA, TAU, replay.reward(), replay.done(), replay.value(), next_state_value) new_densities = policy(replay.state()) new_logprobs = new_densities.log_prob(replay.action()) loss = policy_loss(new_logprobs, replay.logprob().detach(), advantage.detach()) hessian_vector_product hessian_vector_product(loss, parameters, damping=1e-05) [Source] Description Returns a callable that computes the product of the Hessian of loss (w.r.t. parameters) with another vector, using Pearlmutter's trick. Note that parameters and the argument of the callable can be tensors or list of tensors. References Pearlmutter, B. A. 1994. \u201cFast Exact Multiplication by the Hessian.\u201d Neural Computation. Arguments loss (tensor) - The loss of which to compute the Hessian. parameters (tensor or list) - The tensors to take the gradient with respect to. damping (float, optional , default=1e-5) - Damping of the Hessian-vector product. Returns hvp(other) (callable) - A function to compute the Hessian-vector product, given a vector or list other . Example pass conjugate_gradient conjugate_gradient(Ax, b, num_iterations=10, tol=1e-10, eps=1e-08) [Source] Description Computes x = A^{-1}b using the conjugate gradient algorithm. Credit Adapted from Kai Arulkumaran's implementation, with additions inspired from John Schulman's implementation. References Nocedal and Wright. 2006. \"Numerical Optimization, 2nd edition\". Springer. Shewchuk et al. 1994. \u201cAn Introduction to the Conjugate Gradient Method without the Agonizing Pain.\u201d CMU. Arguments Ax (callable) - Given a vector x, computes A@x. b (tensor or list) - The reference vector. num_iterations (int, optional , default=10) - Number of conjugate gradient iterations. tol (float, optional , default=1e-10) - Tolerance for proposed solution. eps (float, optional , default=1e-8) - Numerical stability constant. Returns x (tensor or list) - The solution to Ax = b, as a list if b is a list else a tensor. Example pass cherry.algorithms.sac Description Helper functions for implementing Soft-Actor Critic. You should update the function approximators according to the following order. Entropy weight update. Action-value update. State-value update. (Optional, c.f. below) Policy update. Note that most recent implementations of SAC omit step 3. above by using the Bellman residual instead of modelling a state-value function. For an example of such implementation refer to this link . policy_loss policy_loss(log_probs, q_curr, alpha=1.0) [Source] Description The policy loss of the Soft Actor-Critic. New actions are sampled from the target policy, and those are used to compute the Q-values. While we should back-propagate through the Q-values to the policy parameters, we shouldn't use that gradient to optimize the Q parameters. This is often avoided by either using a target Q function, or by zero-ing out the gradients of the Q function parameters. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments log_probs (tensor) - Log-density of the selected actions. q_curr (tensor) - Q-values of state-action pairs. alpha (float, optional , default=1.0) - Entropy weight. Returns (tensor) - The policy loss for the given arguments. Example densities = policy(batch.state()) actions = densities.sample() log_probs = densities.log_prob(actions) q_curr = q_function(batch.state(), actions) loss = policy_loss(log_probs, q_curr, alpha=0.1) action_value_loss action_value_loss(value, next_value, rewards, dones, gamma) [Source] Description The action-value loss of the Soft Actor-Critic. value should be the value of the current state-action pair, estimated via the Q-function. next_value is the expected value of the next state; it can be estimated via a V-function, or alternatively by computing the Q-value of the next observed state-action pair. In the latter case, make sure that the action is sampled according to the current policy, not the one used to gather the data. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments value (tensor) - Action values of the actual transition. next_value (tensor) - State values of the resulting state. rewards (tensor) - Observed rewards of the transition. dones (tensor) - Which states were terminal. gamma (float) - Discount factor. Returns (tensor) - The policy loss for the given arguments. Example value = qf(batch.state(), batch.action().detach()) next_value = targe_vf(batch.next_state()) loss = action_value_loss(value, next_value, batch.reward(), batch.done(), gamma=0.99) state_value_loss state_value_loss(v_value, log_probs, q_value, alpha=1.0) [Source] Description The state-value loss of the Soft Actor-Critic. This update is computed \"on-policy\": states are sampled from a replay but the state values, action values, and log-densities are computed using the current value functions and policy. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments v_value (tensor) - State values for some observed states. log_probs (tensor) - Log-density of actions sampled from the current policy. q_value (tensor) - Action values of the actions for the current policy. alpha (float, optional , default=1.0) - Entropy weight. Returns (tensor) - The state value loss for the given arguments. Example densities = policy(batch.state()) actions = densities.sample() log_probs = densities.log_prob(actions) q_value = qf(batch.state(), actions) v_value = vf(batch.state()) loss = state_value_loss(v_value, log_probs, q_value, alpha=0.1) entropy_weight_loss entropy_weight_loss(log_alpha, log_probs, target_entropy) [Source] Description Loss of the entropy weight, to automatically tune it. The target entropy needs to be manually tuned. However, a popular heuristic for TanhNormal policies is to use the negative of the action-space dimensionality. (e.g. -4 when operating the voltage of a quad-rotor.) References Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments log_alpha (tensor) - Log of the entropy weight. log_probs (tensor) - Log-density of policy actions. target_entropy (float) - Target of the entropy value. Returns (tensor) - The state value loss for the given arguments. Example densities = policy(batch.state()) actions = densities.sample() log_probs = densities.log_prob(actions) target_entropy = -np.prod(env.action_space.shape).item() loss = entropy_weight_loss(alpha.log(), log_probs, target_entropy)","title":"cherry.algorithms"},{"location":"docs/cherry.algorithms/#cherryalgorithmsa2c","text":"Description Helper functions for implementing A2C. A2C simply computes the gradient of the policy as follows: \\mathbb{E} \\left[ (Q(s, a) - V(s)) \\cdot \\nabla_\\theta \\log \\pi_\\theta (a \\vert s) \\right].","title":"cherry.algorithms.a2c"},{"location":"docs/cherry.algorithms/#policy_loss","text":"policy_loss(log_probs, advantages) [Source] Description The policy loss of the Advantage Actor-Critic. This function simply performs an element-wise multiplication and a mean reduction. References Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments log_probs (tensor) - Log-density of the selected actions. advantages (tensor) - Advantage of the action-state pairs. Returns (tensor) - The policy loss for the given arguments. Example advantages = replay.advantage() log_probs = replay.log_prob() loss = a2c.policy_loss(log_probs, advantages)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#state_value_loss","text":"state_value_loss(values, rewards) [Source] Description The state-value loss of the Advantage Actor-Critic. This function is equivalent to a MSELoss. References Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments values (tensor) - Predicted values for some states. rewards (tensor) - Observed rewards for those states. Returns (tensor) - The value loss for the given arguments. Example values = replay.value() rewards = replay.reward() loss = a2c.state_value_loss(values, rewards)","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmsppo","text":"Description Helper functions for implementing PPO.","title":"cherry.algorithms.ppo"},{"location":"docs/cherry.algorithms/#policy_loss_1","text":"policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) [Source] Description The clipped policy loss of Proximal Policy Optimization. References Schulman et al. 2017. \u201cProximal Policy Optimization Algorithms.\u201d arXiv [cs.LG]. Arguments new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. clip (float, optional , default=0.1) - The clipping coefficient. Returns (tensor) - The clipped policy loss for the given arguments. Example advantage = ch.pg.generalized_advantage(GAMMA, TAU, replay.reward(), replay.done(), replay.value(), next_state_value) new_densities = policy(replay.state()) new_logprobs = new_densities.log_prob(replay.action()) loss = policy_loss(new_logprobs, replay.logprob().detach(), advantage.detach(), clip=0.2)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#state_value_loss_1","text":"state_value_loss(new_values, old_values, rewards, clip=0.1) [Source] Description The clipped state-value loss of Proximal Policy Optimization. References PPO paper Arguments new_values (tensor) - State values from the optimized value function. old_values (tensor) - State values from the reference value function. rewards (tensor) - Observed rewards. clip (float, optional , default=0.1) - The clipping coefficient. Returns (tensor) - The clipped value loss for the given arguments. Example values = v_function(batch.state()) value_loss = ppo.state_value_loss(values, batch.value().detach(), batch.reward(), clip=0.2)","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmstrpo","text":"Description Helper functions for implementing Trust-Region Policy Optimization. Recall that TRPO strives to solve the following objective: \\max_\\theta \\quad \\mathbb{E}\\left[ \\frac{\\pi_\\theta}{\\pi_\\text{old}} \\cdot A \\right] \\\\ \\text{subject to} \\quad D_\\text{KL}(\\pi_\\text{old} \\vert \\vert \\pi_\\theta) \\leq \\delta.","title":"cherry.algorithms.trpo"},{"location":"docs/cherry.algorithms/#policy_loss_2","text":"policy_loss(new_log_probs, old_log_probs, advantages) [Source] Description The policy loss for Trust-Region Policy Optimization. This is also known as the surrogate loss. References Schulman et al. 2015. \u201cTrust Region Policy Optimization.\u201d ICML 2015. Arguments new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. Returns (tensor) - The policy loss for the given arguments. Example advantage = ch.pg.generalized_advantage(GAMMA, TAU, replay.reward(), replay.done(), replay.value(), next_state_value) new_densities = policy(replay.state()) new_logprobs = new_densities.log_prob(replay.action()) loss = policy_loss(new_logprobs, replay.logprob().detach(), advantage.detach())","title":"policy_loss"},{"location":"docs/cherry.algorithms/#hessian_vector_product","text":"hessian_vector_product(loss, parameters, damping=1e-05) [Source] Description Returns a callable that computes the product of the Hessian of loss (w.r.t. parameters) with another vector, using Pearlmutter's trick. Note that parameters and the argument of the callable can be tensors or list of tensors. References Pearlmutter, B. A. 1994. \u201cFast Exact Multiplication by the Hessian.\u201d Neural Computation. Arguments loss (tensor) - The loss of which to compute the Hessian. parameters (tensor or list) - The tensors to take the gradient with respect to. damping (float, optional , default=1e-5) - Damping of the Hessian-vector product. Returns hvp(other) (callable) - A function to compute the Hessian-vector product, given a vector or list other . Example pass","title":"hessian_vector_product"},{"location":"docs/cherry.algorithms/#conjugate_gradient","text":"conjugate_gradient(Ax, b, num_iterations=10, tol=1e-10, eps=1e-08) [Source] Description Computes x = A^{-1}b using the conjugate gradient algorithm. Credit Adapted from Kai Arulkumaran's implementation, with additions inspired from John Schulman's implementation. References Nocedal and Wright. 2006. \"Numerical Optimization, 2nd edition\". Springer. Shewchuk et al. 1994. \u201cAn Introduction to the Conjugate Gradient Method without the Agonizing Pain.\u201d CMU. Arguments Ax (callable) - Given a vector x, computes A@x. b (tensor or list) - The reference vector. num_iterations (int, optional , default=10) - Number of conjugate gradient iterations. tol (float, optional , default=1e-10) - Tolerance for proposed solution. eps (float, optional , default=1e-8) - Numerical stability constant. Returns x (tensor or list) - The solution to Ax = b, as a list if b is a list else a tensor. Example pass","title":"conjugate_gradient"},{"location":"docs/cherry.algorithms/#cherryalgorithmssac","text":"Description Helper functions for implementing Soft-Actor Critic. You should update the function approximators according to the following order. Entropy weight update. Action-value update. State-value update. (Optional, c.f. below) Policy update. Note that most recent implementations of SAC omit step 3. above by using the Bellman residual instead of modelling a state-value function. For an example of such implementation refer to this link .","title":"cherry.algorithms.sac"},{"location":"docs/cherry.algorithms/#policy_loss_3","text":"policy_loss(log_probs, q_curr, alpha=1.0) [Source] Description The policy loss of the Soft Actor-Critic. New actions are sampled from the target policy, and those are used to compute the Q-values. While we should back-propagate through the Q-values to the policy parameters, we shouldn't use that gradient to optimize the Q parameters. This is often avoided by either using a target Q function, or by zero-ing out the gradients of the Q function parameters. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments log_probs (tensor) - Log-density of the selected actions. q_curr (tensor) - Q-values of state-action pairs. alpha (float, optional , default=1.0) - Entropy weight. Returns (tensor) - The policy loss for the given arguments. Example densities = policy(batch.state()) actions = densities.sample() log_probs = densities.log_prob(actions) q_curr = q_function(batch.state(), actions) loss = policy_loss(log_probs, q_curr, alpha=0.1)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#action_value_loss","text":"action_value_loss(value, next_value, rewards, dones, gamma) [Source] Description The action-value loss of the Soft Actor-Critic. value should be the value of the current state-action pair, estimated via the Q-function. next_value is the expected value of the next state; it can be estimated via a V-function, or alternatively by computing the Q-value of the next observed state-action pair. In the latter case, make sure that the action is sampled according to the current policy, not the one used to gather the data. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments value (tensor) - Action values of the actual transition. next_value (tensor) - State values of the resulting state. rewards (tensor) - Observed rewards of the transition. dones (tensor) - Which states were terminal. gamma (float) - Discount factor. Returns (tensor) - The policy loss for the given arguments. Example value = qf(batch.state(), batch.action().detach()) next_value = targe_vf(batch.next_state()) loss = action_value_loss(value, next_value, batch.reward(), batch.done(), gamma=0.99)","title":"action_value_loss"},{"location":"docs/cherry.algorithms/#state_value_loss_2","text":"state_value_loss(v_value, log_probs, q_value, alpha=1.0) [Source] Description The state-value loss of the Soft Actor-Critic. This update is computed \"on-policy\": states are sampled from a replay but the state values, action values, and log-densities are computed using the current value functions and policy. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments v_value (tensor) - State values for some observed states. log_probs (tensor) - Log-density of actions sampled from the current policy. q_value (tensor) - Action values of the actions for the current policy. alpha (float, optional , default=1.0) - Entropy weight. Returns (tensor) - The state value loss for the given arguments. Example densities = policy(batch.state()) actions = densities.sample() log_probs = densities.log_prob(actions) q_value = qf(batch.state(), actions) v_value = vf(batch.state()) loss = state_value_loss(v_value, log_probs, q_value, alpha=0.1)","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#entropy_weight_loss","text":"entropy_weight_loss(log_alpha, log_probs, target_entropy) [Source] Description Loss of the entropy weight, to automatically tune it. The target entropy needs to be manually tuned. However, a popular heuristic for TanhNormal policies is to use the negative of the action-space dimensionality. (e.g. -4 when operating the voltage of a quad-rotor.) References Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments log_alpha (tensor) - Log of the entropy weight. log_probs (tensor) - Log-density of policy actions. target_entropy (float) - Target of the entropy value. Returns (tensor) - The state value loss for the given arguments. Example densities = policy(batch.state()) actions = densities.sample() log_probs = densities.log_prob(actions) target_entropy = -np.prod(env.action_space.shape).item() loss = entropy_weight_loss(alpha.log(), log_probs, target_entropy)","title":"entropy_weight_loss"},{"location":"docs/cherry.debug/","text":"cherry.debug General debugging utilities. debug debug(log_dir='./') Enables some debugging utilities for logging and pdb. Includes: Automatically dropping into a post-mortem pdb debugger session whenever an exception is raised. Enables fast DEBUG logging to a logging file via QueueHandler. Copies all stdout output to the logging file. (Experimental) References Automatically start the debugger on an exception (Python recipe), Thomas Heller, 2001, Link Dealing with handlers that block, Python Documentation, 2019. Link Arguments log_dir (str, optional , Default: './') - Location to store the log files. Example ch.debug.debug() raise Exception('My exception') -> raise('My exception') (Pdb)","title":"cherry.debug"},{"location":"docs/cherry.debug/#cherrydebug","text":"General debugging utilities.","title":"cherry.debug"},{"location":"docs/cherry.debug/#debug","text":"debug(log_dir='./') Enables some debugging utilities for logging and pdb. Includes: Automatically dropping into a post-mortem pdb debugger session whenever an exception is raised. Enables fast DEBUG logging to a logging file via QueueHandler. Copies all stdout output to the logging file. (Experimental) References Automatically start the debugger on an exception (Python recipe), Thomas Heller, 2001, Link Dealing with handlers that block, Python Documentation, 2019. Link Arguments log_dir (str, optional , Default: './') - Location to store the log files. Example ch.debug.debug() raise Exception('My exception') -> raise('My exception') (Pdb)","title":"debug"},{"location":"docs/cherry.distributions/","text":"cherry.distributions Description A set of common distributions. Reparameterization Reparameterization(density) [Source] Description Unifies interface for distributions that support rsample and those that do not. When calling sample() , this class checks whether density has a rsample() member, and defaults to call sample() if it does not. References Kingma and Welling. 2013. \u201cAuto-Encoding Variational Bayes.\u201d arXiv [stat.ML]. Arguments density (Distribution) - The distribution to wrap. Example density = Normal(mean, std) reparam = Reparameterization(density) sample = reparam.sample() # Uses Normal.rsample() ActionDistribution ActionDistribution(env, logstd=None, use_probs=False, reparam=False) [Source] Description A helper module to automatically choose the proper policy distribution, based on the Gym environment action_space . For Discrete action spaces, it uses a Categorical distribution, otherwise it uses a Normal which uses a diagonal covariance matrix. This class enables to write single version policy body that will be compatible with a variety of environments. Arguments env (Environment) - Gym environment for which actions will be sampled. logstd (float/tensor, optional , default=0) - The log standard deviation for the Normal distribution. use_probs (bool, optional , default=False) - Whether to use probabilities or logits for the Categorical case. reparam (bool, optional , default=False) - Whether to use reparameterization in the Normal case. Example env = gym.make('CartPole-v1') action_dist = ActionDistribution(env) TanhNormal TanhNormal(normal_mean, normal_std) [Source] Description Implements a Normal distribution followed by a Tanh, often used with the Soft Actor-Critic. This implementation also exposes sample_and_log_prob and rsample_and_log_prob , which returns both samples and log-densities. The log-densities are computed using the pre-activation values for numerical stability. Credit Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments normal_mean (tensor) - Mean of the Normal distribution. normal_std (tensor) - Standard deviation of the Normal distribution. Example mean = th.zeros(5) std = th.ones(5) dist = TanhNormal(mean, std) samples = dist.rsample() logprobs = dist.log_prob(samples) # Numerically unstable :( samples, logprobs = dist.rsample_and_log_prob() # Stable :)","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#cherrydistributions","text":"Description A set of common distributions.","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#reparameterization","text":"Reparameterization(density) [Source] Description Unifies interface for distributions that support rsample and those that do not. When calling sample() , this class checks whether density has a rsample() member, and defaults to call sample() if it does not. References Kingma and Welling. 2013. \u201cAuto-Encoding Variational Bayes.\u201d arXiv [stat.ML]. Arguments density (Distribution) - The distribution to wrap. Example density = Normal(mean, std) reparam = Reparameterization(density) sample = reparam.sample() # Uses Normal.rsample()","title":"Reparameterization"},{"location":"docs/cherry.distributions/#actiondistribution","text":"ActionDistribution(env, logstd=None, use_probs=False, reparam=False) [Source] Description A helper module to automatically choose the proper policy distribution, based on the Gym environment action_space . For Discrete action spaces, it uses a Categorical distribution, otherwise it uses a Normal which uses a diagonal covariance matrix. This class enables to write single version policy body that will be compatible with a variety of environments. Arguments env (Environment) - Gym environment for which actions will be sampled. logstd (float/tensor, optional , default=0) - The log standard deviation for the Normal distribution. use_probs (bool, optional , default=False) - Whether to use probabilities or logits for the Categorical case. reparam (bool, optional , default=False) - Whether to use reparameterization in the Normal case. Example env = gym.make('CartPole-v1') action_dist = ActionDistribution(env)","title":"ActionDistribution"},{"location":"docs/cherry.distributions/#tanhnormal","text":"TanhNormal(normal_mean, normal_std) [Source] Description Implements a Normal distribution followed by a Tanh, often used with the Soft Actor-Critic. This implementation also exposes sample_and_log_prob and rsample_and_log_prob , which returns both samples and log-densities. The log-densities are computed using the pre-activation values for numerical stability. Credit Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments normal_mean (tensor) - Mean of the Normal distribution. normal_std (tensor) - Standard deviation of the Normal distribution. Example mean = th.zeros(5) std = th.ones(5) dist = TanhNormal(mean, std) samples = dist.rsample() logprobs = dist.log_prob(samples) # Numerically unstable :( samples, logprobs = dist.rsample_and_log_prob() # Stable :)","title":"TanhNormal"},{"location":"docs/cherry.envs/","text":"cherry.envs.utils Description Helper functions for OpenAI Gym environments. is_discrete is_discrete(space, vectorized=False) Returns whether a space is discrete. Arguments space - The space. vectorized - Whether to return the discreteness for the vectorized environments (True) or just the discreteness of the underlying environment (False). get_space_dimension get_space_dimension(space, vectorized_dims=False) Returns the number of elements of a space sample, when unrolled. Arguments space - The space. vectorized_dims - Whether to return the full dimension for vectorized environments (True) or just the dimension for the underlying environment (False). Wrapper Wrapper(env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger. action_size The number of dimensions of a single action. state_size The (flattened) size of a single state. Runner Runner(env) Runner wrapper. TODO: When is_vectorized and using episodes=n, use the parallel environmnents to sample n episodes, and stack them inside a flat replay. run Runner.run(get_action, steps=None, episodes=None, render=False) Runner wrapper's run method. Logger Logger(env, interval=1000, episode_interval=10, title=None, logger=None) Tracks and prints some common statistics about the environment. Recorder Recorder(env, directory='./videos/', format='gif', suffix=None) [Source] Description Wrapper to record episodes from a rollout. Supports GIF and MP4 encoding. Arguments env (Environment) - Environment to record. directory (str, optional , default='./videos/') - Relative path to where videos will be saved. format (str, optional , default='gif') - Format of the videos. Choose in ['gif', 'mp4'], defaults to gif. If it's text environment, the format will be json. suffix (str, optional , default=None): A unique id used as part of the suffix for the file. By default, uses os.getpid(). Credit Adapted from OpenAI Gym's Monitor wrapper. Example env = gym.make('CartPole-v0') env = envs.Recorder(record_env, './videos/', format='gif') env = envs.Runner(env) env.run(get_action, episodes=3, render=True) close Recorder.close() Flush all monitor data to disk and close any open rending windows VisdomLogger VisdomLogger(env, interval=1000, episode_interval=10, render=True, title=None, logger=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes. Torch Torch(env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action) Normalizer Normalizer(env, states=True, rewards=True, clip_states=10.0, clip_rewards=10.0, gamma=0.99, eps=1e-08) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.Normalizer(env, states=True, rewards=False) StateNormalizer StateNormalizer(env, statistics=None, beta=0.99, eps=1e-08) [Source] Description Normalizes the states with a running average. Arguments env (Environment) - Environment to normalize. statistics (dict, optional , default=None) - Dictionary used to bootstrap the normalizing statistics. beta (float, optional , default=0.99) - Moving average weigth. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from Tristan Deleu's implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.StateNormalizer(env) env2 = gym.make('CartPole-v0') env2 = cherry.envs.StateNormalizer(env2, statistics=env.statistics) RewardNormalizer RewardNormalizer(env, statistics=None, beta=0.99, eps=1e-08) [Source] Description Normalizes the rewards with a running average. Arguments env (Environment) - Environment to normalize. statistics (dict, optional , default=None) - Dictionary used to bootstrap the normalizing statistics. beta (float, optional , default=0.99) - Moving average weigth. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from Tristan Deleu's implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.RewardNormalizer(env) env2 = gym.make('CartPole-v0') env2 = cherry.envs.RewardNormalizer(env2, statistics=env.statistics) RewardClipper RewardClipper(env) reward RewardClipper.reward(reward) Bin reward to {+1, 0, -1} by its sign. Monitor Monitor(env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor. OpenAIAtari OpenAIAtari(env) AddTimestep AddTimestep(env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/ StateLambda StateLambda(env, fn) ActionLambda ActionLambda(env, fn) ActionSpaceScaler ActionSpaceScaler(env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"cherry.envs"},{"location":"docs/cherry.envs/#cherryenvsutils","text":"Description Helper functions for OpenAI Gym environments.","title":"cherry.envs.utils"},{"location":"docs/cherry.envs/#is_discrete","text":"is_discrete(space, vectorized=False) Returns whether a space is discrete. Arguments space - The space. vectorized - Whether to return the discreteness for the vectorized environments (True) or just the discreteness of the underlying environment (False).","title":"is_discrete"},{"location":"docs/cherry.envs/#get_space_dimension","text":"get_space_dimension(space, vectorized_dims=False) Returns the number of elements of a space sample, when unrolled. Arguments space - The space. vectorized_dims - Whether to return the full dimension for vectorized environments (True) or just the dimension for the underlying environment (False).","title":"get_space_dimension"},{"location":"docs/cherry.envs/#wrapper","text":"Wrapper(env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger.","title":"Wrapper"},{"location":"docs/cherry.envs/#action_size","text":"The number of dimensions of a single action.","title":"action_size"},{"location":"docs/cherry.envs/#state_size","text":"The (flattened) size of a single state.","title":"state_size"},{"location":"docs/cherry.envs/#runner","text":"Runner(env) Runner wrapper. TODO: When is_vectorized and using episodes=n, use the parallel environmnents to sample n episodes, and stack them inside a flat replay.","title":"Runner"},{"location":"docs/cherry.envs/#run","text":"Runner.run(get_action, steps=None, episodes=None, render=False) Runner wrapper's run method.","title":"run"},{"location":"docs/cherry.envs/#logger","text":"Logger(env, interval=1000, episode_interval=10, title=None, logger=None) Tracks and prints some common statistics about the environment.","title":"Logger"},{"location":"docs/cherry.envs/#recorder","text":"Recorder(env, directory='./videos/', format='gif', suffix=None) [Source] Description Wrapper to record episodes from a rollout. Supports GIF and MP4 encoding. Arguments env (Environment) - Environment to record. directory (str, optional , default='./videos/') - Relative path to where videos will be saved. format (str, optional , default='gif') - Format of the videos. Choose in ['gif', 'mp4'], defaults to gif. If it's text environment, the format will be json. suffix (str, optional , default=None): A unique id used as part of the suffix for the file. By default, uses os.getpid(). Credit Adapted from OpenAI Gym's Monitor wrapper. Example env = gym.make('CartPole-v0') env = envs.Recorder(record_env, './videos/', format='gif') env = envs.Runner(env) env.run(get_action, episodes=3, render=True)","title":"Recorder"},{"location":"docs/cherry.envs/#close","text":"Recorder.close() Flush all monitor data to disk and close any open rending windows","title":"close"},{"location":"docs/cherry.envs/#visdomlogger","text":"VisdomLogger(env, interval=1000, episode_interval=10, render=True, title=None, logger=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes.","title":"VisdomLogger"},{"location":"docs/cherry.envs/#torch","text":"Torch(env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action)","title":"Torch"},{"location":"docs/cherry.envs/#normalizer","text":"Normalizer(env, states=True, rewards=True, clip_states=10.0, clip_rewards=10.0, gamma=0.99, eps=1e-08) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.Normalizer(env, states=True, rewards=False)","title":"Normalizer"},{"location":"docs/cherry.envs/#statenormalizer","text":"StateNormalizer(env, statistics=None, beta=0.99, eps=1e-08) [Source] Description Normalizes the states with a running average. Arguments env (Environment) - Environment to normalize. statistics (dict, optional , default=None) - Dictionary used to bootstrap the normalizing statistics. beta (float, optional , default=0.99) - Moving average weigth. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from Tristan Deleu's implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.StateNormalizer(env) env2 = gym.make('CartPole-v0') env2 = cherry.envs.StateNormalizer(env2, statistics=env.statistics)","title":"StateNormalizer"},{"location":"docs/cherry.envs/#rewardnormalizer","text":"RewardNormalizer(env, statistics=None, beta=0.99, eps=1e-08) [Source] Description Normalizes the rewards with a running average. Arguments env (Environment) - Environment to normalize. statistics (dict, optional , default=None) - Dictionary used to bootstrap the normalizing statistics. beta (float, optional , default=0.99) - Moving average weigth. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from Tristan Deleu's implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.RewardNormalizer(env) env2 = gym.make('CartPole-v0') env2 = cherry.envs.RewardNormalizer(env2, statistics=env.statistics)","title":"RewardNormalizer"},{"location":"docs/cherry.envs/#rewardclipper","text":"RewardClipper(env)","title":"RewardClipper"},{"location":"docs/cherry.envs/#reward","text":"RewardClipper.reward(reward) Bin reward to {+1, 0, -1} by its sign.","title":"reward"},{"location":"docs/cherry.envs/#monitor","text":"Monitor(env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor.","title":"Monitor"},{"location":"docs/cherry.envs/#openaiatari","text":"OpenAIAtari(env)","title":"OpenAIAtari"},{"location":"docs/cherry.envs/#addtimestep","text":"AddTimestep(env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/","title":"AddTimestep"},{"location":"docs/cherry.envs/#statelambda","text":"StateLambda(env, fn)","title":"StateLambda"},{"location":"docs/cherry.envs/#actionlambda","text":"ActionLambda(env, fn)","title":"ActionLambda"},{"location":"docs/cherry.envs/#actionspacescaler","text":"ActionSpaceScaler(env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"ActionSpaceScaler"},{"location":"docs/cherry/","text":"Transition Transition(state, action, reward, next_state, done, device=None, **infos) Description Represents a (s, a, r, s', d) tuple. All attributes (including the ones in infos) are accessible via transition.name_of_attr . (e.g. transition.log_prob if log_prob is in infos .) Arguments state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example for transition in replay: print(transition.state) to Transition.to(*args, **kwargs) Description Moves the constituents of the transition to the desired device, and casts them to the desired format. Note: This is done in-place and doesn't create a new transition. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example sars = Transition(state, action, reward, next_state) sars.to('cuda') ExperienceReplay ExperienceReplay(storage=None, device=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.append(state, # Add experience to the replay action, reward, next_state, done, density: action_density, log_prob: action_density.log_prob(action), ) replay.state() # Tensor of states replay.action() # Tensor of actions replay.density() # list of action_density replay.log_prob() # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True) save ExperienceReplay.save(path) Description Serializes and saves the ExperienceReplay into the given path. Arguments path (str) - File path. Example replay.save('my_replay_file.pt') load ExperienceReplay.load(path) Description Loads data from a serialized ExperienceReplay. Arguments path (str) - File path of serialized ExperienceReplay. Example replay.load('my_replay_file.pt') append ExperienceReplay.append(state=None, action=None, reward=None, next_state=None, done=None, **infos) Description Appends new data to the list ExperienceReplay. Arguments state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example replay.append(state, action, reward, next_state, done, info={ 'density': density, 'log_prob': density.log_prob(action), }) sample ExperienceReplay.sample(size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay - New ExperienceReplay containing the sampled transitions. empty ExperienceReplay.empty() Description Removes all data from an ExperienceReplay. Example replay.empty() to ExperienceReplay.to(*args, **kwargs) Description Calls .to() on all transitions of the experience replay, moving them to the desired device and casting the to the desired format. Note: This return a new experience replay, but the transitions are modified in-place. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example replay.to('cuda:1') policy.to('cuda:1') for sars in replay: cuda_action = policy(sars.state).sample() totensor totensor(array, dtype=None) [Source] Description Converts the argument array to a torch.tensor 1xN, regardless of its type or dimension. Arguments array (int, float, ndarray, tensor) - Data to be converted to array. dtype (dtype, optional , default=None) - Data type to use for representation. By default, uses torch.get_default_dtype() . Returns Tensor of shape 1xN with the appropriate data type. Example array = [5, 6, 7.0] tensor = cherry.totensor(array, dtype=th.float32) array = np.array(array, dtype=np.float64) tensor = cherry.totensor(array, dtype=th.float16) normalize normalize(tensor, epsilon=1e-08) [Source] Description Normalizes a tensor to have zero mean and unit standard deviation values. Arguments tensor (tensor) - The tensor to normalize. epsilon (float, optional , default=1e-8) - Numerical stability constant for normalization. Returns A new tensor, containing the normalized values. Example tensor = torch.arange(23) / 255.0 tensor = cherry.normalize(tensor, epsilon=1e-3)","title":"cherry"},{"location":"docs/cherry/#transition","text":"Transition(state, action, reward, next_state, done, device=None, **infos) Description Represents a (s, a, r, s', d) tuple. All attributes (including the ones in infos) are accessible via transition.name_of_attr . (e.g. transition.log_prob if log_prob is in infos .) Arguments state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example for transition in replay: print(transition.state)","title":"Transition"},{"location":"docs/cherry/#to","text":"Transition.to(*args, **kwargs) Description Moves the constituents of the transition to the desired device, and casts them to the desired format. Note: This is done in-place and doesn't create a new transition. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example sars = Transition(state, action, reward, next_state) sars.to('cuda')","title":"to"},{"location":"docs/cherry/#experiencereplay","text":"ExperienceReplay(storage=None, device=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.append(state, # Add experience to the replay action, reward, next_state, done, density: action_density, log_prob: action_density.log_prob(action), ) replay.state() # Tensor of states replay.action() # Tensor of actions replay.density() # list of action_density replay.log_prob() # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True)","title":"ExperienceReplay"},{"location":"docs/cherry/#save","text":"ExperienceReplay.save(path) Description Serializes and saves the ExperienceReplay into the given path. Arguments path (str) - File path. Example replay.save('my_replay_file.pt')","title":"save"},{"location":"docs/cherry/#load","text":"ExperienceReplay.load(path) Description Loads data from a serialized ExperienceReplay. Arguments path (str) - File path of serialized ExperienceReplay. Example replay.load('my_replay_file.pt')","title":"load"},{"location":"docs/cherry/#append","text":"ExperienceReplay.append(state=None, action=None, reward=None, next_state=None, done=None, **infos) Description Appends new data to the list ExperienceReplay. Arguments state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example replay.append(state, action, reward, next_state, done, info={ 'density': density, 'log_prob': density.log_prob(action), })","title":"append"},{"location":"docs/cherry/#sample","text":"ExperienceReplay.sample(size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay - New ExperienceReplay containing the sampled transitions.","title":"sample"},{"location":"docs/cherry/#empty","text":"ExperienceReplay.empty() Description Removes all data from an ExperienceReplay. Example replay.empty()","title":"empty"},{"location":"docs/cherry/#to_1","text":"ExperienceReplay.to(*args, **kwargs) Description Calls .to() on all transitions of the experience replay, moving them to the desired device and casting the to the desired format. Note: This return a new experience replay, but the transitions are modified in-place. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example replay.to('cuda:1') policy.to('cuda:1') for sars in replay: cuda_action = policy(sars.state).sample()","title":"to"},{"location":"docs/cherry/#totensor","text":"totensor(array, dtype=None) [Source] Description Converts the argument array to a torch.tensor 1xN, regardless of its type or dimension. Arguments array (int, float, ndarray, tensor) - Data to be converted to array. dtype (dtype, optional , default=None) - Data type to use for representation. By default, uses torch.get_default_dtype() . Returns Tensor of shape 1xN with the appropriate data type. Example array = [5, 6, 7.0] tensor = cherry.totensor(array, dtype=th.float32) array = np.array(array, dtype=np.float64) tensor = cherry.totensor(array, dtype=th.float16)","title":"totensor"},{"location":"docs/cherry/#normalize","text":"normalize(tensor, epsilon=1e-08) [Source] Description Normalizes a tensor to have zero mean and unit standard deviation values. Arguments tensor (tensor) - The tensor to normalize. epsilon (float, optional , default=1e-8) - Numerical stability constant for normalization. Returns A new tensor, containing the normalized values. Example tensor = torch.arange(23) / 255.0 tensor = cherry.normalize(tensor, epsilon=1e-3)","title":"normalize"},{"location":"docs/cherry.models/","text":"cherry.models.utils RandomPolicy RandomPolicy(env, *args, **kwargs) [Source] Description Policy that randomly samples actions from the environment action space. Arguments env (Environment) - Environment from which to sample actions. Example policy = ch.models.RandomPolicy(env) env = envs.Runner(env) replay = env.run(policy, steps=2048) polyak_average polyak_average(source, target, alpha) [Source] Description Shifts the parameters of source towards those of target. Note: the parameter alpha indicates the convex combination weight of the source. (i.e. the old parameters are kept at a rate of alpha .) References Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments source (nn.Module) - The module to be shifted. target (nn.Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example target_qf = nn.Linear(23, 34) qf = nn.Linear(23, 34) ch.models.polyak_average(target_qf, qf, alpha=0.9) cherry.models.tabular StateValueFunction StateValueFunction(state_size, init=None) [Source] Description Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example vf = StateValueFunction(env.state_size) state = env.reset() state = ch.onehot(state, env.state_size) state_value = vf(state) ActionValueFunction ActionValueFunction(state_size, action_size, init=None) [Source] Description Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example qf = ActionValueFunction(env.state_size, env.action_size) state = env.reset() state = ch.onehot(state, env.state_size) all_action_values = qf(state) action = ch.onehot(0, env.action_size) action_value = qf(state, action) cherry.models.atari NatureFeatures NatureFeatures(input_size=4, output_size=512, hidden_size=3136) [Source] Description The convolutional body of the DQN architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation. hidden_size (int, optional , default=1568) - Size of the representation after the convolutional layers NatureActor NatureActor(input_size, output_size) [Source] Description The actor head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space. NatureCritic NatureCritic(input_size, output_size=1) [Source] Description The critic head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value. cherry.models.robotics RoboticsMLP RoboticsMLP(input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with proper initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example target_qf = ch.models.robotics.RoboticsMLP(23, 34, layer_sizes=[32, 32]) RoboticsActor RoboticsActor(input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with initialization designed for choosing actions in continuous robotic environments. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example policy_mean = ch.models.robotics.Actor(28, 8, layer_sizes=[64, 32, 16]) LinearValue LinearValue(input_size, reg=1e-05) [Source] Description A linear state-value function, whose parameters are found by minimizing least-squares. Credit Adapted from Tristan Deleu's implementation. References Duan et al. 2016. \u201cBenchmarking Deep Reinforcement Learning for Continuous Control.\u201d https://github.com/tristandeleu/pytorch-maml-rl Arguments inputs_size (int) - Size of input. reg (float, optional , default=1e-5) - Regularization coefficient. Example states = replay.state() rewards = replay.reward() dones = replay.done() returns = ch.td.discount(gamma, rewards, dones) baseline = LinearValue(input_size) baseline.fit(states, returns) next_values = baseline(replay.next_states())","title":"cherry.models"},{"location":"docs/cherry.models/#cherrymodelsutils","text":"","title":"cherry.models.utils"},{"location":"docs/cherry.models/#randompolicy","text":"RandomPolicy(env, *args, **kwargs) [Source] Description Policy that randomly samples actions from the environment action space. Arguments env (Environment) - Environment from which to sample actions. Example policy = ch.models.RandomPolicy(env) env = envs.Runner(env) replay = env.run(policy, steps=2048)","title":"RandomPolicy"},{"location":"docs/cherry.models/#polyak_average","text":"polyak_average(source, target, alpha) [Source] Description Shifts the parameters of source towards those of target. Note: the parameter alpha indicates the convex combination weight of the source. (i.e. the old parameters are kept at a rate of alpha .) References Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments source (nn.Module) - The module to be shifted. target (nn.Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example target_qf = nn.Linear(23, 34) qf = nn.Linear(23, 34) ch.models.polyak_average(target_qf, qf, alpha=0.9)","title":"polyak_average"},{"location":"docs/cherry.models/#cherrymodelstabular","text":"","title":"cherry.models.tabular"},{"location":"docs/cherry.models/#statevaluefunction","text":"StateValueFunction(state_size, init=None) [Source] Description Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example vf = StateValueFunction(env.state_size) state = env.reset() state = ch.onehot(state, env.state_size) state_value = vf(state)","title":"StateValueFunction"},{"location":"docs/cherry.models/#actionvaluefunction","text":"ActionValueFunction(state_size, action_size, init=None) [Source] Description Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example qf = ActionValueFunction(env.state_size, env.action_size) state = env.reset() state = ch.onehot(state, env.state_size) all_action_values = qf(state) action = ch.onehot(0, env.action_size) action_value = qf(state, action)","title":"ActionValueFunction"},{"location":"docs/cherry.models/#cherrymodelsatari","text":"","title":"cherry.models.atari"},{"location":"docs/cherry.models/#naturefeatures","text":"NatureFeatures(input_size=4, output_size=512, hidden_size=3136) [Source] Description The convolutional body of the DQN architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation. hidden_size (int, optional , default=1568) - Size of the representation after the convolutional layers","title":"NatureFeatures"},{"location":"docs/cherry.models/#natureactor","text":"NatureActor(input_size, output_size) [Source] Description The actor head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space.","title":"NatureActor"},{"location":"docs/cherry.models/#naturecritic","text":"NatureCritic(input_size, output_size=1) [Source] Description The critic head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value.","title":"NatureCritic"},{"location":"docs/cherry.models/#cherrymodelsrobotics","text":"","title":"cherry.models.robotics"},{"location":"docs/cherry.models/#roboticsmlp","text":"RoboticsMLP(input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with proper initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example target_qf = ch.models.robotics.RoboticsMLP(23, 34, layer_sizes=[32, 32])","title":"RoboticsMLP"},{"location":"docs/cherry.models/#roboticsactor","text":"RoboticsActor(input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with initialization designed for choosing actions in continuous robotic environments. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example policy_mean = ch.models.robotics.Actor(28, 8, layer_sizes=[64, 32, 16])","title":"RoboticsActor"},{"location":"docs/cherry.models/#linearvalue","text":"LinearValue(input_size, reg=1e-05) [Source] Description A linear state-value function, whose parameters are found by minimizing least-squares. Credit Adapted from Tristan Deleu's implementation. References Duan et al. 2016. \u201cBenchmarking Deep Reinforcement Learning for Continuous Control.\u201d https://github.com/tristandeleu/pytorch-maml-rl Arguments inputs_size (int) - Size of input. reg (float, optional , default=1e-5) - Regularization coefficient. Example states = replay.state() rewards = replay.reward() dones = replay.done() returns = ch.td.discount(gamma, rewards, dones) baseline = LinearValue(input_size) baseline.fit(states, returns) next_values = baseline(replay.next_states())","title":"LinearValue"},{"location":"docs/cherry.nn.init/","text":"cherry.nn.init robotics_init_ robotics_init_(module, gain=None) [Source] Description Default initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments module (nn.Module) - Module to initialize. gain (float, optional , default=sqrt(2.0)) - Gain of orthogonal initialization. Returns Module, whose weight and bias have been modified in-place. Example linear = nn.Linear(23, 5) kostrikov_robotics_(linear) atari_init_ atari_init_(module, gain=None) [Source] Description Default initialization for Atari environments. Credit Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments module (nn.Module) - Module to initialize. gain (float, optional , default=None) - Gain of orthogonal initialization. Default is computed for ReLU activation with torch.nn.init.calculate_gain('relu') . Returns Module, whose weight and bias have been modified in-place. Example linear = nn.Linear(23, 5) atari_init_(linear)","title":"cherry.nn.init"},{"location":"docs/cherry.nn.init/#cherrynninit","text":"","title":"cherry.nn.init"},{"location":"docs/cherry.nn.init/#robotics_init_","text":"robotics_init_(module, gain=None) [Source] Description Default initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments module (nn.Module) - Module to initialize. gain (float, optional , default=sqrt(2.0)) - Gain of orthogonal initialization. Returns Module, whose weight and bias have been modified in-place. Example linear = nn.Linear(23, 5) kostrikov_robotics_(linear)","title":"robotics_init_"},{"location":"docs/cherry.nn.init/#atari_init_","text":"atari_init_(module, gain=None) [Source] Description Default initialization for Atari environments. Credit Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments module (nn.Module) - Module to initialize. gain (float, optional , default=None) - Gain of orthogonal initialization. Default is computed for ReLU activation with torch.nn.init.calculate_gain('relu') . Returns Module, whose weight and bias have been modified in-place. Example linear = nn.Linear(23, 5) atari_init_(linear)","title":"atari_init_"},{"location":"docs/cherry.nn/","text":"RoboticsLinear RoboticsLinear(*args, **kwargs) [Source] Description Akin to nn.Linear , but with proper initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation. Arguments gain (float, optional ) - Gain factor passed to robotics_init_ initialization. This class extends nn.Linear and supports all of its arguments. Example linear = ch.nn.Linear(23, 5, bias=True) action_mean = linear(state)","title":"cherry.nn"},{"location":"docs/cherry.nn/#roboticslinear","text":"RoboticsLinear(*args, **kwargs) [Source] Description Akin to nn.Linear , but with proper initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation. Arguments gain (float, optional ) - Gain factor passed to robotics_init_ initialization. This class extends nn.Linear and supports all of its arguments. Example linear = ch.nn.Linear(23, 5, bias=True) action_mean = linear(state)","title":"RoboticsLinear"},{"location":"docs/cherry.optim/","text":"cherry.optim Description Optimization utilities for scalable, high-performance reinforcement learning. Distributed Distributed(params, opt, sync=None) [Source] Description Synchronizes the gradients of a model across replicas. At every step, Distributed averages the gradient across all replicas before calling the wrapped optimizer. The sync parameters determines how frequently the parameters are synchronized between replicas, to minimize numerical divergences. This is done by calling the sync_parameters() method. If sync is None , this never happens except upon initialization of the class. Arguments params (iterable) - Iterable of parameters. opt (Optimizer) - The optimizer to wrap and synchronize. sync (int, optional , default=None) - Parameter synchronization frequency. References Zinkevich et al. 2010. \u201cParallelized Stochastic Gradient Descent.\u201d Example opt = optim.Adam(model.parameters()) opt = Distributed(model.parameters(), opt, sync=1) opt.step() opt.sync_parameters() sync_parameters Distributed.sync_parameters(root=0) Description Broadcasts all parameters of root to all other replicas. Arguments root (int, optional , default=0) - Rank of root replica.","title":"cherry.optim"},{"location":"docs/cherry.optim/#cherryoptim","text":"Description Optimization utilities for scalable, high-performance reinforcement learning.","title":"cherry.optim"},{"location":"docs/cherry.optim/#distributed","text":"Distributed(params, opt, sync=None) [Source] Description Synchronizes the gradients of a model across replicas. At every step, Distributed averages the gradient across all replicas before calling the wrapped optimizer. The sync parameters determines how frequently the parameters are synchronized between replicas, to minimize numerical divergences. This is done by calling the sync_parameters() method. If sync is None , this never happens except upon initialization of the class. Arguments params (iterable) - Iterable of parameters. opt (Optimizer) - The optimizer to wrap and synchronize. sync (int, optional , default=None) - Parameter synchronization frequency. References Zinkevich et al. 2010. \u201cParallelized Stochastic Gradient Descent.\u201d Example opt = optim.Adam(model.parameters()) opt = Distributed(model.parameters(), opt, sync=1) opt.step() opt.sync_parameters()","title":"Distributed"},{"location":"docs/cherry.optim/#sync_parameters","text":"Distributed.sync_parameters(root=0) Description Broadcasts all parameters of root to all other replicas. Arguments root (int, optional , default=0) - Rank of root replica.","title":"sync_parameters"},{"location":"docs/cherry.pg/","text":"cherry.pg Description Utilities to implement policy gradient algorithms. generalized_advantage generalized_advantage(gamma, tau, rewards, dones, values, next_value) Description Computes the generalized advantage estimator. (GAE) References Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns tensor - Tensor of advantages. Example mass, next_value = policy(replay[-1].next_state) advantages = generalized_advantage(0.99, 0.95, replay.reward(), replay.value(), replay.done(), next_value)","title":"cherry.pg"},{"location":"docs/cherry.pg/#cherrypg","text":"Description Utilities to implement policy gradient algorithms.","title":"cherry.pg"},{"location":"docs/cherry.pg/#generalized_advantage","text":"generalized_advantage(gamma, tau, rewards, dones, values, next_value) Description Computes the generalized advantage estimator. (GAE) References Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns tensor - Tensor of advantages. Example mass, next_value = policy(replay[-1].next_state) advantages = generalized_advantage(0.99, 0.95, replay.reward(), replay.value(), replay.done(), next_value)","title":"generalized_advantage"},{"location":"docs/cherry.plot/","text":"cherry.plot Description Plotting utilities for reproducible research. ci95 ci95(values) [Source] Description Computes the 95% confidence interval around the given values. Arguments values (list) - List of values for which to compute the 95% confidence interval. Returns (float, float) The lower and upper bounds of the confidence interval. Example from statistics import mean smoothed = [] for replay in replays: rewards = replay.rewards.view(-1).tolist() y_smoothed = ch.plot.smooth(rewards) smoothed.append(y_smoothed) means = [mean(r) for r in zip(*smoothed)] confidences = [ch.plot.ci95(r) for r in zip(*smoothed)] lower_bound = [conf[0] for conf in confidences] upper_bound = [conf[1] for conf in confidences] exponential_smoothing exponential_smoothing(x, y=None, temperature=1.0) [Source] Decription Two-sided exponential moving average for smoothing a curve. It performs regular exponential moving average twice from two different sides and then combines the results together. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed, y_smoothed, _ = exponential_smoothing(x_original, y_original, temperature=3.)","title":"cherry.plot"},{"location":"docs/cherry.plot/#cherryplot","text":"Description Plotting utilities for reproducible research.","title":"cherry.plot"},{"location":"docs/cherry.plot/#ci95","text":"ci95(values) [Source] Description Computes the 95% confidence interval around the given values. Arguments values (list) - List of values for which to compute the 95% confidence interval. Returns (float, float) The lower and upper bounds of the confidence interval. Example from statistics import mean smoothed = [] for replay in replays: rewards = replay.rewards.view(-1).tolist() y_smoothed = ch.plot.smooth(rewards) smoothed.append(y_smoothed) means = [mean(r) for r in zip(*smoothed)] confidences = [ch.plot.ci95(r) for r in zip(*smoothed)] lower_bound = [conf[0] for conf in confidences] upper_bound = [conf[1] for conf in confidences]","title":"ci95"},{"location":"docs/cherry.plot/#exponential_smoothing","text":"exponential_smoothing(x, y=None, temperature=1.0) [Source] Decription Two-sided exponential moving average for smoothing a curve. It performs regular exponential moving average twice from two different sides and then combines the results together. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed, y_smoothed, _ = exponential_smoothing(x_original, y_original, temperature=3.)","title":"exponential_smoothing"},{"location":"docs/cherry.td/","text":"cherry.td Description Utilities to implement temporal difference algorithms. discount discount(gamma, rewards, dones, bootstrap=0.0) Description Discounts rewards at an rate of gamma. References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns tensor - Tensor of discounted rewards. Example rewards = th.ones(23, 1) * 8 dones = th.zeros_like(rewards) dones[-1] += 1.0 discounted = ch.rl.discount(0.99, rewards, dones, bootstrap=1.0) temporal_difference temporal_difference(gamma, rewards, dones, values, next_values) Description Returns the temporal difference residual. Reference Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_values (tensor) - Values of the state obtained after the transition from the state used to compute the last value in values . Example values = vf(replay.states()) next_values = vf(replay.next_states()) td_errors = temporal_difference(0.99, replay.reward(), replay.done(), values, next_values)","title":"cherry.td"},{"location":"docs/cherry.td/#cherrytd","text":"Description Utilities to implement temporal difference algorithms.","title":"cherry.td"},{"location":"docs/cherry.td/#discount","text":"discount(gamma, rewards, dones, bootstrap=0.0) Description Discounts rewards at an rate of gamma. References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns tensor - Tensor of discounted rewards. Example rewards = th.ones(23, 1) * 8 dones = th.zeros_like(rewards) dones[-1] += 1.0 discounted = ch.rl.discount(0.99, rewards, dones, bootstrap=1.0)","title":"discount"},{"location":"docs/cherry.td/#temporal_difference","text":"temporal_difference(gamma, rewards, dones, values, next_values) Description Returns the temporal difference residual. Reference Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_values (tensor) - Values of the state obtained after the transition from the state used to compute the last value in values . Example values = vf(replay.states()) next_values = vf(replay.next_states()) td_errors = temporal_difference(0.99, replay.reward(), replay.done(), values, next_values)","title":"temporal_difference"},{"location":"tutorials/debugging_rl/","text":"Debugging Reinforcement Learning Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper, Explain how to use the debug mode, (post-mortem, logger, warning from functions) Maybe showcase all of the above with an example that seems right but doesn't work, Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/debugging_rl/#debugging-reinforcement-learning","text":"Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper, Explain how to use the debug mode, (post-mortem, logger, warning from functions) Maybe showcase all of the above with an example that seems right but doesn't work, Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/distributed_ppo/","text":"Distributed Training with PPO Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/distributed_ppo/#distributed-training-with-ppo","text":"Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/getting_started/","text":"Getting Started with Cherry This document provides an overview of the philosophy behind cherry, the tools it provides, and a small illustrative example. By the end of this tutorial, you should be well-equiped to incorporate cherry in your research workflow. We assume that you are already familiar with reinforcement learning. If, instead, you're looking for an introduction to the field we recommend looking at Josh Achiam's Spinning Up in Deep RL . Installation The first step in getting started with cherry is to install it. You can easily do that by typing the following command in your favorite shell. pip install cherry-rl By default cherry only has two dependencies: torch and gym . However, more dependencies might be required if you plan to use some specific functionalities. For example, the OpenAIAtari wrapper requires OpenCV ( pip install opencv-python ) and the VisdomLogger requires visdom ( pip install visdom ). Note While cherry depends on Gym for its environment wrappers, it doesn't restrict you to Gym environments. For instance, check the examples using simple_rl and pycolab environments for Gym-free usage of cherry. Overview Why do we need cherry ? There are many reinforcement learning libraries, many of which feature high-quality implementations. However, few of them provide the kind of low-level utilities useful to researchers. Cherry aims to alleviate this issue. Instead of an interface akin to PPO(env_name).train(1000) , it provides researchers with a set of tools they can use to write readable, replicable, and flexible implementations. Cherry prioritizes time-to-correct-implementation over time-to-run, by explicitely helping you check, debug, and reliably report your results. How to use cherry ? Our goal is to make cherry a natural extension to PyTorch, with reinforcement learning in mind. To this end, we closely follow the package structure of PyTorch while providing additional utilities where we see fit. So if your goal is to implement a novel distributed off-policy policy gradient algorithm, you can count on cherry to provide you experience replays, policy gradient losses, discounting/advantage functions, and distributed optimizers. Those functions not only reduce the time spent writing code, they also check that your implementation is sane. (e.g. do the log probabilities and rewards have identical shapes?) Moreover, cherry provides the implementation details necessary to make deep reinforcement learning work. (e.g. initializations, modules, and wrappers commonly used in robotic or Atari benchmarks.) Importantly, it includes built-in debugging functionalities: cherry can help you visualize what is happening under the hood of your algorithm to help you find bugs faster. What's the future of cherry ? Reinforcement learning is a fast moving field, and it is difficult to predict which advances are safe bets for the future. Our long-term development strategy can be summarized as follows. Have as many recent and high-quality examples as possible. Merge advances that turn up to be fundamental in theory or practice into the core library. We hope to combat the reproducibility crisis by extensively testing and benchmarking our implementations. Note Cherry is in its early days and is still missing some of the well-established methods from the past 60 years. Those ones are being implemented as fast as we can :) Core Features The following features are fundamental components of cherry. Transitions and Experience Replay A majority of algorithms needs to store, retrieve, and sample past experience. To that end, you can use cherry's ExperienceReplay . An experience replay is implemented as a wrapper around a standard Python list. The major difference is that the append() method expects arguments used to create a Transition . In addition to behaving like a list, it exposes methods that act on this list, such as to(device) (moves the replay to a device), sample() (randomly samples some experience), or load() / save() (for convenient serialization). An ExperienceReplay contains Transition s, which are akin to ( state , action , reward , next_state , done ) named tuples with possibly additional custom fields. Those fields are easily accessible directly from the replay by accessing the method named after them. For example, calling replay.action() will fetch the action field from every transition stored in replay , stack them along the first dimension, and return that large tensor. The same is true for custom fields; if all transitions have a logprob field, replay.logprob() will return the result of stacking them. Temporal Difference and Policy Gradients Many low-level utilities used to implement temporal difference and policy gradient algorithms are available in the cherry.td and cherry.pg modules, respectively. Those modules include classical methods such as discounting rewards or computing the temporal difference , as well as more recent advances such as the generalized advantage estimator . We tried our best to avoid philosophical dissonance when a method belonged to both families of algorithms. Models and PyTorch Similar to PyTorch, we provide differentiable modules in cherry.nn , domain-specific initialization schemes in cherry.nn.init , and optimization utilities in cherry.optim . In addition, popular higher-level models are available in cherry.models ; for instance, those include tabular modules , the Atari CNN features extractor , and a Multi-Layer Perceptron for continuous control. Gym Wrappers Given the popularity of OpenAI Gym environment in modern reinforcement learning benchmarks, cherry includes convenient wrappers in the cherry.envs package. Examples include normalization of states and actions , Atari frames pre-processing, customizable state / action processing, and automatic collection of experience in a replay. Plots Reporting comparable results has become a central problem in modern reinforcement learning. In order to alleviate this issue, cherry provides utilities to smooth and compute confidence intervals over lists of rewards. Those are available in the cherry.plot submodule. Implementing Policy Gradient As an introductory example let us dissect the following snippet, which demonstrates how to implement the policy gradient theorem using cherry. import cherry as ch env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) env = ch.envs.Runner(env) env.seed(42) policy = PolicyNet() optimizer = optim.Adam(policy.parameters(), lr=1e-2) action_dist = ch.distributions.ActionDistribution(env) def get_action(state): mass = action_dist(policy(state)) action = mass.sample() log_prob = mass.log_prob(action) return action, {'log_prob': log_prob} for step in range(1000): replay = env.run(get_action, episodes=1) rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() After importing cherry, the first step is to instanciate, wrap, and seed the desired gym environment. env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) env = ch.envs.Runner(env) env.seed(42) The Logger , Torch , and Runner classes are Gym environment wrappers that systematically modify the behaviour of an environment: Logger keeps track of metrics and prints them at a given interval. Torch converts Gym states into PyTorch tensors, and action tensors into numpy arrays. Runner implements a run() method which allows to easily gather transitions for a number of steps or episodes. One particularity of wrappers is that they automatically expose methods of the wrapped environment: env.seed(42) calls the seed() method from the CartPole-v0 environment. Second, we instanciate the policy, optimizer, as well as the action distribution. The action distribution is created with action_dist = ch.distributions.ActionDistribution(env) which will automatically choose a diagonal Gaussian for continuous action-spaces and a categorical distribution for discrete ones. Next, we define get_action() which specifies how to get an action from our agent and will be used in conjuction to env.run() to quickly collect experience data: replay = env.run(get_action, episodes=1) env.run() assumes that the first returned value by get_action is the action to be passed to the environment and the second, optional, returned value is a dictionary to be saved into the experience replay. Under the hood, env.run() creates a new ExperienceReplay and fills it with the desired number of transitions; instead of episodes=1 we could have passed steps=100 . Finally, we discount and normalize the rewards and take an optimization step on the policy gradient loss. rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() When calling replay.reward() , replay.done() , or replay.log_prob() , the experience replay will concatenate the corresponding attribute across all of its transitions and return a new tensor . This means that this operation is rather expensive (you should cache it when possible) and that modifying this tensor does not modify the corresponding transitions in replay . Note that in this case log_prob is a custom attribute which is not declared in the original implementation of ExperienceReplay , and we could have given it any name by changing the dictionary key in get_action() . Conclusion You should now be able to use cherry in your own work. For more information, have a look at the documentation , the other tutorials , or the numerous examples . Since one of the characteristics of cherry is to avoid providing \"pre-baked\" algorithms, we tried our best to heavily document its usage.","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#getting-started-with-cherry","text":"This document provides an overview of the philosophy behind cherry, the tools it provides, and a small illustrative example. By the end of this tutorial, you should be well-equiped to incorporate cherry in your research workflow. We assume that you are already familiar with reinforcement learning. If, instead, you're looking for an introduction to the field we recommend looking at Josh Achiam's Spinning Up in Deep RL .","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#installation","text":"The first step in getting started with cherry is to install it. You can easily do that by typing the following command in your favorite shell. pip install cherry-rl By default cherry only has two dependencies: torch and gym . However, more dependencies might be required if you plan to use some specific functionalities. For example, the OpenAIAtari wrapper requires OpenCV ( pip install opencv-python ) and the VisdomLogger requires visdom ( pip install visdom ). Note While cherry depends on Gym for its environment wrappers, it doesn't restrict you to Gym environments. For instance, check the examples using simple_rl and pycolab environments for Gym-free usage of cherry.","title":"Installation"},{"location":"tutorials/getting_started/#overview","text":"Why do we need cherry ? There are many reinforcement learning libraries, many of which feature high-quality implementations. However, few of them provide the kind of low-level utilities useful to researchers. Cherry aims to alleviate this issue. Instead of an interface akin to PPO(env_name).train(1000) , it provides researchers with a set of tools they can use to write readable, replicable, and flexible implementations. Cherry prioritizes time-to-correct-implementation over time-to-run, by explicitely helping you check, debug, and reliably report your results. How to use cherry ? Our goal is to make cherry a natural extension to PyTorch, with reinforcement learning in mind. To this end, we closely follow the package structure of PyTorch while providing additional utilities where we see fit. So if your goal is to implement a novel distributed off-policy policy gradient algorithm, you can count on cherry to provide you experience replays, policy gradient losses, discounting/advantage functions, and distributed optimizers. Those functions not only reduce the time spent writing code, they also check that your implementation is sane. (e.g. do the log probabilities and rewards have identical shapes?) Moreover, cherry provides the implementation details necessary to make deep reinforcement learning work. (e.g. initializations, modules, and wrappers commonly used in robotic or Atari benchmarks.) Importantly, it includes built-in debugging functionalities: cherry can help you visualize what is happening under the hood of your algorithm to help you find bugs faster. What's the future of cherry ? Reinforcement learning is a fast moving field, and it is difficult to predict which advances are safe bets for the future. Our long-term development strategy can be summarized as follows. Have as many recent and high-quality examples as possible. Merge advances that turn up to be fundamental in theory or practice into the core library. We hope to combat the reproducibility crisis by extensively testing and benchmarking our implementations. Note Cherry is in its early days and is still missing some of the well-established methods from the past 60 years. Those ones are being implemented as fast as we can :)","title":"Overview"},{"location":"tutorials/getting_started/#core-features","text":"The following features are fundamental components of cherry.","title":"Core Features"},{"location":"tutorials/getting_started/#transitions-and-experience-replay","text":"A majority of algorithms needs to store, retrieve, and sample past experience. To that end, you can use cherry's ExperienceReplay . An experience replay is implemented as a wrapper around a standard Python list. The major difference is that the append() method expects arguments used to create a Transition . In addition to behaving like a list, it exposes methods that act on this list, such as to(device) (moves the replay to a device), sample() (randomly samples some experience), or load() / save() (for convenient serialization). An ExperienceReplay contains Transition s, which are akin to ( state , action , reward , next_state , done ) named tuples with possibly additional custom fields. Those fields are easily accessible directly from the replay by accessing the method named after them. For example, calling replay.action() will fetch the action field from every transition stored in replay , stack them along the first dimension, and return that large tensor. The same is true for custom fields; if all transitions have a logprob field, replay.logprob() will return the result of stacking them.","title":"Transitions and Experience Replay"},{"location":"tutorials/getting_started/#temporal-difference-and-policy-gradients","text":"Many low-level utilities used to implement temporal difference and policy gradient algorithms are available in the cherry.td and cherry.pg modules, respectively. Those modules include classical methods such as discounting rewards or computing the temporal difference , as well as more recent advances such as the generalized advantage estimator . We tried our best to avoid philosophical dissonance when a method belonged to both families of algorithms.","title":"Temporal Difference and Policy Gradients"},{"location":"tutorials/getting_started/#models-and-pytorch","text":"Similar to PyTorch, we provide differentiable modules in cherry.nn , domain-specific initialization schemes in cherry.nn.init , and optimization utilities in cherry.optim . In addition, popular higher-level models are available in cherry.models ; for instance, those include tabular modules , the Atari CNN features extractor , and a Multi-Layer Perceptron for continuous control.","title":"Models and PyTorch"},{"location":"tutorials/getting_started/#gym-wrappers","text":"Given the popularity of OpenAI Gym environment in modern reinforcement learning benchmarks, cherry includes convenient wrappers in the cherry.envs package. Examples include normalization of states and actions , Atari frames pre-processing, customizable state / action processing, and automatic collection of experience in a replay.","title":"Gym Wrappers"},{"location":"tutorials/getting_started/#plots","text":"Reporting comparable results has become a central problem in modern reinforcement learning. In order to alleviate this issue, cherry provides utilities to smooth and compute confidence intervals over lists of rewards. Those are available in the cherry.plot submodule.","title":"Plots"},{"location":"tutorials/getting_started/#implementing-policy-gradient","text":"As an introductory example let us dissect the following snippet, which demonstrates how to implement the policy gradient theorem using cherry. import cherry as ch env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) env = ch.envs.Runner(env) env.seed(42) policy = PolicyNet() optimizer = optim.Adam(policy.parameters(), lr=1e-2) action_dist = ch.distributions.ActionDistribution(env) def get_action(state): mass = action_dist(policy(state)) action = mass.sample() log_prob = mass.log_prob(action) return action, {'log_prob': log_prob} for step in range(1000): replay = env.run(get_action, episodes=1) rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() After importing cherry, the first step is to instanciate, wrap, and seed the desired gym environment. env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) env = ch.envs.Runner(env) env.seed(42) The Logger , Torch , and Runner classes are Gym environment wrappers that systematically modify the behaviour of an environment: Logger keeps track of metrics and prints them at a given interval. Torch converts Gym states into PyTorch tensors, and action tensors into numpy arrays. Runner implements a run() method which allows to easily gather transitions for a number of steps or episodes. One particularity of wrappers is that they automatically expose methods of the wrapped environment: env.seed(42) calls the seed() method from the CartPole-v0 environment. Second, we instanciate the policy, optimizer, as well as the action distribution. The action distribution is created with action_dist = ch.distributions.ActionDistribution(env) which will automatically choose a diagonal Gaussian for continuous action-spaces and a categorical distribution for discrete ones. Next, we define get_action() which specifies how to get an action from our agent and will be used in conjuction to env.run() to quickly collect experience data: replay = env.run(get_action, episodes=1) env.run() assumes that the first returned value by get_action is the action to be passed to the environment and the second, optional, returned value is a dictionary to be saved into the experience replay. Under the hood, env.run() creates a new ExperienceReplay and fills it with the desired number of transitions; instead of episodes=1 we could have passed steps=100 . Finally, we discount and normalize the rewards and take an optimization step on the policy gradient loss. rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() When calling replay.reward() , replay.done() , or replay.log_prob() , the experience replay will concatenate the corresponding attribute across all of its transitions and return a new tensor . This means that this operation is rather expensive (you should cache it when possible) and that modifying this tensor does not modify the corresponding transitions in replay . Note that in this case log_prob is a custom attribute which is not declared in the original implementation of ExperienceReplay , and we could have given it any name by changing the dictionary key in get_action() .","title":"Implementing Policy Gradient"},{"location":"tutorials/getting_started/#conclusion","text":"You should now be able to use cherry in your own work. For more information, have a look at the documentation , the other tutorials , or the numerous examples . Since one of the characteristics of cherry is to avoid providing \"pre-baked\" algorithms, we tried our best to heavily document its usage.","title":"Conclusion"},{"location":"tutorials/recurrent_a2c/","text":"Recurrent Policy Gradients with A2C Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"},{"location":"tutorials/recurrent_a2c/#recurrent-policy-gradients-with-a2c","text":"Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"}]}