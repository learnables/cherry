{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cherry is a reinforcement learning framework for researchers built on top of PyTorch. Unlike other reinforcement learning implementations, cherry doesn't implement a single monolithic interface to existing algorithms. Instead, it provides you with low-level, common tools to write your own algorithms. Drawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible. So if you don't like a specific tool, you don\u2019t need to use it. Features Cherry extends PyTorch with only a handful of new core concepts. PyTorch modules for reinforcement learning: cherry.nn.Policy : base class for \\pi(a \\mid s) policies. cherry.nn.ActionValue : base class for Q(s, a) action-value functions. Data structures for reinforcement learning compatible with PyTorch: cherry.Transition : namedtuple to store (s_t, a_t, r_t, s_{t+1}) transitions (and more). cherry.ExperienceReplay : a list-like buffer to store and sample transitions. Low-level interface \u00e0 la PyTorch to write and debug your algorithms. cherry.td.* and cherry.pg.* : temporal difference and policy gradient utilities. cherry.algorithms.* : helper functions for popular algorithms ( PPO , TD3 , DrQ , and more ). cherry.debug.* and cherry.plot.* : logging, visualization, and debugging tools. To learn more about the tools and philosophy behind cherry, check out our Getting Started tutorial . Overview and Examples \u00b6 The following snippet showcases a few of the tools offered by cherry. Many more high-quality examples are available in the examples/ folder. Defining a cherry.nn.Policy \u00b6 class VisionPolicy ( cherry . nn . Policy ): # inherits from torch.nn.Module def __init__ ( self , feature_extractor , actor ): super ( VisionGaussianPolicy , self ) . __init__ () self . feature_extractor = feature_extractor self . actor = actor def forward ( self , obs ): mean = self . actor ( self . feature_extractor ( obs )) std = 0.1 * torch . ones_like ( mean ) return cherry . distributions . TanhNormal ( mean , std ) # policies always return a distribution policy = VisionPolicy ( MyResnetExtractor (), MyMLPActor ()) action = policy . act ( obs ) # sampled from policy's distribution deterministic_action = policy . act ( obs , deterministic = True ) # distribution's mode action_distribution = policy ( obs ) # work with the policy's distribution Building a cherry.ExperienceReplay of cherry.Transition \u00b6 # building the replay replay = cherry . ExperienceReplay () state = env . reset () for t in range ( 1000 ): action = policy . act ( state ) next_state , reward , done , info = env . step ( action ) replay . append ( state , action , reward , next_state , done ) next_state = state # manipulating the replay replay = replay [ - 256 :] # indexes like a list batch = replay . sample ( 32 , contiguous = True ) # sample transitions into a replay batch = batch . to ( 'cuda' ) # move replay to device for transition in reversed ( batch ): # iterate over a replay transition . reward *= 0.99 # get all states, actions, and rewards as PyTorch tensors. reinforce_loss = - torch . sum ( policy ( batch . state ()) . log_prob ( batch . action ()) * batch . reward ()) Designing algorithms with cherry.td , cherry.pg , and cherry.algorithms \u00b6 # defining a new algorithm @dataclasses . dataclass class MyA2C : discount : float = 0.99 def update ( self , replay , policy , state_value , optimizer ): # discount rewards values = state_value ( replay . action ()) discounted_rewards = cherry . td . discount ( self . discount , replay . reward (), replay . done (), bootstrap = values [ - 1 ] . detach () ) # Compute losses policy_loss = cherry . algorithms . A2C . policy_loss ( log_probs = policy ( replay . state ()) . log_prob ( replay . action ()), advantages = discounted_rewards - values . detach (), ) value_loss = cherry . algorithms . A2C . state_value_loss ( values , discounted_rewards ) # Optimization step optimizer . zero_grad () ( policy_loss + value_loss ) . backward () optimizer . step () return { 'a2c/policy_loss' : policy_loss , 'a2c/value_loss' : value_loss } # using MyA2C my_a2c = MyA2C ( discount = 0.95 ) my_policy = MyPolicy () linear_value = cherry . models . LinearValue ( 128 ) adam = torch . optim . Adam ( policy . parameters ()) for step in range ( 1000 ): replay = collect_experience ( policy ) my_a2c . update ( replay , my_policy , linear_value , adam ) Install \u00b6 pip install cherry-rl Changelog \u00b6 A human-readable changelog is available in the CHANGELOG.md file. Documentation \u00b6 Documentation and tutorials are available on cherry\u2019s website: http://cherry-rl.net . Contributing \u00b6 Here are a couple of guidelines we strive to follow. It's always a good idea to open an issue first, where we can discuss how to best proceed. If you want to contribute a new example using cherry, it would preferably stand in a single file. If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: it allows for automatic testing, it ensures that the functionality is correctly implemented, it shows users how to use your functionality, and it gives a concrete example when discussing the best way to merge your implementation. Acknowledgements \u00b6 Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , Kai Arulkumaran's implementations , RLLab / Garage . Why 'cherry' ? \u00b6 Because it's the sweetest part of the cake .","title":"Home"},{"location":"#overview-and-examples","text":"The following snippet showcases a few of the tools offered by cherry. Many more high-quality examples are available in the examples/ folder.","title":"Overview and Examples"},{"location":"#defining-a-cherrynnpolicy","text":"class VisionPolicy ( cherry . nn . Policy ): # inherits from torch.nn.Module def __init__ ( self , feature_extractor , actor ): super ( VisionGaussianPolicy , self ) . __init__ () self . feature_extractor = feature_extractor self . actor = actor def forward ( self , obs ): mean = self . actor ( self . feature_extractor ( obs )) std = 0.1 * torch . ones_like ( mean ) return cherry . distributions . TanhNormal ( mean , std ) # policies always return a distribution policy = VisionPolicy ( MyResnetExtractor (), MyMLPActor ()) action = policy . act ( obs ) # sampled from policy's distribution deterministic_action = policy . act ( obs , deterministic = True ) # distribution's mode action_distribution = policy ( obs ) # work with the policy's distribution","title":"Defining a cherry.nn.Policy"},{"location":"#building-a-cherryexperiencereplay-of-cherrytransition","text":"# building the replay replay = cherry . ExperienceReplay () state = env . reset () for t in range ( 1000 ): action = policy . act ( state ) next_state , reward , done , info = env . step ( action ) replay . append ( state , action , reward , next_state , done ) next_state = state # manipulating the replay replay = replay [ - 256 :] # indexes like a list batch = replay . sample ( 32 , contiguous = True ) # sample transitions into a replay batch = batch . to ( 'cuda' ) # move replay to device for transition in reversed ( batch ): # iterate over a replay transition . reward *= 0.99 # get all states, actions, and rewards as PyTorch tensors. reinforce_loss = - torch . sum ( policy ( batch . state ()) . log_prob ( batch . action ()) * batch . reward ())","title":"Building a cherry.ExperienceReplay of cherry.Transition"},{"location":"#designing-algorithms-with-cherrytd-cherrypg-and-cherryalgorithms","text":"# defining a new algorithm @dataclasses . dataclass class MyA2C : discount : float = 0.99 def update ( self , replay , policy , state_value , optimizer ): # discount rewards values = state_value ( replay . action ()) discounted_rewards = cherry . td . discount ( self . discount , replay . reward (), replay . done (), bootstrap = values [ - 1 ] . detach () ) # Compute losses policy_loss = cherry . algorithms . A2C . policy_loss ( log_probs = policy ( replay . state ()) . log_prob ( replay . action ()), advantages = discounted_rewards - values . detach (), ) value_loss = cherry . algorithms . A2C . state_value_loss ( values , discounted_rewards ) # Optimization step optimizer . zero_grad () ( policy_loss + value_loss ) . backward () optimizer . step () return { 'a2c/policy_loss' : policy_loss , 'a2c/value_loss' : value_loss } # using MyA2C my_a2c = MyA2C ( discount = 0.95 ) my_policy = MyPolicy () linear_value = cherry . models . LinearValue ( 128 ) adam = torch . optim . Adam ( policy . parameters ()) for step in range ( 1000 ): replay = collect_experience ( policy ) my_a2c . update ( replay , my_policy , linear_value , adam )","title":"Designing algorithms with cherry.td, cherry.pg, and cherry.algorithms"},{"location":"#install","text":"pip install cherry-rl","title":"Install"},{"location":"#changelog","text":"A human-readable changelog is available in the CHANGELOG.md file.","title":"Changelog"},{"location":"#documentation","text":"Documentation and tutorials are available on cherry\u2019s website: http://cherry-rl.net .","title":"Documentation"},{"location":"#contributing","text":"Here are a couple of guidelines we strive to follow. It's always a good idea to open an issue first, where we can discuss how to best proceed. If you want to contribute a new example using cherry, it would preferably stand in a single file. If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: it allows for automatic testing, it ensures that the functionality is correctly implemented, it shows users how to use your functionality, and it gives a concrete example when discussing the best way to merge your implementation.","title":"Contributing"},{"location":"#acknowledgements","text":"Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , Kai Arulkumaran's implementations , RLLab / Garage .","title":"Acknowledgements"},{"location":"#why-cherry","text":"Because it's the sweetest part of the cake .","title":"Why 'cherry' ?"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 Added \u00b6 Changed \u00b6 Fixed \u00b6 v0.2.0 \u00b6 Added \u00b6 Introduce cherry.nn.Policy, cherry.nn.ActionValue, and cherry.nn.StateValue. Algorithm class utilities for: A2C, PPO, TRPO, DDPG, TD3, SAC, and DrQ/DrQv2. DMC examples for SAC, DrQ, and DrQv2. N-steps returns sampling in ExperienceReplay. Changed \u00b6 Discontinue most of cherry.wrappers. Fixed \u00b6 Fixes return value of StateNormalizer and RewardNormalizer wrappers. Requirements to generate docs. v0.1.4 \u00b6 Fixed \u00b6 Support for torch 1.5 and new _parse_to behavior in ExperienceReplay. (thanks @ManifoldFR) v0.1.3 \u00b6 Added \u00b6 A CHANGELOG.md file. Changed \u00b6 Travis testing with different versions of Python (3.6, 3.7), torch (1.1, 1.2, 1.3, 1.4), and torchvision (0.3, 0.4, 0.5). Fixed \u00b6 fix bug in torch_wrapper when use GPU by callling Tensor.cpu().detach().numpy() to convert CUDA tensor to numpy.(@walkacross) Bugfix when using td.discount with replays coming from vectorized environments (@galatolofederico) env.action_size and env.state_size when the number of vectorized environments is 1. (thanks @galatolofederico) Actor-critic integration test being to finicky. cherry.onehot support for numpy's float and integer types. (thanks @ngoby)","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"","title":"Changed"},{"location":"changelog/#fixed","text":"","title":"Fixed"},{"location":"changelog/#v020","text":"","title":"v0.2.0"},{"location":"changelog/#added_1","text":"Introduce cherry.nn.Policy, cherry.nn.ActionValue, and cherry.nn.StateValue. Algorithm class utilities for: A2C, PPO, TRPO, DDPG, TD3, SAC, and DrQ/DrQv2. DMC examples for SAC, DrQ, and DrQv2. N-steps returns sampling in ExperienceReplay.","title":"Added"},{"location":"changelog/#changed_1","text":"Discontinue most of cherry.wrappers.","title":"Changed"},{"location":"changelog/#fixed_1","text":"Fixes return value of StateNormalizer and RewardNormalizer wrappers. Requirements to generate docs.","title":"Fixed"},{"location":"changelog/#v014","text":"","title":"v0.1.4"},{"location":"changelog/#fixed_2","text":"Support for torch 1.5 and new _parse_to behavior in ExperienceReplay. (thanks @ManifoldFR)","title":"Fixed"},{"location":"changelog/#v013","text":"","title":"v0.1.3"},{"location":"changelog/#added_2","text":"A CHANGELOG.md file.","title":"Added"},{"location":"changelog/#changed_2","text":"Travis testing with different versions of Python (3.6, 3.7), torch (1.1, 1.2, 1.3, 1.4), and torchvision (0.3, 0.4, 0.5).","title":"Changed"},{"location":"changelog/#fixed_3","text":"fix bug in torch_wrapper when use GPU by callling Tensor.cpu().detach().numpy() to convert CUDA tensor to numpy.(@walkacross) Bugfix when using td.discount with replays coming from vectorized environments (@galatolofederico) env.action_size and env.state_size when the number of vectorized environments is 1. (thanks @galatolofederico) Actor-critic integration test being to finicky. cherry.onehot support for numpy's float and integer types. (thanks @ngoby)","title":"Fixed"},{"location":"_build/pydocmd/","text":"Cherry is a reinforcement learning framework for researchers built on top of PyTorch. Unlike other reinforcement learning implementations, cherry doesn't implement a single monolithic interface to existing algorithms. Instead, it provides you with low-level, common tools to write your own algorithms. Drawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible. So if you don't like a specific tool, you don\u2019t need to use it. Features Pythonic and low-level interface \u00e0 la Pytorch. Support for tabular (!) and function approximation algorithms. Various OpenAI Gym environment wrappers. Helper functions for popular algorithms. (e.g. A2C, DDPG, TRPO, PPO, SAC) Logging, visualization, and debugging tools. Painless and efficient distributed training on CPUs and GPUs. Unit, integration, and regression tested, continuously integrated. To learn more about the tools and philosophy behind cherry, check out our Getting Started tutorial . Example \u00b6 The following snippet showcases some of the tools offered by cherry. import cherry as ch # Wrap environments env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) policy = PolicyNet () optimizer = optim . Adam ( policy . parameters (), lr = 1e-2 ) replay = ch . ExperienceReplay () # Manage transitions for step in range ( 1000 ): state = env . reset () while True : mass = Categorical ( policy ( state )) action = mass . sample () log_prob = mass . log_prob ( action ) next_state , reward , done , _ = env . step ( action ) # Build the ExperienceReplay replay . append ( state , action , reward , next_state , done , log_prob = log_prob ) if done : break else : state = next_state # Discounting and normalizing rewards rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () replay . empty () Many more high-quality examples are available in the examples/ folder. Installation \u00b6 Note Cherry is considered in early alpha release. Stuff might break. pip install cherry-rl Changelog \u00b6 A human-readable changelog is available in the CHANGELOG.md file. Documentation \u00b6 Documentation and tutorials are available on cherry\u2019s website: http://cherry-rl.net . Contributing \u00b6 First, thanks for your consideration in contributing to cherry. Here are a couple of guidelines we strive to follow. It's always a good idea to open an issue first, where we can discuss how to best proceed. If you want to contribute a new example using cherry, it would preferably stand in a single file. If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: it allows for automatic testing, it ensures that the functionality is correctly implemented, it shows users how to use your functionality, and it gives a concrete example when discussing the best way to merge your implementation. We don't have forums, but are happy to discuss with you on slack. Make sure to send an email to smr.arnold@gmail.com to get an invite. Acknowledgements \u00b6 Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , Kai Arulkumaran's implementations , RLLab / Garage . Why 'cherry' ? \u00b6 Because it's the sweetest part of the cake .","title":"Index"},{"location":"_build/pydocmd/#example","text":"The following snippet showcases some of the tools offered by cherry. import cherry as ch # Wrap environments env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) policy = PolicyNet () optimizer = optim . Adam ( policy . parameters (), lr = 1e-2 ) replay = ch . ExperienceReplay () # Manage transitions for step in range ( 1000 ): state = env . reset () while True : mass = Categorical ( policy ( state )) action = mass . sample () log_prob = mass . log_prob ( action ) next_state , reward , done , _ = env . step ( action ) # Build the ExperienceReplay replay . append ( state , action , reward , next_state , done , log_prob = log_prob ) if done : break else : state = next_state # Discounting and normalizing rewards rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () replay . empty () Many more high-quality examples are available in the examples/ folder.","title":"Example"},{"location":"_build/pydocmd/#installation","text":"Note Cherry is considered in early alpha release. Stuff might break. pip install cherry-rl","title":"Installation"},{"location":"_build/pydocmd/#changelog","text":"A human-readable changelog is available in the CHANGELOG.md file.","title":"Changelog"},{"location":"_build/pydocmd/#documentation","text":"Documentation and tutorials are available on cherry\u2019s website: http://cherry-rl.net .","title":"Documentation"},{"location":"_build/pydocmd/#contributing","text":"First, thanks for your consideration in contributing to cherry. Here are a couple of guidelines we strive to follow. It's always a good idea to open an issue first, where we can discuss how to best proceed. If you want to contribute a new example using cherry, it would preferably stand in a single file. If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: it allows for automatic testing, it ensures that the functionality is correctly implemented, it shows users how to use your functionality, and it gives a concrete example when discussing the best way to merge your implementation. We don't have forums, but are happy to discuss with you on slack. Make sure to send an email to smr.arnold@gmail.com to get an invite.","title":"Contributing"},{"location":"_build/pydocmd/#acknowledgements","text":"Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , Kai Arulkumaran's implementations , RLLab / Garage .","title":"Acknowledgements"},{"location":"_build/pydocmd/#why-cherry","text":"Because it's the sweetest part of the cake .","title":"Why 'cherry' ?"},{"location":"_build/pydocmd/docs/cherry.algorithms/","text":"cherry.algorithms.a2c \u00b6 Description Helper functions for implementing A2C. A2C simply computes the gradient of the policy as follows: \\mathbb{E} \\left[ (Q(s, a) - V(s)) \\cdot \\nabla_\\theta \\log \\pi_\\theta (a \\vert s) \\right]. policy_loss \u00b6 policy_loss ( log_probs , advantages ) [Source] Description The policy loss of the Advantage Actor-Critic. This function simply performs an element-wise multiplication and a mean reduction. References Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments log_probs (tensor) - Log-density of the selected actions. advantages (tensor) - Advantage of the action-state pairs. Returns (tensor) - The policy loss for the given arguments. Example advantages = replay . advantage () log_probs = replay . log_prob () loss = a2c . policy_loss ( log_probs , advantages ) state_value_loss \u00b6 state_value_loss ( values , rewards ) [Source] Description The state-value loss of the Advantage Actor-Critic. This function is equivalent to a MSELoss. References Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments values (tensor) - Predicted values for some states. rewards (tensor) - Observed rewards for those states. Returns (tensor) - The value loss for the given arguments. Example values = replay . value () rewards = replay . reward () loss = a2c . state_value_loss ( values , rewards ) cherry.algorithms.ppo \u00b6 Description Helper functions for implementing PPO. policy_loss \u00b6 policy_loss ( new_log_probs , old_log_probs , advantages , clip = 0.1 ) [Source] Description The clipped policy loss of Proximal Policy Optimization. References Schulman et al. 2017. \u201cProximal Policy Optimization Algorithms.\u201d arXiv [cs.LG]. Arguments new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. clip (float, optional , default=0.1) - The clipping coefficient. Returns (tensor) - The clipped policy loss for the given arguments. Example advantage = ch . pg . generalized_advantage ( GAMMA , TAU , replay . reward (), replay . done (), replay . value (), next_state_value ) new_densities = policy ( replay . state ()) new_logprobs = new_densities . log_prob ( replay . action ()) loss = policy_loss ( new_logprobs , replay . logprob () . detach (), advantage . detach (), clip = 0.2 ) state_value_loss \u00b6 state_value_loss ( new_values , old_values , rewards , clip = 0.1 ) [Source] Description The clipped state-value loss of Proximal Policy Optimization. References PPO paper Arguments new_values (tensor) - State values from the optimized value function. old_values (tensor) - State values from the reference value function. rewards (tensor) - Observed rewards. clip (float, optional , default=0.1) - The clipping coefficient. Returns (tensor) - The clipped value loss for the given arguments. Example values = v_function ( batch . state ()) value_loss = ppo . state_value_loss ( values , batch . value () . detach (), batch . reward (), clip = 0.2 ) cherry.algorithms.trpo \u00b6 Description Helper functions for implementing Trust-Region Policy Optimization. Recall that TRPO strives to solve the following objective: \\max_\\theta \\quad \\mathbb{E}\\left[ \\frac{\\pi_\\theta}{\\pi_\\text{old}} \\cdot A \\right] \\\\ \\text{subject to} \\quad D_\\text{KL}(\\pi_\\text{old} \\vert \\vert \\pi_\\theta) \\leq \\delta. policy_loss \u00b6 policy_loss ( new_log_probs , old_log_probs , advantages ) [Source] Description The policy loss for Trust-Region Policy Optimization. This is also known as the surrogate loss. References Schulman et al. 2015. \u201cTrust Region Policy Optimization.\u201d ICML 2015. Arguments new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. Returns (tensor) - The policy loss for the given arguments. Example advantage = ch . pg . generalized_advantage ( GAMMA , TAU , replay . reward (), replay . done (), replay . value (), next_state_value ) new_densities = policy ( replay . state ()) new_logprobs = new_densities . log_prob ( replay . action ()) loss = policy_loss ( new_logprobs , replay . logprob () . detach (), advantage . detach ()) hessian_vector_product \u00b6 hessian_vector_product ( loss , parameters , damping = 1e-05 ) [Source] Description Returns a callable that computes the product of the Hessian of loss (w.r.t. parameters) with another vector, using Pearlmutter's trick. Note that parameters and the argument of the callable can be tensors or list of tensors. References Pearlmutter, B. A. 1994. \u201cFast Exact Multiplication by the Hessian.\u201d Neural Computation. Arguments loss (tensor) - The loss of which to compute the Hessian. parameters (tensor or list) - The tensors to take the gradient with respect to. damping (float, optional , default=1e-5) - Damping of the Hessian-vector product. Returns hvp(other) (callable) - A function to compute the Hessian-vector product, given a vector or list other . Example pass conjugate_gradient \u00b6 conjugate_gradient ( Ax , b , num_iterations = 10 , tol = 1e-10 , eps = 1e-08 ) [Source] Description Computes x = A^{-1}b using the conjugate gradient algorithm. Credit Adapted from Kai Arulkumaran's implementation, with additions inspired from John Schulman's implementation. References Nocedal and Wright. 2006. \"Numerical Optimization, 2nd edition\". Springer. Shewchuk et al. 1994. \u201cAn Introduction to the Conjugate Gradient Method without the Agonizing Pain.\u201d CMU. Arguments Ax (callable) - Given a vector x, computes A@x. b (tensor or list) - The reference vector. num_iterations (int, optional , default=10) - Number of conjugate gradient iterations. tol (float, optional , default=1e-10) - Tolerance for proposed solution. eps (float, optional , default=1e-8) - Numerical stability constant. Returns x (tensor or list) - The solution to Ax = b, as a list if b is a list else a tensor. Example pass cherry.algorithms.sac \u00b6 Description Helper functions for implementing Soft-Actor Critic. You should update the function approximators according to the following order. Entropy weight update. Action-value update. State-value update. (Optional, c.f. below) Policy update. Note that most recent implementations of SAC omit step 3. above by using the Bellman residual instead of modelling a state-value function. For an example of such implementation refer to this link . policy_loss \u00b6 policy_loss ( log_probs , q_curr , alpha = 1.0 ) [Source] Description The policy loss of the Soft Actor-Critic. New actions are sampled from the target policy, and those are used to compute the Q-values. While we should back-propagate through the Q-values to the policy parameters, we shouldn't use that gradient to optimize the Q parameters. This is often avoided by either using a target Q function, or by zero-ing out the gradients of the Q function parameters. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments log_probs (tensor) - Log-density of the selected actions. q_curr (tensor) - Q-values of state-action pairs. alpha (float, optional , default=1.0) - Entropy weight. Returns (tensor) - The policy loss for the given arguments. Example densities = policy ( batch . state ()) actions = densities . sample () log_probs = densities . log_prob ( actions ) q_curr = q_function ( batch . state (), actions ) loss = policy_loss ( log_probs , q_curr , alpha = 0.1 ) action_value_loss \u00b6 action_value_loss ( value , next_value , rewards , dones , gamma ) [Source] Description The action-value loss of the Soft Actor-Critic. value should be the value of the current state-action pair, estimated via the Q-function. next_value is the expected value of the next state; it can be estimated via a V-function, or alternatively by computing the Q-value of the next observed state-action pair. In the latter case, make sure that the action is sampled according to the current policy, not the one used to gather the data. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments value (tensor) - Action values of the actual transition. next_value (tensor) - State values of the resulting state. rewards (tensor) - Observed rewards of the transition. dones (tensor) - Which states were terminal. gamma (float) - Discount factor. Returns (tensor) - The policy loss for the given arguments. Example value = qf ( batch . state (), batch . action () . detach ()) next_value = targe_vf ( batch . next_state ()) loss = action_value_loss ( value , next_value , batch . reward (), batch . done (), gamma = 0.99 ) state_value_loss \u00b6 state_value_loss ( v_value , log_probs , q_value , alpha = 1.0 ) [Source] Description The state-value loss of the Soft Actor-Critic. This update is computed \"on-policy\": states are sampled from a replay but the state values, action values, and log-densities are computed using the current value functions and policy. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments v_value (tensor) - State values for some observed states. log_probs (tensor) - Log-density of actions sampled from the current policy. q_value (tensor) - Action values of the actions for the current policy. alpha (float, optional , default=1.0) - Entropy weight. Returns (tensor) - The state value loss for the given arguments. Example densities = policy ( batch . state ()) actions = densities . sample () log_probs = densities . log_prob ( actions ) q_value = qf ( batch . state (), actions ) v_value = vf ( batch . state ()) loss = state_value_loss ( v_value , log_probs , q_value , alpha = 0.1 ) entropy_weight_loss \u00b6 entropy_weight_loss ( log_alpha , log_probs , target_entropy ) [Source] Description Loss of the entropy weight, to automatically tune it. The target entropy needs to be manually tuned. However, a popular heuristic for TanhNormal policies is to use the negative of the action-space dimensionality. (e.g. -4 when operating the voltage of a quad-rotor.) References Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments log_alpha (tensor) - Log of the entropy weight. log_probs (tensor) - Log-density of policy actions. target_entropy (float) - Target of the entropy value. Returns (tensor) - The state value loss for the given arguments. Example densities = policy ( batch . state ()) actions = densities . sample () log_probs = densities . log_prob ( actions ) target_entropy = - np . prod ( env . action_space . shape ) . item () loss = entropy_weight_loss ( alpha . log (), log_probs , target_entropy )","title":"cherry.algorithms.a2c"},{"location":"_build/pydocmd/docs/cherry.algorithms/#cherryalgorithmsa2c","text":"Description Helper functions for implementing A2C. A2C simply computes the gradient of the policy as follows: \\mathbb{E} \\left[ (Q(s, a) - V(s)) \\cdot \\nabla_\\theta \\log \\pi_\\theta (a \\vert s) \\right].","title":"cherry.algorithms.a2c"},{"location":"_build/pydocmd/docs/cherry.algorithms/#policy_loss","text":"policy_loss ( log_probs , advantages ) [Source] Description The policy loss of the Advantage Actor-Critic. This function simply performs an element-wise multiplication and a mean reduction. References Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments log_probs (tensor) - Log-density of the selected actions. advantages (tensor) - Advantage of the action-state pairs. Returns (tensor) - The policy loss for the given arguments. Example advantages = replay . advantage () log_probs = replay . log_prob () loss = a2c . policy_loss ( log_probs , advantages )","title":"policy_loss"},{"location":"_build/pydocmd/docs/cherry.algorithms/#state_value_loss","text":"state_value_loss ( values , rewards ) [Source] Description The state-value loss of the Advantage Actor-Critic. This function is equivalent to a MSELoss. References Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments values (tensor) - Predicted values for some states. rewards (tensor) - Observed rewards for those states. Returns (tensor) - The value loss for the given arguments. Example values = replay . value () rewards = replay . reward () loss = a2c . state_value_loss ( values , rewards )","title":"state_value_loss"},{"location":"_build/pydocmd/docs/cherry.algorithms/#cherryalgorithmsppo","text":"Description Helper functions for implementing PPO.","title":"cherry.algorithms.ppo"},{"location":"_build/pydocmd/docs/cherry.algorithms/#policy_loss_1","text":"policy_loss ( new_log_probs , old_log_probs , advantages , clip = 0.1 ) [Source] Description The clipped policy loss of Proximal Policy Optimization. References Schulman et al. 2017. \u201cProximal Policy Optimization Algorithms.\u201d arXiv [cs.LG]. Arguments new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. clip (float, optional , default=0.1) - The clipping coefficient. Returns (tensor) - The clipped policy loss for the given arguments. Example advantage = ch . pg . generalized_advantage ( GAMMA , TAU , replay . reward (), replay . done (), replay . value (), next_state_value ) new_densities = policy ( replay . state ()) new_logprobs = new_densities . log_prob ( replay . action ()) loss = policy_loss ( new_logprobs , replay . logprob () . detach (), advantage . detach (), clip = 0.2 )","title":"policy_loss"},{"location":"_build/pydocmd/docs/cherry.algorithms/#state_value_loss_1","text":"state_value_loss ( new_values , old_values , rewards , clip = 0.1 ) [Source] Description The clipped state-value loss of Proximal Policy Optimization. References PPO paper Arguments new_values (tensor) - State values from the optimized value function. old_values (tensor) - State values from the reference value function. rewards (tensor) - Observed rewards. clip (float, optional , default=0.1) - The clipping coefficient. Returns (tensor) - The clipped value loss for the given arguments. Example values = v_function ( batch . state ()) value_loss = ppo . state_value_loss ( values , batch . value () . detach (), batch . reward (), clip = 0.2 )","title":"state_value_loss"},{"location":"_build/pydocmd/docs/cherry.algorithms/#cherryalgorithmstrpo","text":"Description Helper functions for implementing Trust-Region Policy Optimization. Recall that TRPO strives to solve the following objective: \\max_\\theta \\quad \\mathbb{E}\\left[ \\frac{\\pi_\\theta}{\\pi_\\text{old}} \\cdot A \\right] \\\\ \\text{subject to} \\quad D_\\text{KL}(\\pi_\\text{old} \\vert \\vert \\pi_\\theta) \\leq \\delta.","title":"cherry.algorithms.trpo"},{"location":"_build/pydocmd/docs/cherry.algorithms/#policy_loss_2","text":"policy_loss ( new_log_probs , old_log_probs , advantages ) [Source] Description The policy loss for Trust-Region Policy Optimization. This is also known as the surrogate loss. References Schulman et al. 2015. \u201cTrust Region Policy Optimization.\u201d ICML 2015. Arguments new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. Returns (tensor) - The policy loss for the given arguments. Example advantage = ch . pg . generalized_advantage ( GAMMA , TAU , replay . reward (), replay . done (), replay . value (), next_state_value ) new_densities = policy ( replay . state ()) new_logprobs = new_densities . log_prob ( replay . action ()) loss = policy_loss ( new_logprobs , replay . logprob () . detach (), advantage . detach ())","title":"policy_loss"},{"location":"_build/pydocmd/docs/cherry.algorithms/#hessian_vector_product","text":"hessian_vector_product ( loss , parameters , damping = 1e-05 ) [Source] Description Returns a callable that computes the product of the Hessian of loss (w.r.t. parameters) with another vector, using Pearlmutter's trick. Note that parameters and the argument of the callable can be tensors or list of tensors. References Pearlmutter, B. A. 1994. \u201cFast Exact Multiplication by the Hessian.\u201d Neural Computation. Arguments loss (tensor) - The loss of which to compute the Hessian. parameters (tensor or list) - The tensors to take the gradient with respect to. damping (float, optional , default=1e-5) - Damping of the Hessian-vector product. Returns hvp(other) (callable) - A function to compute the Hessian-vector product, given a vector or list other . Example pass","title":"hessian_vector_product"},{"location":"_build/pydocmd/docs/cherry.algorithms/#conjugate_gradient","text":"conjugate_gradient ( Ax , b , num_iterations = 10 , tol = 1e-10 , eps = 1e-08 ) [Source] Description Computes x = A^{-1}b using the conjugate gradient algorithm. Credit Adapted from Kai Arulkumaran's implementation, with additions inspired from John Schulman's implementation. References Nocedal and Wright. 2006. \"Numerical Optimization, 2nd edition\". Springer. Shewchuk et al. 1994. \u201cAn Introduction to the Conjugate Gradient Method without the Agonizing Pain.\u201d CMU. Arguments Ax (callable) - Given a vector x, computes A@x. b (tensor or list) - The reference vector. num_iterations (int, optional , default=10) - Number of conjugate gradient iterations. tol (float, optional , default=1e-10) - Tolerance for proposed solution. eps (float, optional , default=1e-8) - Numerical stability constant. Returns x (tensor or list) - The solution to Ax = b, as a list if b is a list else a tensor. Example pass","title":"conjugate_gradient"},{"location":"_build/pydocmd/docs/cherry.algorithms/#cherryalgorithmssac","text":"Description Helper functions for implementing Soft-Actor Critic. You should update the function approximators according to the following order. Entropy weight update. Action-value update. State-value update. (Optional, c.f. below) Policy update. Note that most recent implementations of SAC omit step 3. above by using the Bellman residual instead of modelling a state-value function. For an example of such implementation refer to this link .","title":"cherry.algorithms.sac"},{"location":"_build/pydocmd/docs/cherry.algorithms/#policy_loss_3","text":"policy_loss ( log_probs , q_curr , alpha = 1.0 ) [Source] Description The policy loss of the Soft Actor-Critic. New actions are sampled from the target policy, and those are used to compute the Q-values. While we should back-propagate through the Q-values to the policy parameters, we shouldn't use that gradient to optimize the Q parameters. This is often avoided by either using a target Q function, or by zero-ing out the gradients of the Q function parameters. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments log_probs (tensor) - Log-density of the selected actions. q_curr (tensor) - Q-values of state-action pairs. alpha (float, optional , default=1.0) - Entropy weight. Returns (tensor) - The policy loss for the given arguments. Example densities = policy ( batch . state ()) actions = densities . sample () log_probs = densities . log_prob ( actions ) q_curr = q_function ( batch . state (), actions ) loss = policy_loss ( log_probs , q_curr , alpha = 0.1 )","title":"policy_loss"},{"location":"_build/pydocmd/docs/cherry.algorithms/#action_value_loss","text":"action_value_loss ( value , next_value , rewards , dones , gamma ) [Source] Description The action-value loss of the Soft Actor-Critic. value should be the value of the current state-action pair, estimated via the Q-function. next_value is the expected value of the next state; it can be estimated via a V-function, or alternatively by computing the Q-value of the next observed state-action pair. In the latter case, make sure that the action is sampled according to the current policy, not the one used to gather the data. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments value (tensor) - Action values of the actual transition. next_value (tensor) - State values of the resulting state. rewards (tensor) - Observed rewards of the transition. dones (tensor) - Which states were terminal. gamma (float) - Discount factor. Returns (tensor) - The policy loss for the given arguments. Example value = qf ( batch . state (), batch . action () . detach ()) next_value = targe_vf ( batch . next_state ()) loss = action_value_loss ( value , next_value , batch . reward (), batch . done (), gamma = 0.99 )","title":"action_value_loss"},{"location":"_build/pydocmd/docs/cherry.algorithms/#state_value_loss_2","text":"state_value_loss ( v_value , log_probs , q_value , alpha = 1.0 ) [Source] Description The state-value loss of the Soft Actor-Critic. This update is computed \"on-policy\": states are sampled from a replay but the state values, action values, and log-densities are computed using the current value functions and policy. References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments v_value (tensor) - State values for some observed states. log_probs (tensor) - Log-density of actions sampled from the current policy. q_value (tensor) - Action values of the actions for the current policy. alpha (float, optional , default=1.0) - Entropy weight. Returns (tensor) - The state value loss for the given arguments. Example densities = policy ( batch . state ()) actions = densities . sample () log_probs = densities . log_prob ( actions ) q_value = qf ( batch . state (), actions ) v_value = vf ( batch . state ()) loss = state_value_loss ( v_value , log_probs , q_value , alpha = 0.1 )","title":"state_value_loss"},{"location":"_build/pydocmd/docs/cherry.algorithms/#entropy_weight_loss","text":"entropy_weight_loss ( log_alpha , log_probs , target_entropy ) [Source] Description Loss of the entropy weight, to automatically tune it. The target entropy needs to be manually tuned. However, a popular heuristic for TanhNormal policies is to use the negative of the action-space dimensionality. (e.g. -4 when operating the voltage of a quad-rotor.) References Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments log_alpha (tensor) - Log of the entropy weight. log_probs (tensor) - Log-density of policy actions. target_entropy (float) - Target of the entropy value. Returns (tensor) - The state value loss for the given arguments. Example densities = policy ( batch . state ()) actions = densities . sample () log_probs = densities . log_prob ( actions ) target_entropy = - np . prod ( env . action_space . shape ) . item () loss = entropy_weight_loss ( alpha . log (), log_probs , target_entropy )","title":"entropy_weight_loss"},{"location":"_build/pydocmd/docs/cherry.debug/","text":"cherry.debug \u00b6 General debugging utilities. debug \u00b6 debug ( log_dir = './' ) Enables some debugging utilities for logging and pdb. Includes: Automatically dropping into a post-mortem pdb debugger session whenever an exception is raised. Enables fast DEBUG logging to a logging file via QueueHandler. Copies all stdout output to the logging file. (Experimental) References Automatically start the debugger on an exception (Python recipe), Thomas Heller, 2001, Link Dealing with handlers that block, Python Documentation, 2019. Link Arguments log_dir (str, optional , Default: './') - Location to store the log files. Example ch . debug . debug () raise Exception ( 'My exception' ) -> raise ( 'My exception' ) ( Pdb )","title":"cherry.debug"},{"location":"_build/pydocmd/docs/cherry.debug/#cherrydebug","text":"General debugging utilities.","title":"cherry.debug"},{"location":"_build/pydocmd/docs/cherry.debug/#debug","text":"debug ( log_dir = './' ) Enables some debugging utilities for logging and pdb. Includes: Automatically dropping into a post-mortem pdb debugger session whenever an exception is raised. Enables fast DEBUG logging to a logging file via QueueHandler. Copies all stdout output to the logging file. (Experimental) References Automatically start the debugger on an exception (Python recipe), Thomas Heller, 2001, Link Dealing with handlers that block, Python Documentation, 2019. Link Arguments log_dir (str, optional , Default: './') - Location to store the log files. Example ch . debug . debug () raise Exception ( 'My exception' ) -> raise ( 'My exception' ) ( Pdb )","title":"debug"},{"location":"_build/pydocmd/docs/cherry.distributions/","text":"cherry.distributions \u00b6 Description A set of common distributions. Reparameterization \u00b6 Reparameterization ( density ) [Source] Description Unifies interface for distributions that support rsample and those that do not. When calling sample() , this class checks whether density has a rsample() member, and defaults to call sample() if it does not. References Kingma and Welling. 2013. \u201cAuto-Encoding Variational Bayes.\u201d arXiv [stat.ML]. Arguments density (Distribution) - The distribution to wrap. Example density = Normal ( mean , std ) reparam = Reparameterization ( density ) sample = reparam . sample () # Uses Normal.rsample() ActionDistribution \u00b6 ActionDistribution ( env , logstd = None , use_probs = False , reparam = False ) [Source] Description A helper module to automatically choose the proper policy distribution, based on the Gym environment action_space . For Discrete action spaces, it uses a Categorical distribution, otherwise it uses a Normal which uses a diagonal covariance matrix. This class enables to write single version policy body that will be compatible with a variety of environments. Arguments env (Environment) - Gym environment for which actions will be sampled. logstd (float/tensor, optional , default=0) - The log standard deviation for the Normal distribution. use_probs (bool, optional , default=False) - Whether to use probabilities or logits for the Categorical case. reparam (bool, optional , default=False) - Whether to use reparameterization in the Normal case. Example env = gym . make ( 'CartPole-v1' ) action_dist = ActionDistribution ( env ) TanhNormal \u00b6 TanhNormal ( normal_mean , normal_std ) [Source] Description Implements a Normal distribution followed by a Tanh, often used with the Soft Actor-Critic. This implementation also exposes sample_and_log_prob and rsample_and_log_prob , which returns both samples and log-densities. The log-densities are computed using the pre-activation values for numerical stability. Credit Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments normal_mean (tensor) - Mean of the Normal distribution. normal_std (tensor) - Standard deviation of the Normal distribution. Example mean = th . zeros ( 5 ) std = th . ones ( 5 ) dist = TanhNormal ( mean , std ) samples = dist . rsample () logprobs = dist . log_prob ( samples ) # Numerically unstable :( samples , logprobs = dist . rsample_and_log_prob () # Stable :)","title":"cherry.distributions"},{"location":"_build/pydocmd/docs/cherry.distributions/#cherrydistributions","text":"Description A set of common distributions.","title":"cherry.distributions"},{"location":"_build/pydocmd/docs/cherry.distributions/#reparameterization","text":"Reparameterization ( density ) [Source] Description Unifies interface for distributions that support rsample and those that do not. When calling sample() , this class checks whether density has a rsample() member, and defaults to call sample() if it does not. References Kingma and Welling. 2013. \u201cAuto-Encoding Variational Bayes.\u201d arXiv [stat.ML]. Arguments density (Distribution) - The distribution to wrap. Example density = Normal ( mean , std ) reparam = Reparameterization ( density ) sample = reparam . sample () # Uses Normal.rsample()","title":"Reparameterization"},{"location":"_build/pydocmd/docs/cherry.distributions/#actiondistribution","text":"ActionDistribution ( env , logstd = None , use_probs = False , reparam = False ) [Source] Description A helper module to automatically choose the proper policy distribution, based on the Gym environment action_space . For Discrete action spaces, it uses a Categorical distribution, otherwise it uses a Normal which uses a diagonal covariance matrix. This class enables to write single version policy body that will be compatible with a variety of environments. Arguments env (Environment) - Gym environment for which actions will be sampled. logstd (float/tensor, optional , default=0) - The log standard deviation for the Normal distribution. use_probs (bool, optional , default=False) - Whether to use probabilities or logits for the Categorical case. reparam (bool, optional , default=False) - Whether to use reparameterization in the Normal case. Example env = gym . make ( 'CartPole-v1' ) action_dist = ActionDistribution ( env )","title":"ActionDistribution"},{"location":"_build/pydocmd/docs/cherry.distributions/#tanhnormal","text":"TanhNormal ( normal_mean , normal_std ) [Source] Description Implements a Normal distribution followed by a Tanh, often used with the Soft Actor-Critic. This implementation also exposes sample_and_log_prob and rsample_and_log_prob , which returns both samples and log-densities. The log-densities are computed using the pre-activation values for numerical stability. Credit Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py References Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Arguments normal_mean (tensor) - Mean of the Normal distribution. normal_std (tensor) - Standard deviation of the Normal distribution. Example mean = th . zeros ( 5 ) std = th . ones ( 5 ) dist = TanhNormal ( mean , std ) samples = dist . rsample () logprobs = dist . log_prob ( samples ) # Numerically unstable :( samples , logprobs = dist . rsample_and_log_prob () # Stable :)","title":"TanhNormal"},{"location":"_build/pydocmd/docs/cherry.envs/","text":"cherry.envs.utils \u00b6 Description Helper functions for OpenAI Gym environments. is_discrete \u00b6 is_discrete ( space , vectorized = False ) Returns whether a space is discrete. Arguments space - The space. vectorized - Whether to return the discreteness for the vectorized environments (True) or just the discreteness of the underlying environment (False). get_space_dimension \u00b6 get_space_dimension ( space , vectorized_dims = False ) Returns the number of elements of a space sample, when unrolled. Arguments space - The space. vectorized_dims - Whether to return the full dimension for vectorized environments (True) or just the dimension for the underlying environment (False). Wrapper \u00b6 Wrapper ( env ) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: 1 2 3 4 env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger. action_size \u00b6 The number of dimensions of a single action. state_size \u00b6 The (flattened) size of a single state. Runner \u00b6 Runner ( env ) Runner wrapper. TODO: When is_vectorized and using episodes=n, use the parallel environmnents to sample n episodes, and stack them inside a flat replay. run \u00b6 Runner . run ( get_action , steps = None , episodes = None , render = False ) Runner wrapper's run method. Logger \u00b6 Logger ( env , interval = 1000 , episode_interval = 10 , title = None , logger = None ) Tracks and prints some common statistics about the environment. Recorder \u00b6 Recorder ( env , directory = './videos/' , format = 'gif' , suffix = None ) [Source] Description Wrapper to record episodes from a rollout. Supports GIF and MP4 encoding. Arguments env (Environment) - Environment to record. directory (str, optional , default='./videos/') - Relative path to where videos will be saved. format (str, optional , default='gif') - Format of the videos. Choose in ['gif', 'mp4'], defaults to gif. If it's text environment, the format will be json. suffix (str, optional , default=None): A unique id used as part of the suffix for the file. By default, uses os.getpid(). Credit Adapted from OpenAI Gym's Monitor wrapper. Example env = gym . make ( 'CartPole-v0' ) env = envs . Recorder ( record_env , './videos/' , format = 'gif' ) env = envs . Runner ( env ) env . run ( get_action , episodes = 3 , render = True ) close \u00b6 Recorder . close () Flush all monitor data to disk and close any open rending windows VisdomLogger \u00b6 VisdomLogger ( env , interval = 1000 , episode_interval = 10 , render = True , title = None , logger = None ) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes. Torch \u00b6 Torch ( env ) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action) Normalizer \u00b6 Normalizer ( env , states = True , rewards = True , clip_states = 10.0 , clip_rewards = 10.0 , gamma = 0.99 , eps = 1e-08 ) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym . make ( 'CartPole-v0' ) env = cherry . envs . Normalizer ( env , states = True , rewards = False ) StateNormalizer \u00b6 StateNormalizer ( env , statistics = None , beta = 0.99 , eps = 1e-08 ) [Source] Description Normalizes the states with a running average. Arguments env (Environment) - Environment to normalize. statistics (dict, optional , default=None) - Dictionary used to bootstrap the normalizing statistics. beta (float, optional , default=0.99) - Moving average weigth. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from Tristan Deleu's implementation. Example env = gym . make ( 'CartPole-v0' ) env = cherry . envs . StateNormalizer ( env ) env2 = gym . make ( 'CartPole-v0' ) env2 = cherry . envs . StateNormalizer ( env2 , statistics = env . statistics ) RewardNormalizer \u00b6 RewardNormalizer ( env , statistics = None , beta = 0.99 , eps = 1e-08 ) [Source] Description Normalizes the rewards with a running average. Arguments env (Environment) - Environment to normalize. statistics (dict, optional , default=None) - Dictionary used to bootstrap the normalizing statistics. beta (float, optional , default=0.99) - Moving average weigth. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from Tristan Deleu's implementation. Example env = gym . make ( 'CartPole-v0' ) env = cherry . envs . RewardNormalizer ( env ) env2 = gym . make ( 'CartPole-v0' ) env2 = cherry . envs . RewardNormalizer ( env2 , statistics = env . statistics ) RewardClipper \u00b6 RewardClipper ( env ) reward \u00b6 RewardClipper . reward ( reward ) Bin reward to {+1, 0, -1} by its sign. Monitor \u00b6 Monitor ( env , directory , * args , ** kwargs ) Sugar coating on top of Gym's Monitor. OpenAIAtari \u00b6 OpenAIAtari ( env ) AddTimestep \u00b6 AddTimestep ( env = None ) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/ StateLambda \u00b6 StateLambda ( env , fn ) ActionLambda \u00b6 ActionLambda ( env , fn ) ActionSpaceScaler \u00b6 ActionSpaceScaler ( env , clip = 1.0 ) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"cherry.envs.utils"},{"location":"_build/pydocmd/docs/cherry.envs/#cherryenvsutils","text":"Description Helper functions for OpenAI Gym environments.","title":"cherry.envs.utils"},{"location":"_build/pydocmd/docs/cherry.envs/#is_discrete","text":"is_discrete ( space , vectorized = False ) Returns whether a space is discrete. Arguments space - The space. vectorized - Whether to return the discreteness for the vectorized environments (True) or just the discreteness of the underlying environment (False).","title":"is_discrete"},{"location":"_build/pydocmd/docs/cherry.envs/#get_space_dimension","text":"get_space_dimension ( space , vectorized_dims = False ) Returns the number of elements of a space sample, when unrolled. Arguments space - The space. vectorized_dims - Whether to return the full dimension for vectorized environments (True) or just the dimension for the underlying environment (False).","title":"get_space_dimension"},{"location":"_build/pydocmd/docs/cherry.envs/#wrapper","text":"Wrapper ( env ) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: 1 2 3 4 env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger.","title":"Wrapper"},{"location":"_build/pydocmd/docs/cherry.envs/#action_size","text":"The number of dimensions of a single action.","title":"action_size"},{"location":"_build/pydocmd/docs/cherry.envs/#state_size","text":"The (flattened) size of a single state.","title":"state_size"},{"location":"_build/pydocmd/docs/cherry.envs/#runner","text":"Runner ( env ) Runner wrapper. TODO: When is_vectorized and using episodes=n, use the parallel environmnents to sample n episodes, and stack them inside a flat replay.","title":"Runner"},{"location":"_build/pydocmd/docs/cherry.envs/#run","text":"Runner . run ( get_action , steps = None , episodes = None , render = False ) Runner wrapper's run method.","title":"run"},{"location":"_build/pydocmd/docs/cherry.envs/#logger","text":"Logger ( env , interval = 1000 , episode_interval = 10 , title = None , logger = None ) Tracks and prints some common statistics about the environment.","title":"Logger"},{"location":"_build/pydocmd/docs/cherry.envs/#recorder","text":"Recorder ( env , directory = './videos/' , format = 'gif' , suffix = None ) [Source] Description Wrapper to record episodes from a rollout. Supports GIF and MP4 encoding. Arguments env (Environment) - Environment to record. directory (str, optional , default='./videos/') - Relative path to where videos will be saved. format (str, optional , default='gif') - Format of the videos. Choose in ['gif', 'mp4'], defaults to gif. If it's text environment, the format will be json. suffix (str, optional , default=None): A unique id used as part of the suffix for the file. By default, uses os.getpid(). Credit Adapted from OpenAI Gym's Monitor wrapper. Example env = gym . make ( 'CartPole-v0' ) env = envs . Recorder ( record_env , './videos/' , format = 'gif' ) env = envs . Runner ( env ) env . run ( get_action , episodes = 3 , render = True )","title":"Recorder"},{"location":"_build/pydocmd/docs/cherry.envs/#close","text":"Recorder . close () Flush all monitor data to disk and close any open rending windows","title":"close"},{"location":"_build/pydocmd/docs/cherry.envs/#visdomlogger","text":"VisdomLogger ( env , interval = 1000 , episode_interval = 10 , render = True , title = None , logger = None ) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes.","title":"VisdomLogger"},{"location":"_build/pydocmd/docs/cherry.envs/#torch","text":"Torch ( env ) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action)","title":"Torch"},{"location":"_build/pydocmd/docs/cherry.envs/#normalizer","text":"Normalizer ( env , states = True , rewards = True , clip_states = 10.0 , clip_rewards = 10.0 , gamma = 0.99 , eps = 1e-08 ) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym . make ( 'CartPole-v0' ) env = cherry . envs . Normalizer ( env , states = True , rewards = False )","title":"Normalizer"},{"location":"_build/pydocmd/docs/cherry.envs/#statenormalizer","text":"StateNormalizer ( env , statistics = None , beta = 0.99 , eps = 1e-08 ) [Source] Description Normalizes the states with a running average. Arguments env (Environment) - Environment to normalize. statistics (dict, optional , default=None) - Dictionary used to bootstrap the normalizing statistics. beta (float, optional , default=0.99) - Moving average weigth. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from Tristan Deleu's implementation. Example env = gym . make ( 'CartPole-v0' ) env = cherry . envs . StateNormalizer ( env ) env2 = gym . make ( 'CartPole-v0' ) env2 = cherry . envs . StateNormalizer ( env2 , statistics = env . statistics )","title":"StateNormalizer"},{"location":"_build/pydocmd/docs/cherry.envs/#rewardnormalizer","text":"RewardNormalizer ( env , statistics = None , beta = 0.99 , eps = 1e-08 ) [Source] Description Normalizes the rewards with a running average. Arguments env (Environment) - Environment to normalize. statistics (dict, optional , default=None) - Dictionary used to bootstrap the normalizing statistics. beta (float, optional , default=0.99) - Moving average weigth. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from Tristan Deleu's implementation. Example env = gym . make ( 'CartPole-v0' ) env = cherry . envs . RewardNormalizer ( env ) env2 = gym . make ( 'CartPole-v0' ) env2 = cherry . envs . RewardNormalizer ( env2 , statistics = env . statistics )","title":"RewardNormalizer"},{"location":"_build/pydocmd/docs/cherry.envs/#rewardclipper","text":"RewardClipper ( env )","title":"RewardClipper"},{"location":"_build/pydocmd/docs/cherry.envs/#reward","text":"RewardClipper . reward ( reward ) Bin reward to {+1, 0, -1} by its sign.","title":"reward"},{"location":"_build/pydocmd/docs/cherry.envs/#monitor","text":"Monitor ( env , directory , * args , ** kwargs ) Sugar coating on top of Gym's Monitor.","title":"Monitor"},{"location":"_build/pydocmd/docs/cherry.envs/#openaiatari","text":"OpenAIAtari ( env )","title":"OpenAIAtari"},{"location":"_build/pydocmd/docs/cherry.envs/#addtimestep","text":"AddTimestep ( env = None ) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/","title":"AddTimestep"},{"location":"_build/pydocmd/docs/cherry.envs/#statelambda","text":"StateLambda ( env , fn )","title":"StateLambda"},{"location":"_build/pydocmd/docs/cherry.envs/#actionlambda","text":"ActionLambda ( env , fn )","title":"ActionLambda"},{"location":"_build/pydocmd/docs/cherry.envs/#actionspacescaler","text":"ActionSpaceScaler ( env , clip = 1.0 ) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"ActionSpaceScaler"},{"location":"_build/pydocmd/docs/cherry/","text":"Transition \u00b6 Transition ( state , action , reward , next_state , done , device = None , ** infos ) Description Represents a (s, a, r, s', d) tuple. All attributes (including the ones in infos) are accessible via transition.name_of_attr . (e.g. transition.log_prob if log_prob is in infos .) Arguments state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example for transition in replay : print ( transition . state ) to \u00b6 Transition . to ( * args , ** kwargs ) Description Moves the constituents of the transition to the desired device, and casts them to the desired format. Note: This is done in-place and doesn't create a new transition. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example sars = Transition ( state , action , reward , next_state ) sars . to ( 'cuda' ) ExperienceReplay \u00b6 ExperienceReplay ( storage = None , device = None ) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch . ExperienceReplay () # Instanciate a new replay replay . append ( state , # Add experience to the replay action , reward , next_state , done , density : action_density , log_prob : action_density . log_prob ( action ), ) replay . state () # Tensor of states replay . action () # Tensor of actions replay . density () # list of action_density replay . log_prob () # Tensor of log_probabilities new_replay = replay [ - 10 :] # Last 10 transitions in new_replay #Sample some previous experience batch = replay . sample ( 32 , contiguous = True ) save \u00b6 ExperienceReplay . save ( path ) Description Serializes and saves the ExperienceReplay into the given path. Arguments path (str) - File path. Example replay . save ( 'my_replay_file.pt' ) load \u00b6 ExperienceReplay . load ( path ) Description Loads data from a serialized ExperienceReplay. Arguments path (str) - File path of serialized ExperienceReplay. Example replay . load ( 'my_replay_file.pt' ) append \u00b6 ExperienceReplay . append ( state = None , action = None , reward = None , next_state = None , done = None , ** infos ) Description Appends new data to the list ExperienceReplay. Arguments state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example replay . append ( state , action , reward , next_state , done , info = { 'density' : density , 'log_prob' : density . log_prob ( action ), }) sample \u00b6 ExperienceReplay . sample ( size = 1 , contiguous = False , episodes = False ) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay - New ExperienceReplay containing the sampled transitions. empty \u00b6 ExperienceReplay . empty () Description Removes all data from an ExperienceReplay. Example replay . empty () to \u00b6 ExperienceReplay . to ( * args , ** kwargs ) Description Calls .to() on all transitions of the experience replay, moving them to the desired device and casting the to the desired format. Note: This return a new experience replay, but the transitions are modified in-place. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example replay . to ( 'cuda:1' ) policy . to ( 'cuda:1' ) for sars in replay : cuda_action = policy ( sars . state ) . sample () totensor \u00b6 totensor ( array , dtype = None ) [Source] Description Converts the argument array to a torch.tensor 1xN, regardless of its type or dimension. Arguments array (int, float, ndarray, tensor) - Data to be converted to array. dtype (dtype, optional , default=None) - Data type to use for representation. By default, uses torch.get_default_dtype() . Returns Tensor of shape 1xN with the appropriate data type. Example array = [ 5 , 6 , 7.0 ] tensor = cherry . totensor ( array , dtype = th . float32 ) array = np . array ( array , dtype = np . float64 ) tensor = cherry . totensor ( array , dtype = th . float16 ) normalize \u00b6 normalize ( tensor , epsilon = 1e-08 ) [Source] Description Normalizes a tensor to have zero mean and unit standard deviation values. Arguments tensor (tensor) - The tensor to normalize. epsilon (float, optional , default=1e-8) - Numerical stability constant for normalization. Returns A new tensor, containing the normalized values. Example tensor = torch . arange ( 23 ) / 255.0 tensor = cherry . normalize ( tensor , epsilon = 1e-3 )","title":"Transition"},{"location":"_build/pydocmd/docs/cherry/#transition","text":"Transition ( state , action , reward , next_state , done , device = None , ** infos ) Description Represents a (s, a, r, s', d) tuple. All attributes (including the ones in infos) are accessible via transition.name_of_attr . (e.g. transition.log_prob if log_prob is in infos .) Arguments state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example for transition in replay : print ( transition . state )","title":"Transition"},{"location":"_build/pydocmd/docs/cherry/#to","text":"Transition . to ( * args , ** kwargs ) Description Moves the constituents of the transition to the desired device, and casts them to the desired format. Note: This is done in-place and doesn't create a new transition. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example sars = Transition ( state , action , reward , next_state ) sars . to ( 'cuda' )","title":"to"},{"location":"_build/pydocmd/docs/cherry/#experiencereplay","text":"ExperienceReplay ( storage = None , device = None ) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch . ExperienceReplay () # Instanciate a new replay replay . append ( state , # Add experience to the replay action , reward , next_state , done , density : action_density , log_prob : action_density . log_prob ( action ), ) replay . state () # Tensor of states replay . action () # Tensor of actions replay . density () # list of action_density replay . log_prob () # Tensor of log_probabilities new_replay = replay [ - 10 :] # Last 10 transitions in new_replay #Sample some previous experience batch = replay . sample ( 32 , contiguous = True )","title":"ExperienceReplay"},{"location":"_build/pydocmd/docs/cherry/#save","text":"ExperienceReplay . save ( path ) Description Serializes and saves the ExperienceReplay into the given path. Arguments path (str) - File path. Example replay . save ( 'my_replay_file.pt' )","title":"save"},{"location":"_build/pydocmd/docs/cherry/#load","text":"ExperienceReplay . load ( path ) Description Loads data from a serialized ExperienceReplay. Arguments path (str) - File path of serialized ExperienceReplay. Example replay . load ( 'my_replay_file.pt' )","title":"load"},{"location":"_build/pydocmd/docs/cherry/#append","text":"ExperienceReplay . append ( state = None , action = None , reward = None , next_state = None , done = None , ** infos ) Description Appends new data to the list ExperienceReplay. Arguments state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example replay . append ( state , action , reward , next_state , done , info = { 'density' : density , 'log_prob' : density . log_prob ( action ), })","title":"append"},{"location":"_build/pydocmd/docs/cherry/#sample","text":"ExperienceReplay . sample ( size = 1 , contiguous = False , episodes = False ) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay - New ExperienceReplay containing the sampled transitions.","title":"sample"},{"location":"_build/pydocmd/docs/cherry/#empty","text":"ExperienceReplay . empty () Description Removes all data from an ExperienceReplay. Example replay . empty ()","title":"empty"},{"location":"_build/pydocmd/docs/cherry/#to_1","text":"ExperienceReplay . to ( * args , ** kwargs ) Description Calls .to() on all transitions of the experience replay, moving them to the desired device and casting the to the desired format. Note: This return a new experience replay, but the transitions are modified in-place. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example replay . to ( 'cuda:1' ) policy . to ( 'cuda:1' ) for sars in replay : cuda_action = policy ( sars . state ) . sample ()","title":"to"},{"location":"_build/pydocmd/docs/cherry/#totensor","text":"totensor ( array , dtype = None ) [Source] Description Converts the argument array to a torch.tensor 1xN, regardless of its type or dimension. Arguments array (int, float, ndarray, tensor) - Data to be converted to array. dtype (dtype, optional , default=None) - Data type to use for representation. By default, uses torch.get_default_dtype() . Returns Tensor of shape 1xN with the appropriate data type. Example array = [ 5 , 6 , 7.0 ] tensor = cherry . totensor ( array , dtype = th . float32 ) array = np . array ( array , dtype = np . float64 ) tensor = cherry . totensor ( array , dtype = th . float16 )","title":"totensor"},{"location":"_build/pydocmd/docs/cherry/#normalize","text":"normalize ( tensor , epsilon = 1e-08 ) [Source] Description Normalizes a tensor to have zero mean and unit standard deviation values. Arguments tensor (tensor) - The tensor to normalize. epsilon (float, optional , default=1e-8) - Numerical stability constant for normalization. Returns A new tensor, containing the normalized values. Example tensor = torch . arange ( 23 ) / 255.0 tensor = cherry . normalize ( tensor , epsilon = 1e-3 )","title":"normalize"},{"location":"_build/pydocmd/docs/cherry.models/","text":"cherry.models.utils \u00b6 RandomPolicy \u00b6 RandomPolicy ( env , * args , ** kwargs ) [Source] Description Policy that randomly samples actions from the environment action space. Arguments env (Environment) - Environment from which to sample actions. Example policy = ch . models . RandomPolicy ( env ) env = envs . Runner ( env ) replay = env . run ( policy , steps = 2048 ) polyak_average \u00b6 polyak_average ( source , target , alpha ) [Source] Description Shifts the parameters of source towards those of target. Note: the parameter alpha indicates the convex combination weight of the source. (i.e. the old parameters are kept at a rate of alpha .) References Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments source (nn.Module) - The module to be shifted. target (nn.Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example target_qf = nn . Linear ( 23 , 34 ) qf = nn . Linear ( 23 , 34 ) ch . models . polyak_average ( target_qf , qf , alpha = 0.9 ) cherry.models.tabular \u00b6 StateValueFunction \u00b6 StateValueFunction ( state_size , init = None ) [Source] Description Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example vf = StateValueFunction ( env . state_size ) state = env . reset () state = ch . onehot ( state , env . state_size ) state_value = vf ( state ) ActionValueFunction \u00b6 ActionValueFunction ( state_size , action_size , init = None ) [Source] Description Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example qf = ActionValueFunction ( env . state_size , env . action_size ) state = env . reset () state = ch . onehot ( state , env . state_size ) all_action_values = qf ( state ) action = ch . onehot ( 0 , env . action_size ) action_value = qf ( state , action ) cherry.models.atari \u00b6 NatureFeatures \u00b6 NatureFeatures ( input_size = 4 , output_size = 512 , hidden_size = 3136 ) [Source] Description The convolutional body of the DQN architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation. hidden_size (int, optional , default=1568) - Size of the representation after the convolutional layers NatureActor \u00b6 NatureActor ( input_size , output_size ) [Source] Description The actor head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space. NatureCritic \u00b6 NatureCritic ( input_size , output_size = 1 ) [Source] Description The critic head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value. cherry.models.robotics \u00b6 RoboticsMLP \u00b6 RoboticsMLP ( input_size , output_size , layer_sizes = None ) [Source] Description A multi-layer perceptron with proper initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example target_qf = ch . models . robotics . RoboticsMLP ( 23 , 34 , layer_sizes = [ 32 , 32 ]) RoboticsActor \u00b6 RoboticsActor ( input_size , output_size , layer_sizes = None ) [Source] Description A multi-layer perceptron with initialization designed for choosing actions in continuous robotic environments. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example policy_mean = ch . models . robotics . Actor ( 28 , 8 , layer_sizes = [ 64 , 32 , 16 ]) LinearValue \u00b6 LinearValue ( input_size , reg = 1e-05 ) [Source] Description A linear state-value function, whose parameters are found by minimizing least-squares. Credit Adapted from Tristan Deleu's implementation. References Duan et al. 2016. \u201cBenchmarking Deep Reinforcement Learning for Continuous Control.\u201d https://github.com/tristandeleu/pytorch-maml-rl Arguments inputs_size (int) - Size of input. reg (float, optional , default=1e-5) - Regularization coefficient. Example states = replay . state () rewards = replay . reward () dones = replay . done () returns = ch . td . discount ( gamma , rewards , dones ) baseline = LinearValue ( input_size ) baseline . fit ( states , returns ) next_values = baseline ( replay . next_states ())","title":"cherry.models.utils"},{"location":"_build/pydocmd/docs/cherry.models/#cherrymodelsutils","text":"","title":"cherry.models.utils"},{"location":"_build/pydocmd/docs/cherry.models/#randompolicy","text":"RandomPolicy ( env , * args , ** kwargs ) [Source] Description Policy that randomly samples actions from the environment action space. Arguments env (Environment) - Environment from which to sample actions. Example policy = ch . models . RandomPolicy ( env ) env = envs . Runner ( env ) replay = env . run ( policy , steps = 2048 )","title":"RandomPolicy"},{"location":"_build/pydocmd/docs/cherry.models/#polyak_average","text":"polyak_average ( source , target , alpha ) [Source] Description Shifts the parameters of source towards those of target. Note: the parameter alpha indicates the convex combination weight of the source. (i.e. the old parameters are kept at a rate of alpha .) References Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments source (nn.Module) - The module to be shifted. target (nn.Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example target_qf = nn . Linear ( 23 , 34 ) qf = nn . Linear ( 23 , 34 ) ch . models . polyak_average ( target_qf , qf , alpha = 0.9 )","title":"polyak_average"},{"location":"_build/pydocmd/docs/cherry.models/#cherrymodelstabular","text":"","title":"cherry.models.tabular"},{"location":"_build/pydocmd/docs/cherry.models/#statevaluefunction","text":"StateValueFunction ( state_size , init = None ) [Source] Description Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example vf = StateValueFunction ( env . state_size ) state = env . reset () state = ch . onehot ( state , env . state_size ) state_value = vf ( state )","title":"StateValueFunction"},{"location":"_build/pydocmd/docs/cherry.models/#actionvaluefunction","text":"ActionValueFunction ( state_size , action_size , init = None ) [Source] Description Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example qf = ActionValueFunction ( env . state_size , env . action_size ) state = env . reset () state = ch . onehot ( state , env . state_size ) all_action_values = qf ( state ) action = ch . onehot ( 0 , env . action_size ) action_value = qf ( state , action )","title":"ActionValueFunction"},{"location":"_build/pydocmd/docs/cherry.models/#cherrymodelsatari","text":"","title":"cherry.models.atari"},{"location":"_build/pydocmd/docs/cherry.models/#naturefeatures","text":"NatureFeatures ( input_size = 4 , output_size = 512 , hidden_size = 3136 ) [Source] Description The convolutional body of the DQN architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation. hidden_size (int, optional , default=1568) - Size of the representation after the convolutional layers","title":"NatureFeatures"},{"location":"_build/pydocmd/docs/cherry.models/#natureactor","text":"NatureActor ( input_size , output_size ) [Source] Description The actor head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space.","title":"NatureActor"},{"location":"_build/pydocmd/docs/cherry.models/#naturecritic","text":"NatureCritic ( input_size , output_size = 1 ) [Source] Description The critic head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value.","title":"NatureCritic"},{"location":"_build/pydocmd/docs/cherry.models/#cherrymodelsrobotics","text":"","title":"cherry.models.robotics"},{"location":"_build/pydocmd/docs/cherry.models/#roboticsmlp","text":"RoboticsMLP ( input_size , output_size , layer_sizes = None ) [Source] Description A multi-layer perceptron with proper initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example target_qf = ch . models . robotics . RoboticsMLP ( 23 , 34 , layer_sizes = [ 32 , 32 ])","title":"RoboticsMLP"},{"location":"_build/pydocmd/docs/cherry.models/#roboticsactor","text":"RoboticsActor ( input_size , output_size , layer_sizes = None ) [Source] Description A multi-layer perceptron with initialization designed for choosing actions in continuous robotic environments. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example policy_mean = ch . models . robotics . Actor ( 28 , 8 , layer_sizes = [ 64 , 32 , 16 ])","title":"RoboticsActor"},{"location":"_build/pydocmd/docs/cherry.models/#linearvalue","text":"LinearValue ( input_size , reg = 1e-05 ) [Source] Description A linear state-value function, whose parameters are found by minimizing least-squares. Credit Adapted from Tristan Deleu's implementation. References Duan et al. 2016. \u201cBenchmarking Deep Reinforcement Learning for Continuous Control.\u201d https://github.com/tristandeleu/pytorch-maml-rl Arguments inputs_size (int) - Size of input. reg (float, optional , default=1e-5) - Regularization coefficient. Example states = replay . state () rewards = replay . reward () dones = replay . done () returns = ch . td . discount ( gamma , rewards , dones ) baseline = LinearValue ( input_size ) baseline . fit ( states , returns ) next_values = baseline ( replay . next_states ())","title":"LinearValue"},{"location":"_build/pydocmd/docs/cherry.nn.init/","text":"cherry.nn.init \u00b6 robotics_init_ \u00b6 robotics_init_ ( module , gain = None ) [Source] Description Default initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments module (nn.Module) - Module to initialize. gain (float, optional , default=sqrt(2.0)) - Gain of orthogonal initialization. Returns Module, whose weight and bias have been modified in-place. Example linear = nn . Linear ( 23 , 5 ) kostrikov_robotics_ ( linear ) atari_init_ \u00b6 atari_init_ ( module , gain = None ) [Source] Description Default initialization for Atari environments. Credit Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments module (nn.Module) - Module to initialize. gain (float, optional , default=None) - Gain of orthogonal initialization. Default is computed for ReLU activation with torch.nn.init.calculate_gain('relu') . Returns Module, whose weight and bias have been modified in-place. Example linear = nn . Linear ( 23 , 5 ) atari_init_ ( linear )","title":"cherry.nn.init"},{"location":"_build/pydocmd/docs/cherry.nn.init/#cherrynninit","text":"","title":"cherry.nn.init"},{"location":"_build/pydocmd/docs/cherry.nn.init/#robotics_init_","text":"robotics_init_ ( module , gain = None ) [Source] Description Default initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments module (nn.Module) - Module to initialize. gain (float, optional , default=sqrt(2.0)) - Gain of orthogonal initialization. Returns Module, whose weight and bias have been modified in-place. Example linear = nn . Linear ( 23 , 5 ) kostrikov_robotics_ ( linear )","title":"robotics_init_"},{"location":"_build/pydocmd/docs/cherry.nn.init/#atari_init_","text":"atari_init_ ( module , gain = None ) [Source] Description Default initialization for Atari environments. Credit Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments module (nn.Module) - Module to initialize. gain (float, optional , default=None) - Gain of orthogonal initialization. Default is computed for ReLU activation with torch.nn.init.calculate_gain('relu') . Returns Module, whose weight and bias have been modified in-place. Example linear = nn . Linear ( 23 , 5 ) atari_init_ ( linear )","title":"atari_init_"},{"location":"_build/pydocmd/docs/cherry.nn/","text":"RoboticsLinear \u00b6 RoboticsLinear ( * args , ** kwargs ) [Source] Description Akin to nn.Linear , but with proper initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation. Arguments gain (float, optional ) - Gain factor passed to robotics_init_ initialization. This class extends nn.Linear and supports all of its arguments. Example linear = ch . nn . Linear ( 23 , 5 , bias = True ) action_mean = linear ( state )","title":"RoboticsLinear"},{"location":"_build/pydocmd/docs/cherry.nn/#roboticslinear","text":"RoboticsLinear ( * args , ** kwargs ) [Source] Description Akin to nn.Linear , but with proper initialization for robotic control. Credit Adapted from Ilya Kostrikov's implementation. Arguments gain (float, optional ) - Gain factor passed to robotics_init_ initialization. This class extends nn.Linear and supports all of its arguments. Example linear = ch . nn . Linear ( 23 , 5 , bias = True ) action_mean = linear ( state )","title":"RoboticsLinear"},{"location":"_build/pydocmd/docs/cherry.optim/","text":"cherry.optim \u00b6 Description Optimization utilities for scalable, high-performance reinforcement learning. Distributed \u00b6 Distributed ( params , opt , sync = None ) [Source] Description Synchronizes the gradients of a model across replicas. At every step, Distributed averages the gradient across all replicas before calling the wrapped optimizer. The sync parameters determines how frequently the parameters are synchronized between replicas, to minimize numerical divergences. This is done by calling the sync_parameters() method. If sync is None , this never happens except upon initialization of the class. Arguments params (iterable) - Iterable of parameters. opt (Optimizer) - The optimizer to wrap and synchronize. sync (int, optional , default=None) - Parameter synchronization frequency. References Zinkevich et al. 2010. \u201cParallelized Stochastic Gradient Descent.\u201d Example opt = optim . Adam ( model . parameters ()) opt = Distributed ( model . parameters (), opt , sync = 1 ) opt . step () opt . sync_parameters () sync_parameters \u00b6 Distributed . sync_parameters ( root = 0 ) Description Broadcasts all parameters of root to all other replicas. Arguments root (int, optional , default=0) - Rank of root replica.","title":"cherry.optim"},{"location":"_build/pydocmd/docs/cherry.optim/#cherryoptim","text":"Description Optimization utilities for scalable, high-performance reinforcement learning.","title":"cherry.optim"},{"location":"_build/pydocmd/docs/cherry.optim/#distributed","text":"Distributed ( params , opt , sync = None ) [Source] Description Synchronizes the gradients of a model across replicas. At every step, Distributed averages the gradient across all replicas before calling the wrapped optimizer. The sync parameters determines how frequently the parameters are synchronized between replicas, to minimize numerical divergences. This is done by calling the sync_parameters() method. If sync is None , this never happens except upon initialization of the class. Arguments params (iterable) - Iterable of parameters. opt (Optimizer) - The optimizer to wrap and synchronize. sync (int, optional , default=None) - Parameter synchronization frequency. References Zinkevich et al. 2010. \u201cParallelized Stochastic Gradient Descent.\u201d Example opt = optim . Adam ( model . parameters ()) opt = Distributed ( model . parameters (), opt , sync = 1 ) opt . step () opt . sync_parameters ()","title":"Distributed"},{"location":"_build/pydocmd/docs/cherry.optim/#sync_parameters","text":"Distributed . sync_parameters ( root = 0 ) Description Broadcasts all parameters of root to all other replicas. Arguments root (int, optional , default=0) - Rank of root replica.","title":"sync_parameters"},{"location":"_build/pydocmd/docs/cherry.pg/","text":"cherry.pg \u00b6 Description Utilities to implement policy gradient algorithms. generalized_advantage \u00b6 generalized_advantage ( gamma , tau , rewards , dones , values , next_value ) Description Computes the generalized advantage estimator. (GAE) References Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns tensor - Tensor of advantages. Example mass , next_value = policy ( replay [ - 1 ] . next_state ) advantages = generalized_advantage ( 0.99 , 0.95 , replay . reward (), replay . value (), replay . done (), next_value )","title":"cherry.pg"},{"location":"_build/pydocmd/docs/cherry.pg/#cherrypg","text":"Description Utilities to implement policy gradient algorithms.","title":"cherry.pg"},{"location":"_build/pydocmd/docs/cherry.pg/#generalized_advantage","text":"generalized_advantage ( gamma , tau , rewards , dones , values , next_value ) Description Computes the generalized advantage estimator. (GAE) References Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns tensor - Tensor of advantages. Example mass , next_value = policy ( replay [ - 1 ] . next_state ) advantages = generalized_advantage ( 0.99 , 0.95 , replay . reward (), replay . value (), replay . done (), next_value )","title":"generalized_advantage"},{"location":"_build/pydocmd/docs/cherry.plot/","text":"cherry.plot \u00b6 Description Plotting utilities for reproducible research. ci95 \u00b6 ci95 ( values ) [Source] Description Computes the 95% confidence interval around the given values. Arguments values (list) - List of values for which to compute the 95% confidence interval. Returns (float, float) The lower and upper bounds of the confidence interval. Example from statistics import mean smoothed = [] for replay in replays : rewards = replay . rewards . view ( - 1 ) . tolist () y_smoothed = ch . plot . smooth ( rewards ) smoothed . append ( y_smoothed ) means = [ mean ( r ) for r in zip ( * smoothed )] confidences = [ ch . plot . ci95 ( r ) for r in zip ( * smoothed )] lower_bound = [ conf [ 0 ] for conf in confidences ] upper_bound = [ conf [ 1 ] for conf in confidences ] exponential_smoothing \u00b6 exponential_smoothing ( x , y = None , temperature = 1.0 ) [Source] Decription Two-sided exponential moving average for smoothing a curve. It performs regular exponential moving average twice from two different sides and then combines the results together. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed , y_smoothed , _ = exponential_smoothing ( x_original , y_original , temperature = 3. )","title":"cherry.plot"},{"location":"_build/pydocmd/docs/cherry.plot/#cherryplot","text":"Description Plotting utilities for reproducible research.","title":"cherry.plot"},{"location":"_build/pydocmd/docs/cherry.plot/#ci95","text":"ci95 ( values ) [Source] Description Computes the 95% confidence interval around the given values. Arguments values (list) - List of values for which to compute the 95% confidence interval. Returns (float, float) The lower and upper bounds of the confidence interval. Example from statistics import mean smoothed = [] for replay in replays : rewards = replay . rewards . view ( - 1 ) . tolist () y_smoothed = ch . plot . smooth ( rewards ) smoothed . append ( y_smoothed ) means = [ mean ( r ) for r in zip ( * smoothed )] confidences = [ ch . plot . ci95 ( r ) for r in zip ( * smoothed )] lower_bound = [ conf [ 0 ] for conf in confidences ] upper_bound = [ conf [ 1 ] for conf in confidences ]","title":"ci95"},{"location":"_build/pydocmd/docs/cherry.plot/#exponential_smoothing","text":"exponential_smoothing ( x , y = None , temperature = 1.0 ) [Source] Decription Two-sided exponential moving average for smoothing a curve. It performs regular exponential moving average twice from two different sides and then combines the results together. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed , y_smoothed , _ = exponential_smoothing ( x_original , y_original , temperature = 3. )","title":"exponential_smoothing"},{"location":"_build/pydocmd/docs/cherry.td/","text":"cherry.td \u00b6 Description Utilities to implement temporal difference algorithms. discount \u00b6 discount ( gamma , rewards , dones , bootstrap = 0.0 ) Description Discounts rewards at an rate of gamma. References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns tensor - Tensor of discounted rewards. Example rewards = th . ones ( 23 , 1 ) * 8 dones = th . zeros_like ( rewards ) dones [ - 1 ] += 1.0 discounted = ch . rl . discount ( 0.99 , rewards , dones , bootstrap = 1.0 ) temporal_difference \u00b6 temporal_difference ( gamma , rewards , dones , values , next_values ) Description Returns the temporal difference residual. Reference Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_values (tensor) - Values of the state obtained after the transition from the state used to compute the last value in values . Example values = vf ( replay . states ()) next_values = vf ( replay . next_states ()) td_errors = temporal_difference ( 0.99 , replay . reward (), replay . done (), values , next_values )","title":"cherry.td"},{"location":"_build/pydocmd/docs/cherry.td/#cherrytd","text":"Description Utilities to implement temporal difference algorithms.","title":"cherry.td"},{"location":"_build/pydocmd/docs/cherry.td/#discount","text":"discount ( gamma , rewards , dones , bootstrap = 0.0 ) Description Discounts rewards at an rate of gamma. References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns tensor - Tensor of discounted rewards. Example rewards = th . ones ( 23 , 1 ) * 8 dones = th . zeros_like ( rewards ) dones [ - 1 ] += 1.0 discounted = ch . rl . discount ( 0.99 , rewards , dones , bootstrap = 1.0 )","title":"discount"},{"location":"_build/pydocmd/docs/cherry.td/#temporal_difference","text":"temporal_difference ( gamma , rewards , dones , values , next_values ) Description Returns the temporal difference residual. Reference Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_values (tensor) - Values of the state obtained after the transition from the state used to compute the last value in values . Example values = vf ( replay . states ()) next_values = vf ( replay . next_states ()) td_errors = temporal_difference ( 0.99 , replay . reward (), replay . done (), values , next_values )","title":"temporal_difference"},{"location":"_build/pydocmd/tutorials/debugging_rl/","text":"Debugging Reinforcement Learning \u00b6 Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper, Explain how to use the debug mode, (post-mortem, logger, warning from functions) Maybe showcase all of the above with an example that seems right but doesn't work, Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"_build/pydocmd/tutorials/debugging_rl/#debugging-reinforcement-learning","text":"Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper, Explain how to use the debug mode, (post-mortem, logger, warning from functions) Maybe showcase all of the above with an example that seems right but doesn't work, Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"_build/pydocmd/tutorials/distributed_ppo/","text":"Distributed Training with PPO \u00b6 Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"_build/pydocmd/tutorials/distributed_ppo/#distributed-training-with-ppo","text":"Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"_build/pydocmd/tutorials/getting_started/","text":"Getting Started with Cherry \u00b6 This document provides an overview of the philosophy behind cherry, the tools it provides, and a small illustrative example. By the end of this tutorial, you should be well-equiped to incorporate cherry in your research workflow. We assume that you are already familiar with reinforcement learning. If, instead, you're looking for an introduction to the field we recommend looking at Josh Achiam's Spinning Up in Deep RL . Installation \u00b6 The first step in getting started with cherry is to install it. You can easily do that by typing the following command in your favorite shell. pip install cherry-rl By default cherry only has two dependencies: torch and gym . However, more dependencies might be required if you plan to use some specific functionalities. For example, the OpenAIAtari wrapper requires OpenCV ( pip install opencv-python ) and the VisdomLogger requires visdom ( pip install visdom ). Note While cherry depends on Gym for its environment wrappers, it doesn't restrict you to Gym environments. For instance, check the examples using simple_rl and pycolab environments for Gym-free usage of cherry. Overview \u00b6 Why do we need cherry ? There are many reinforcement learning libraries, many of which feature high-quality implementations. However, few of them provide the kind of low-level utilities useful to researchers. Cherry aims to alleviate this issue. Instead of an interface akin to PPO(env_name).train(1000) , it provides researchers with a set of tools they can use to write readable, replicable, and flexible implementations. Cherry prioritizes time-to-correct-implementation over time-to-run, by explicitely helping you check, debug, and reliably report your results. How to use cherry ? Our goal is to make cherry a natural extension to PyTorch, with reinforcement learning in mind. To this end, we closely follow the package structure of PyTorch while providing additional utilities where we see fit. So if your goal is to implement a novel distributed off-policy policy gradient algorithm, you can count on cherry to provide you experience replays, policy gradient losses, discounting/advantage functions, and distributed optimizers. Those functions not only reduce the time spent writing code, they also check that your implementation is sane. (e.g. do the log probabilities and rewards have identical shapes?) Moreover, cherry provides the implementation details necessary to make deep reinforcement learning work. (e.g. initializations, modules, and wrappers commonly used in robotic or Atari benchmarks.) Importantly, it includes built-in debugging functionalities: cherry can help you visualize what is happening under the hood of your algorithm to help you find bugs faster. What's the future of cherry ? Reinforcement learning is a fast moving field, and it is difficult to predict which advances are safe bets for the future. Our long-term development strategy can be summarized as follows. Have as many recent and high-quality examples as possible. Merge advances that turn up to be fundamental in theory or practice into the core library. We hope to combat the reproducibility crisis by extensively testing and benchmarking our implementations. Note Cherry is in its early days and is still missing some of the well-established methods from the past 60 years. Those ones are being implemented as fast as we can :) Core Features \u00b6 The following features are fundamental components of cherry. Transitions and Experience Replay \u00b6 A majority of algorithms needs to store, retrieve, and sample past experience. To that end, you can use cherry's ExperienceReplay . An experience replay is implemented as a wrapper around a standard Python list. The major difference is that the append() method expects arguments used to create a Transition . In addition to behaving like a list, it exposes methods that act on this list, such as to(device) (moves the replay to a device), sample() (randomly samples some experience), or load() / save() (for convenient serialization). An ExperienceReplay contains Transition s, which are akin to ( state , action , reward , next_state , done ) named tuples with possibly additional custom fields. Those fields are easily accessible directly from the replay by accessing the method named after them. For example, calling replay.action() will fetch the action field from every transition stored in replay , stack them along the first dimension, and return that large tensor. The same is true for custom fields; if all transitions have a logprob field, replay.logprob() will return the result of stacking them. Temporal Difference and Policy Gradients \u00b6 Many low-level utilities used to implement temporal difference and policy gradient algorithms are available in the cherry.td and cherry.pg modules, respectively. Those modules include classical methods such as discounting rewards or computing the temporal difference , as well as more recent advances such as the generalized advantage estimator . We tried our best to avoid philosophical dissonance when a method belonged to both families of algorithms. Models and PyTorch \u00b6 Similar to PyTorch, we provide differentiable modules in cherry.nn , domain-specific initialization schemes in cherry.nn.init , and optimization utilities in cherry.optim . In addition, popular higher-level models are available in cherry.models ; for instance, those include tabular modules , the Atari CNN features extractor , and a Multi-Layer Perceptron for continuous control. Gym Wrappers \u00b6 Given the popularity of OpenAI Gym environment in modern reinforcement learning benchmarks, cherry includes convenient wrappers in the cherry.envs package. Examples include normalization of states and actions , Atari frames pre-processing, customizable state / action processing, and automatic collection of experience in a replay. Plots \u00b6 Reporting comparable results has become a central problem in modern reinforcement learning. In order to alleviate this issue, cherry provides utilities to smooth and compute confidence intervals over lists of rewards. Those are available in the cherry.plot submodule. Implementing Policy Gradient \u00b6 As an introductory example let us dissect the following snippet, which demonstrates how to implement the policy gradient theorem using cherry. import cherry as ch env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) env = ch . envs . Runner ( env ) env . seed ( 42 ) policy = PolicyNet () optimizer = optim . Adam ( policy . parameters (), lr = 1e-2 ) action_dist = ch . distributions . ActionDistribution ( env ) def get_action ( state ): mass = action_dist ( policy ( state )) action = mass . sample () log_prob = mass . log_prob ( action ) return action , { 'log_prob' : log_prob } for step in range ( 1000 ): replay = env . run ( get_action , episodes = 1 ) rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () After importing cherry, the first step is to instanciate, wrap, and seed the desired gym environment. env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) env = ch . envs . Runner ( env ) env . seed ( 42 ) The Logger , Torch , and Runner classes are Gym environment wrappers that systematically modify the behaviour of an environment: Logger keeps track of metrics and prints them at a given interval. Torch converts Gym states into PyTorch tensors, and action tensors into numpy arrays. Runner implements a run() method which allows to easily gather transitions for a number of steps or episodes. One particularity of wrappers is that they automatically expose methods of the wrapped environment: env.seed(42) calls the seed() method from the CartPole-v0 environment. Second, we instanciate the policy, optimizer, as well as the action distribution. The action distribution is created with action_dist = ch . distributions . ActionDistribution ( env ) which will automatically choose a diagonal Gaussian for continuous action-spaces and a categorical distribution for discrete ones. Next, we define get_action() which specifies how to get an action from our agent and will be used in conjuction to env.run() to quickly collect experience data: replay = env . run ( get_action , episodes = 1 ) env.run() assumes that the first returned value by get_action is the action to be passed to the environment and the second, optional, returned value is a dictionary to be saved into the experience replay. Under the hood, env.run() creates a new ExperienceReplay and fills it with the desired number of transitions; instead of episodes=1 we could have passed steps=100 . Finally, we discount and normalize the rewards and take an optimization step on the policy gradient loss. rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () When calling replay.reward() , replay.done() , or replay.log_prob() , the experience replay will concatenate the corresponding attribute across all of its transitions and return a new tensor . This means that this operation is rather expensive (you should cache it when possible) and that modifying this tensor does not modify the corresponding transitions in replay . Note that in this case log_prob is a custom attribute which is not declared in the original implementation of ExperienceReplay , and we could have given it any name by changing the dictionary key in get_action() . Conclusion \u00b6 You should now be able to use cherry in your own work. For more information, have a look at the documentation , the other tutorials , or the numerous examples . Since one of the characteristics of cherry is to avoid providing \"pre-baked\" algorithms, we tried our best to heavily document its usage.","title":"Getting Started with Cherry"},{"location":"_build/pydocmd/tutorials/getting_started/#getting-started-with-cherry","text":"This document provides an overview of the philosophy behind cherry, the tools it provides, and a small illustrative example. By the end of this tutorial, you should be well-equiped to incorporate cherry in your research workflow. We assume that you are already familiar with reinforcement learning. If, instead, you're looking for an introduction to the field we recommend looking at Josh Achiam's Spinning Up in Deep RL .","title":"Getting Started with Cherry"},{"location":"_build/pydocmd/tutorials/getting_started/#installation","text":"The first step in getting started with cherry is to install it. You can easily do that by typing the following command in your favorite shell. pip install cherry-rl By default cherry only has two dependencies: torch and gym . However, more dependencies might be required if you plan to use some specific functionalities. For example, the OpenAIAtari wrapper requires OpenCV ( pip install opencv-python ) and the VisdomLogger requires visdom ( pip install visdom ). Note While cherry depends on Gym for its environment wrappers, it doesn't restrict you to Gym environments. For instance, check the examples using simple_rl and pycolab environments for Gym-free usage of cherry.","title":"Installation"},{"location":"_build/pydocmd/tutorials/getting_started/#overview","text":"Why do we need cherry ? There are many reinforcement learning libraries, many of which feature high-quality implementations. However, few of them provide the kind of low-level utilities useful to researchers. Cherry aims to alleviate this issue. Instead of an interface akin to PPO(env_name).train(1000) , it provides researchers with a set of tools they can use to write readable, replicable, and flexible implementations. Cherry prioritizes time-to-correct-implementation over time-to-run, by explicitely helping you check, debug, and reliably report your results. How to use cherry ? Our goal is to make cherry a natural extension to PyTorch, with reinforcement learning in mind. To this end, we closely follow the package structure of PyTorch while providing additional utilities where we see fit. So if your goal is to implement a novel distributed off-policy policy gradient algorithm, you can count on cherry to provide you experience replays, policy gradient losses, discounting/advantage functions, and distributed optimizers. Those functions not only reduce the time spent writing code, they also check that your implementation is sane. (e.g. do the log probabilities and rewards have identical shapes?) Moreover, cherry provides the implementation details necessary to make deep reinforcement learning work. (e.g. initializations, modules, and wrappers commonly used in robotic or Atari benchmarks.) Importantly, it includes built-in debugging functionalities: cherry can help you visualize what is happening under the hood of your algorithm to help you find bugs faster. What's the future of cherry ? Reinforcement learning is a fast moving field, and it is difficult to predict which advances are safe bets for the future. Our long-term development strategy can be summarized as follows. Have as many recent and high-quality examples as possible. Merge advances that turn up to be fundamental in theory or practice into the core library. We hope to combat the reproducibility crisis by extensively testing and benchmarking our implementations. Note Cherry is in its early days and is still missing some of the well-established methods from the past 60 years. Those ones are being implemented as fast as we can :)","title":"Overview"},{"location":"_build/pydocmd/tutorials/getting_started/#core-features","text":"The following features are fundamental components of cherry.","title":"Core Features"},{"location":"_build/pydocmd/tutorials/getting_started/#transitions-and-experience-replay","text":"A majority of algorithms needs to store, retrieve, and sample past experience. To that end, you can use cherry's ExperienceReplay . An experience replay is implemented as a wrapper around a standard Python list. The major difference is that the append() method expects arguments used to create a Transition . In addition to behaving like a list, it exposes methods that act on this list, such as to(device) (moves the replay to a device), sample() (randomly samples some experience), or load() / save() (for convenient serialization). An ExperienceReplay contains Transition s, which are akin to ( state , action , reward , next_state , done ) named tuples with possibly additional custom fields. Those fields are easily accessible directly from the replay by accessing the method named after them. For example, calling replay.action() will fetch the action field from every transition stored in replay , stack them along the first dimension, and return that large tensor. The same is true for custom fields; if all transitions have a logprob field, replay.logprob() will return the result of stacking them.","title":"Transitions and Experience Replay"},{"location":"_build/pydocmd/tutorials/getting_started/#temporal-difference-and-policy-gradients","text":"Many low-level utilities used to implement temporal difference and policy gradient algorithms are available in the cherry.td and cherry.pg modules, respectively. Those modules include classical methods such as discounting rewards or computing the temporal difference , as well as more recent advances such as the generalized advantage estimator . We tried our best to avoid philosophical dissonance when a method belonged to both families of algorithms.","title":"Temporal Difference and Policy Gradients"},{"location":"_build/pydocmd/tutorials/getting_started/#models-and-pytorch","text":"Similar to PyTorch, we provide differentiable modules in cherry.nn , domain-specific initialization schemes in cherry.nn.init , and optimization utilities in cherry.optim . In addition, popular higher-level models are available in cherry.models ; for instance, those include tabular modules , the Atari CNN features extractor , and a Multi-Layer Perceptron for continuous control.","title":"Models and PyTorch"},{"location":"_build/pydocmd/tutorials/getting_started/#gym-wrappers","text":"Given the popularity of OpenAI Gym environment in modern reinforcement learning benchmarks, cherry includes convenient wrappers in the cherry.envs package. Examples include normalization of states and actions , Atari frames pre-processing, customizable state / action processing, and automatic collection of experience in a replay.","title":"Gym Wrappers"},{"location":"_build/pydocmd/tutorials/getting_started/#plots","text":"Reporting comparable results has become a central problem in modern reinforcement learning. In order to alleviate this issue, cherry provides utilities to smooth and compute confidence intervals over lists of rewards. Those are available in the cherry.plot submodule.","title":"Plots"},{"location":"_build/pydocmd/tutorials/getting_started/#implementing-policy-gradient","text":"As an introductory example let us dissect the following snippet, which demonstrates how to implement the policy gradient theorem using cherry. import cherry as ch env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) env = ch . envs . Runner ( env ) env . seed ( 42 ) policy = PolicyNet () optimizer = optim . Adam ( policy . parameters (), lr = 1e-2 ) action_dist = ch . distributions . ActionDistribution ( env ) def get_action ( state ): mass = action_dist ( policy ( state )) action = mass . sample () log_prob = mass . log_prob ( action ) return action , { 'log_prob' : log_prob } for step in range ( 1000 ): replay = env . run ( get_action , episodes = 1 ) rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () After importing cherry, the first step is to instanciate, wrap, and seed the desired gym environment. env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) env = ch . envs . Runner ( env ) env . seed ( 42 ) The Logger , Torch , and Runner classes are Gym environment wrappers that systematically modify the behaviour of an environment: Logger keeps track of metrics and prints them at a given interval. Torch converts Gym states into PyTorch tensors, and action tensors into numpy arrays. Runner implements a run() method which allows to easily gather transitions for a number of steps or episodes. One particularity of wrappers is that they automatically expose methods of the wrapped environment: env.seed(42) calls the seed() method from the CartPole-v0 environment. Second, we instanciate the policy, optimizer, as well as the action distribution. The action distribution is created with action_dist = ch . distributions . ActionDistribution ( env ) which will automatically choose a diagonal Gaussian for continuous action-spaces and a categorical distribution for discrete ones. Next, we define get_action() which specifies how to get an action from our agent and will be used in conjuction to env.run() to quickly collect experience data: replay = env . run ( get_action , episodes = 1 ) env.run() assumes that the first returned value by get_action is the action to be passed to the environment and the second, optional, returned value is a dictionary to be saved into the experience replay. Under the hood, env.run() creates a new ExperienceReplay and fills it with the desired number of transitions; instead of episodes=1 we could have passed steps=100 . Finally, we discount and normalize the rewards and take an optimization step on the policy gradient loss. rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () When calling replay.reward() , replay.done() , or replay.log_prob() , the experience replay will concatenate the corresponding attribute across all of its transitions and return a new tensor . This means that this operation is rather expensive (you should cache it when possible) and that modifying this tensor does not modify the corresponding transitions in replay . Note that in this case log_prob is a custom attribute which is not declared in the original implementation of ExperienceReplay , and we could have given it any name by changing the dictionary key in get_action() .","title":"Implementing Policy Gradient"},{"location":"_build/pydocmd/tutorials/getting_started/#conclusion","text":"You should now be able to use cherry in your own work. For more information, have a look at the documentation , the other tutorials , or the numerous examples . Since one of the characteristics of cherry is to avoid providing \"pre-baked\" algorithms, we tried our best to heavily document its usage.","title":"Conclusion"},{"location":"_build/pydocmd/tutorials/recurrent_a2c/","text":"Recurrent Policy Gradients with A2C \u00b6 Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"},{"location":"_build/pydocmd/tutorials/recurrent_a2c/#recurrent-policy-gradients-with-a2c","text":"Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"},{"location":"api/cherry.algorithms/","text":"cherry.algorithms \u00b6 cherry.algorithms.arguments.AlgorithmArguments \u00b6 [Source] Description \u00b6 Utility functions to work with dataclass algorithms. Example \u00b6 @dataclasses . dataclass class MyNewAlgorithm ( AlgorithmArguments ) . my_arg1 : float = 0.0 def update ( self , my_arg1 , ** kwargs ): pass cherry.algorithms.a2c.A2C dataclass \u00b6 [Source] Description \u00b6 Helper functions for implementing A2C. A2C simply computes the gradient of the policy as follows: \\mathbb{E} \\left[ (Q(s, a) - V(s)) \\cdot \\nabla_\\theta \\log \\pi_\\theta (a \\vert s) \\right]. policy_loss ( log_probs , advantages ) staticmethod \u00b6 Description \u00b6 The policy loss of the Advantage Actor-Critic. This function simply performs an element-wise multiplication and a mean reduction. References \u00b6 Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments \u00b6 log_probs (tensor) - Log-density of the selected actions. advantages (tensor) - Advantage of the action-state pairs. Returns \u00b6 (tensor) - The policy loss for the given arguments. Example \u00b6 advantages = replay . advantage () log_probs = replay . log_prob () loss = a2c . policy_loss ( log_probs , advantages ) state_value_loss ( values , rewards ) staticmethod \u00b6 Description \u00b6 The state-value loss of the Advantage Actor-Critic. This function is equivalent to a MSELoss. References \u00b6 Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d arXiv [cs.LG]. Arguments \u00b6 values (tensor) - Predicted values for some states. rewards (tensor) - Observed rewards for those states. Returns \u00b6 (tensor) - The value loss for the given arguments. Example \u00b6 values = replay . value () rewards = replay . reward () loss = a2c . state_value_loss ( values , rewards ) cherry.algorithms.ddpg.DDPG dataclass \u00b6 [Source] Description \u00b6 Utilities to implement deep deterministic policy gradient algorithms from [1]. References \u00b6 Lillicrap et al., \"Continuous Control with Deep Reinforcement Learning\", ICLR 2016. state_value_loss ( values , next_values , rewards , dones , gamma ) staticmethod \u00b6 Description \u00b6 The discounted Bellman loss, computed as: \\vert\\vert R + (1 - \\textrm{dones}) \\cdot \\gamma \\cdot V(s_{t+1}) - V(s_t) \\vert\\vert^2 Arguments \u00b6 values (tensor) - State values for timestep t. next_values (tensor) - State values for timestep t+1. rewards (tensor) - Vector of rewards for timestep t. dones (tensor) - Termination flag. gamma (float) - Discount factor. Returns \u00b6 (tensor) - The state value loss above. cherry.algorithms.drq.DrQ dataclass \u00b6 [Source] Description \u00b6 Utilities to implement DrQ from [1]. DrQ (Data-regularized Q) extends SAC to more efficiently train policies and action values from pixels. References \u00b6 Kostrikov et al., \"Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels\", ICLR 2021. Arguments \u00b6 batch_size (int, optional , default=512) - Number of samples to get from the replay. discount (float, optional , default=0.99) - Discount factor. use_automatic_entropy_tuning (bool, optional , default=True) - Whether to optimize the entropy weight \\alpha . policy_delay (int, optional , default=1) - Delay between policy updates. target_delay (int, optional , default=1) - Delay between action value updates. target_polyak_weight (float, optional , default=0.995) - Weight factor alpha for Polyak averaging; see cherry.models.polyak_average . __init__ ( self , batch_size : int = 512 , discount : float = 0.99 , use_automatic_entropy_tuning : bool = True , policy_delay : int = 2 , target_delay : int = 2 , target_polyak_weight : float = 0.995 ) -> None special \u00b6 update ( self , replay , policy , action_value , target_action_value , features , target_features , log_alpha , target_entropy , policy_optimizer , action_value_optimizer , features_optimizer , alpha_optimizer , update_policy = True , update_target = False , update_value = True , update_entropy = True , augmentation_transform = None , device = None , ** kwargs ) \u00b6 Description \u00b6 Implements a single DrQ update. Arguments \u00b6 replay (cherry.ExperienceReplay) - Offline replay to sample transitions from. policy (cherry.nn.Policy) - Policy to optimize. action_value (cherry.nn.ActionValue) - Twin action value to optimize; see cherry.nn.Twin. target_action_value (cherry.nn.ActionValue) - Target action value. features (torch.nn.Module) - Feature extractor for the policy and action value. target_features (torch.nn.Module) - Feature extractor for the target action value. log_alpha (torch.Tensor) - SAC's (log) entropy weight. target_entropy (torch.Tensor) - SAC's target for the policy entropy (typically \\vert\\mathcal{A}\\vert ). policy_optimizer (torch.optim.Optimizer) - Optimizer for the policy . action_value_optimizer (torch.optim.Optimizer) - Optimizer for the action_value . features_optimizer (torch.optim.Optimizer) - Optimizer for the features . alpha_optimizer (torch.optim.Optimizer) - Optimizer for log_alpha . update_policy (bool, optional , default=True) - Whether to update the policy. update_target (bool, optional , default=False) - Whether to update the action value target network. update_value (bool, optional , default=True) - Whether to update the action value. update_entropy (bool, optional , default=True) - Whether to update the entropy weight. augmentation_transform (torch.nn.Module, optional , default=None) - Data augmentation transform to augment image observations. Defaults to RandomShiftsAug(4) (as in the paper). device (torch.device) - The device used to compute the update. cherry.algorithms.drqv2.DrQv2 dataclass \u00b6 [Source] Description \u00b6 Utilities to implement DrQ-v2 from [1]. DrQ-v2 builds on DrQ but replaces the underlying SAC with TD3. It is noticeably faster in terms of wall-clock time and sample complexity. References \u00b6 Yarats et al., \"Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning\", ICLR 2022. Arguments \u00b6 batch_size (int, optional , default=512) - Number of samples to get from the replay. discount (float, optional , default=0.99) - Discount factor. policy_delay (int, optional , default=1) - Delay between policy updates. target_delay (int, optional , default=1) - Delay between action value updates. target_polyak_weight (float, optional , default=0.995) - Weight factor alpha for Polyak averaging; see cherry.models.polyak_average . nsteps (int, optional , default=1) - Number of bootstrapping steps to compute the target values. std_decay (float, optional , default=0.0) - Exponential decay rate of the policy's standard deviation. A reasonable value for DMC is 0.99997. min_std (float, optional , default=0.1) - Minimum standard deviation for the policy. __init__ ( self , batch_size : int = 512 , discount : float = 0.99 , policy_delay : int = 1 , target_delay : int = 1 , target_polyak_weight : float = 0.995 , nsteps : int = 1 , std_decay : float = 0.0 , min_std : float = 0.1 ) -> None special \u00b6 update ( self , replay , policy , action_value , target_action_value , features , policy_optimizer , action_value_optimizer , features_optimizer , update_policy = True , update_target = True , update_value = True , augmentation_transform = None , device = None , ** kwargs ) \u00b6 Description \u00b6 Implements a single DrQ-v2 update. Arguments \u00b6 replay (cherry.ExperienceReplay) - Offline replay to sample transitions from. policy (cherry.nn.Policy) - Policy to optimize. action_value (cherry.nn.ActionValue) - Twin action value to optimize; see cherry.nn.Twin. target_action_value (cherry.nn.ActionValue) - Target action value. features (torch.nn.Module) - Feature extractor for the policy and action value. policy_optimizer (torch.optim.Optimizer) - Optimizer for the policy . features_optimizer (torch.optim.Optimizer) - Optimizer for the features . update_policy (bool, optional , default=True) - Whether to update the policy. update_target (bool, optional , default=True) - Whether to update the action value target network. update_value (bool, optional , default=True) - Whether to update the action value. augmentation_transform (torch.nn.Module, optional , default=None) - Data augmentation transform to augment image observations. Defaults to RandomShiftsAug(4) (as in the paper). device (torch.device) - The device used to compute the update. cherry.algorithms.ppo.PPO dataclass \u00b6 [Source] Description \u00b6 Utilities to implement PPO from [1]. The idea behing PPO is to cheaply approximate TRPO's trust-region with the following objective: \\mathbb{E}[\\min( \\frac{\\pi_{\\theta}}{\\pi_{\\textrm{old}}} \\cdot A, \\mathrm{clip}(\\frac{\\pi_{\\theta}}{\\pi_{\\textrm{old}}} \\cdot A, 1-\\epsilon, 1+\\epsilon) )], where \\pi_{\\theta} is the current policy and \\pi_{\\mathrm{old}} is the policy used to collect the online replay's data. References \u00b6 Schulman et al., \u201cProximal Policy Optimization Algorithms\u201d, ArXiv 2017. Arguments \u00b6 Note: the following arguments were optimized for continuous control on PyBullet / MuJoCo. num_steps (int, optional , default=320) - Number of of PPO gradient steps in a single update. batch_size (int, optional , default=512) - Number of samples to get from the replay. policy_clip (float, optional , default=0.2) - Clip constant for the policy. value_clip (float, optional , default=0.2) - Clip constant for state value function. value_weight (float, optional , default=0.5) - Scaling factor fo the state value function penalty. entropy_weight (float, optional , default=0.0) - Scaling factor of the entropy penalty. discount (float, optional , default=0.99) - Discount factor. gae_tau (float, optional , default=0.95) - Bias-variance trade-off for the generalized advantage estimator. gradient_norm (float, optional , default=0.5) - Maximum gradient norm. eps (float, optional , default=0.5) - Numerical stability constant. __init__ ( self , num_steps : int = 320 , batch_size : float = 64 , policy_clip : float = 0.2 , value_clip : float = 0.2 , value_weight : float = 0.5 , entropy_weight : float = 0.0 , discount : float = 0.99 , gae_tau : float = 0.95 , gradient_norm : float = 0.5 , eps : float = 1e-08 ) -> None special \u00b6 policy_loss ( new_log_probs , old_log_probs , advantages , clip = 0.1 ) staticmethod \u00b6 Description \u00b6 The clipped policy loss of Proximal Policy Optimization. Arguments \u00b6 new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. clip (float, optional , default=0.1) - The clipping coefficient. Returns \u00b6 loss (tensor) - The clipped policy loss for the given arguments. Example advantage = ch . pg . generalized_advantage ( GAMMA , TAU , replay . reward (), replay . done (), replay . value (), next_state_value , ) new_densities = policy ( replay . state ()) new_logprobs = new_densities . log_prob ( replay . action ()) loss = policy_loss ( new_logprobs , replay . logprob () . detach (), advantage . detach (), clip = 0.2 , ) state_value_loss ( new_values , old_values , rewards , clip = 0.1 ) staticmethod \u00b6 Description \u00b6 The clipped state-value loss of Proximal Policy Optimization. Arguments \u00b6 new_values (tensor) - State values from the optimized value function. old_values (tensor) - State values from the reference value function. rewards (tensor) - Observed rewards. clip (float, optional , default=0.1) - The clipping coefficient. Returns \u00b6 loss (tensor) - The clipped value loss for the given arguments. Example \u00b6 values = v_function ( batch . state ()) value_loss = ppo . state_value_loss ( values , batch . value () . detach (), batch . reward (), clip = 0.2 , ) update ( self , replay , policy , optimizer , state_value , ** kwargs ) \u00b6 Description \u00b6 Implements a single PPO update. Arguments \u00b6 replay (cherry.ExperienceReplay) - Online replay to sample transitions from. policy (cherry.nn.Policy) - Policy to optimize. state_value (cherry.nn.StateValue) - State value function V(s) . optimizer (torch.optim.Optimizer) - Optimizer for the policy . cherry.algorithms.td3.TD3 dataclass \u00b6 [Source] Description \u00b6 Utilities to implement TD3 from [1]. The main idea behind TD3 is to extend DDPG with twin action value functions. Namely, the action values are computed with: \\min_{i=1, 2} Q_i(s_t, \\pi(s_t) + \\epsilon), where \\pi is a deterministic policy and \\epsilon is (typically) sampled from a Gaussian distribution. See cherry.nn.Twin to easily implement such twin Q-functions. The authors also suggest to delay the updates to the policy. This simply boils down to applying 1 policy update every N times the action value function is updated. This implementation also supports delaying updates to the action value and its target network. References \u00b6 Fujimoto et al., \"Addressing Function Approximation Error in Actor-Critic Methods\", ICML 2018. Arguments \u00b6 batch_size (int, optional , default=512) - Number of samples to get from the replay. discount (float, optional , default=0.99) - Discount factor. policy_delay (int, optional , default=1) - Delay between policy updates. target_delay (int, optional , default=1) - Delay between action value updates. target_polyak_weight (float, optional , default=0.995) - Weight factor alpha for Polyak averaging; see cherry.models.polyak_average . nsteps (int, optional , default=1) - Number of bootstrapping steps to compute the target values. __init__ ( self , batch_size : int = 512 , discount : float = 0.99 , policy_delay : int = 1 , target_delay : int = 1 , target_polyak_weight : float = 0.995 , nsteps : int = 1 ) -> None special \u00b6 update ( self , replay , policy , action_value , target_action_value , policy_optimizer , action_value_optimizer , update_policy = True , update_target = True , update_value = True , device = None , ** kwargs ) \u00b6 Description \u00b6 Implements a single TD3 update. Arguments \u00b6 replay (cherry.ExperienceReplay) - Offline replay to sample transitions from. policy (cherry.nn.Policy) - Policy to optimize. action_value (cherry.nn.ActionValue) - Twin action value to optimize; see cherry.nn.Twin. target_action_value (cherry.nn.ActionValue) - Target action value. policy_optimizer (torch.optim.Optimizer) - Optimizer for the policy . action_value_optimizer (torch.optim.Optimizer) - Optimizer for the action_value . update_policy (bool, optional , default=True) - Whether to update the policy. update_target (bool, optional , default=True) - Whether to update the action value target network. update_value (bool, optional , default=True) - Whether to update the action value. device (torch.device) - The device used to compute the update. cherry.algorithms.trpo.TRPO dataclass \u00b6 Description \u00b6 Helper functions for implementing Trust-Region Policy Optimization. Recall that TRPO strives to solve the following objective: \\max_\\theta \\quad \\mathbb{E}\\left[ \\frac{\\pi_\\theta}{\\pi_\\text{old}} \\cdot A \\right] \\\\ \\text{subject to} \\quad D_\\text{KL}(\\pi_\\text{old} \\vert \\vert \\pi_\\theta) \\leq \\delta. conjugate_gradient ( Ax , b , num_iterations = 10 , tol = 1e-10 , eps = 1e-08 ) staticmethod \u00b6 [Source] Description \u00b6 Computes x = A^{-1}b using the conjugate gradient algorithm. Credit \u00b6 Adapted from Kai Arulkumaran's implementation, with additions inspired from John Schulman's implementation. References \u00b6 Nocedal and Wright. 2006. \"Numerical Optimization, 2nd edition\". Springer. Shewchuk et al. 1994. \u201cAn Introduction to the Conjugate Gradient Method without the Agonizing Pain.\u201d CMU. Arguments \u00b6 Ax (callable) - Given a vector x, computes A@x. b (tensor or list) - The reference vector. num_iterations (int, optional , default=10) - Number of conjugate gradient iterations. tol (float, optional , default=1e-10) - Tolerance for proposed solution. eps (float, optional , default=1e-8) - Numerical stability constant. Returns \u00b6 x (tensor or list) - The solution to Ax = b, as a list if b is a list else a tensor. hessian_vector_product ( loss , parameters , damping = 1e-05 ) staticmethod \u00b6 [Source] Description \u00b6 Returns a callable that computes the product of the Hessian of loss (w.r.t. parameters) with another vector, using Pearlmutter's trick. Note that parameters and the argument of the callable can be tensors or list of tensors. References \u00b6 Pearlmutter, B. A. 1994. \u201cFast Exact Multiplication by the Hessian.\u201d Neural Computation. Arguments \u00b6 loss (tensor) - The loss of which to compute the Hessian. parameters (tensor or list) - The tensors to take the gradient with respect to. damping (float, optional , default=1e-5) - Damping of the Hessian-vector product. Returns \u00b6 hvp(other) (callable) - A function to compute the Hessian-vector product, given a vector or list other . line_search ( params_init , params_update , model , stop_criterion , initial_stepsize = 1.0 , backtrack_factor = 0.5 , max_iterations = 15 ) staticmethod \u00b6 [Source] Description \u00b6 Computes line-search for model parameters given a parameter update and a stopping criterion. Credit \u00b6 Adapted from Kai Arulkumaran's implementation, with additions inspired from John Schulman's implementation. References \u00b6 Nocedal and Wright. 2006. \"Numerical Optimization, 2nd edition\". Springer. Arguments \u00b6 params_init (tensor or iteratble) - Initial parameter values. params_update (tensor or iteratble) - Update direction. model (Module) - The model to be updated. stop_criterion (callable) - Given a model, decided whether to stop the line-search. initial_stepsize (float, optional , default=1.0) - Initial stepsize of search. backtrack_factor (float, optional , default=0.5) - Backtracking factor. max_iterations (int, optional , default=15) - Max number of backtracking iterations. Returns \u00b6 new_model (Module) - The updated model if line-search is successful, else the model with initial parameter values. Example \u00b6 def ls_criterion ( new_policy ): new_density = new_policy ( states ) new_kl = kl_divergence ( old_density , new_densityl ) . mean () new_loss = - qvalue ( new_density . sample ()) . mean () return new_loss < policy_loss and new_kl < max_kl with torch . no_grad (): policy = trpo . line_search ( params_init = policy . parameters (), params_update = step , model = policy , criterion = ls_criterion ) policy_loss ( new_log_probs , old_log_probs , advantages ) staticmethod \u00b6 [Source] Description \u00b6 The policy loss for Trust-Region Policy Optimization. This is also known as the surrogate loss. References \u00b6 Schulman et al. 2015. \u201cTrust Region Policy Optimization.\u201d ICML 2015. Arguments \u00b6 new_log_probs (tensor) - The log-density of actions from the target policy. old_log_probs (tensor) - The log-density of actions from the behaviour policy. advantages (tensor) - Advantage of the actions. Returns \u00b6 (tensor) - The policy loss for the given arguments. Example \u00b6 advantage = ch . pg . generalized_advantage ( GAMMA , TAU , replay . reward (), replay . done (), replay . value (), next_state_value ) new_densities = policy ( replay . state ()) new_logprobs = new_densities . log_prob ( replay . action ()) loss = policy_loss ( new_logprobs , replay . logprob () . detach (), advantage . detach ()) cherry.algorithms.sac.SAC dataclass \u00b6 [Source] Description \u00b6 Utilities to implement SAC from [1]. The update() function updates the function approximators in the following order: Entropy weight update. Action-value update. State-value update. (Optional, c.f. below) Policy update. Note that most recent implementations of SAC omit step 3. above by using the Bellman residual instead of modelling a state-value function. For an example of such implementation refer to this link . References \u00b6 1 2 3 4 5 New actions are sampled from the target policy , and those are used to compute the Q - values . While we should back - propagate through the Q - values to the policy parameters , we shouldn ' t use that gradient to optimize the Q parameters . This is often avoided by either using a target Q function , or by zero - ing out the gradients of the Q function parameters . Arguments \u00b6 batch_size (int, optional , default=512) - Number of samples to get from the replay. discount (float, optional , default=0.99) - Discount factor. use_automatic_entropy_tuning (bool, optional , default=True) - Whether to optimize the entropy weight \\alpha . policy_delay (int, optional , default=1) - Delay between policy updates. target_delay (int, optional , default=1) - Delay between action value updates. target_polyak_weight (float, optional , default=0.995) - Weight factor alpha for Polyak averaging; see cherry.models.polyak_average . __init__ ( self , batch_size : int = 512 , discount : float = 0.99 , use_automatic_entropy_tuning : bool = True , policy_delay : int = 2 , target_delay : int = 2 , target_polyak_weight : float = 0.01 ) -> None special \u00b6 action_value_loss ( value , next_value , rewards , dones , gamma ) staticmethod \u00b6 Description \u00b6 The action-value loss of the Soft Actor-Critic. value should be the value of the current state-action pair, estimated via the Q-function. next_value is the expected value of the next state; it can be estimated via a V-function, or alternatively by computing the Q-value of the next observed state-action pair. In the latter case, make sure that the action is sampled according to the current policy, not the one used to gather the data. Arguments \u00b6 value (tensor) - Action values of the actual transition. next_value (tensor) - State values of the resulting state. rewards (tensor) - Observed rewards of the transition. dones (tensor) - Which states were terminal. gamma (float) - Discount factor. Returns \u00b6 (tensor) - The policy loss for the given arguments. Example \u00b6 value = qf ( batch . state (), batch . action () . detach ()) next_value = targe_vf ( batch . next_state ()) loss = action_value_loss ( value , next_value , batch . reward (), batch . done (), gamma = 0.99 ) policy_loss ( log_probs , q_curr , alpha = 1.0 ) staticmethod \u00b6 Description \u00b6 The policy loss of the Soft Actor-Critic. New actions are sampled from the target policy, and those are used to compute the Q-values. While we should back-propagate through the Q-values to the policy parameters, we shouldn't use that gradient to optimize the Q parameters. This is often avoided by either using a target Q function, or by zero-ing out the gradients of the Q function parameters. Arguments \u00b6 log_probs (tensor) - Log-density of the selected actions. q_curr (tensor) - Q-values of state-action pairs. alpha (float, optional , default=1.0) - Entropy weight. Returns \u00b6 (tensor) - The policy loss for the given arguments. Example \u00b6 densities = policy ( batch . state ()) actions = densities . sample () log_probs = densities . log_prob ( actions ) q_curr = q_function ( batch . state (), actions ) loss = policy_loss ( log_probs , q_curr , alpha = 0.1 ) update ( self , replay , policy , action_value , target_action_value , log_alpha , target_entropy , policy_optimizer , features_optimizer , action_value_optimizer , alpha_optimizer , features = None , target_features = None , update_policy = True , update_target = False , update_value = True , update_entropy = True , device = None , ** kwargs ) \u00b6 Description \u00b6 Implements a single SAC update. Arguments \u00b6 replay (cherry.ExperienceReplay) - Offline replay to sample transitions from. policy (cherry.nn.Policy) - Policy to optimize. action_value (cherry.nn.ActionValue) - Twin action value to optimize; see cherry.nn.Twin. target_action_value (cherry.nn.ActionValue) - Target action value. log_alpha (torch.Tensor) - SAC's (log) entropy weight. target_entropy (torch.Tensor) - SAC's target for the policy entropy (typically \\vert\\mathcal{A}\\vert ). policy_optimizer (torch.optim.Optimizer) - Optimizer for the policy . action_value_optimizer (torch.optim.Optimizer) - Optimizer for the action_value . features_optimizer (torch.optim.Optimizer) - Optimizer for the features . alpha_optimizer (torch.optim.Optimizer) - Optimizer for log_alpha . features (torch.nn.Module, optional , default=None) - Feature extractor for the policy and action value. target_features (torch.nn.Module, optional , default=None) - Feature extractor for the target action value. update_policy (bool, optional , default=True) - Whether to update the policy. update_target (bool, optional , default=False) - Whether to update the action value target network. update_value (bool, optional , default=True) - Whether to update the action value. update_entropy (bool, optional , default=True) - Whether to update the entropy weight. device (torch.device) - The device used to compute the update.","title":"cherry.algorithms"},{"location":"api/cherry.algorithms/#cherryalgorithms","text":"","title":"cherry.algorithms"},{"location":"api/cherry.algorithms/#cherry.algorithms.arguments.AlgorithmArguments","text":"[Source]","title":"AlgorithmArguments"},{"location":"api/cherry.algorithms/#cherry.algorithms.a2c.A2C","text":"[Source]","title":"A2C"},{"location":"api/cherry.algorithms/#cherry.algorithms.a2c.A2C.policy_loss","text":"","title":"policy_loss()"},{"location":"api/cherry.algorithms/#cherry.algorithms.a2c.A2C.state_value_loss","text":"","title":"state_value_loss()"},{"location":"api/cherry.algorithms/#cherry.algorithms.ddpg.DDPG","text":"[Source]","title":"DDPG"},{"location":"api/cherry.algorithms/#cherry.algorithms.ddpg.DDPG.state_value_loss","text":"","title":"state_value_loss()"},{"location":"api/cherry.algorithms/#cherry.algorithms.drq.DrQ","text":"[Source]","title":"DrQ"},{"location":"api/cherry.algorithms/#cherry.algorithms.drq.DrQ.__init__","text":"","title":"__init__()"},{"location":"api/cherry.algorithms/#cherry.algorithms.drq.DrQ.update","text":"","title":"update()"},{"location":"api/cherry.algorithms/#cherry.algorithms.drqv2.DrQv2","text":"[Source]","title":"DrQv2"},{"location":"api/cherry.algorithms/#cherry.algorithms.drqv2.DrQv2.__init__","text":"","title":"__init__()"},{"location":"api/cherry.algorithms/#cherry.algorithms.drqv2.DrQv2.update","text":"","title":"update()"},{"location":"api/cherry.algorithms/#cherry.algorithms.ppo.PPO","text":"[Source]","title":"PPO"},{"location":"api/cherry.algorithms/#cherry.algorithms.ppo.PPO.__init__","text":"","title":"__init__()"},{"location":"api/cherry.algorithms/#cherry.algorithms.ppo.PPO.policy_loss","text":"","title":"policy_loss()"},{"location":"api/cherry.algorithms/#cherry.algorithms.ppo.PPO.state_value_loss","text":"","title":"state_value_loss()"},{"location":"api/cherry.algorithms/#cherry.algorithms.ppo.PPO.update","text":"","title":"update()"},{"location":"api/cherry.algorithms/#cherry.algorithms.td3.TD3","text":"[Source]","title":"TD3"},{"location":"api/cherry.algorithms/#cherry.algorithms.td3.TD3.__init__","text":"","title":"__init__()"},{"location":"api/cherry.algorithms/#cherry.algorithms.td3.TD3.update","text":"","title":"update()"},{"location":"api/cherry.algorithms/#cherry.algorithms.trpo.TRPO","text":"","title":"TRPO"},{"location":"api/cherry.algorithms/#cherry.algorithms.trpo.TRPO.conjugate_gradient","text":"[Source]","title":"conjugate_gradient()"},{"location":"api/cherry.algorithms/#cherry.algorithms.trpo.TRPO.hessian_vector_product","text":"[Source]","title":"hessian_vector_product()"},{"location":"api/cherry.algorithms/#cherry.algorithms.trpo.TRPO.line_search","text":"[Source]","title":"line_search()"},{"location":"api/cherry.algorithms/#cherry.algorithms.trpo.TRPO.policy_loss","text":"[Source]","title":"policy_loss()"},{"location":"api/cherry.algorithms/#cherry.algorithms.sac.SAC","text":"[Source]","title":"SAC"},{"location":"api/cherry.algorithms/#cherry.algorithms.sac.SAC.__init__","text":"","title":"__init__()"},{"location":"api/cherry.algorithms/#cherry.algorithms.sac.SAC.action_value_loss","text":"","title":"action_value_loss()"},{"location":"api/cherry.algorithms/#cherry.algorithms.sac.SAC.policy_loss","text":"","title":"policy_loss()"},{"location":"api/cherry.algorithms/#cherry.algorithms.sac.SAC.update","text":"","title":"update()"},{"location":"api/cherry.debug/","text":"cherry.debug \u00b6 cherry . debug . debug ( log_dir = './' ) \u00b6 [Source] Description \u00b6 Enables some debugging utilities for logging and pdb. Includes: Automatically dropping into a post-mortem pdb debugger session whenever an exception is raised. Enables fast DEBUG logging to a logging file via QueueHandler. Copies all stdout output to the logging file. (Experimental) References \u00b6 Automatically start the debugger on an exception (Python recipe), Thomas Heller, 2001, Link Dealing with handlers that block, Python Documentation, 2019. Link Arguments \u00b6 log_dir (str, optional , Default: './') - Location to store the log files. Example \u00b6 ch . debug . debug () raise Exception ( 'My exception' ) -> raise ( 'My exception' ) ( Pdb )","title":"cherry.debug"},{"location":"api/cherry.debug/#cherrydebug","text":"","title":"cherry.debug"},{"location":"api/cherry.debug/#cherry.debug.debug","text":"[Source]","title":"debug()"},{"location":"api/cherry.distributions/","text":"cherry.distributions \u00b6 cherry.distributions.Categorical \u00b6 [Source] Description \u00b6 Similar to torch.nn.Categorical , but reshapes tensors of N samples into (N, 1)-shaped tensors. Arguments \u00b6 Identical to torch.distribution.Categorical . Example \u00b6 dist = Categorical ( logits = torch . randn ( bsz , action_size )) actions = dist . sample () # shape: bsz x 1 log_probs = dist . log_prob ( actions ) # shape: bsz x 1 deterministic_action = action . mode () mode ( self ) \u00b6 Description \u00b6 Returns the model of normal distribution (ie, argmax over probabilities). cherry.distributions.Normal \u00b6 [Source] Description \u00b6 Similar to PyTorch's Independent(Normal(loc, std)) : when computing log-densities or the entropy, we sum over the last dimension. This is typically used to compute log-probabilities of N-dimensional actions sampled from a multivariate Gaussian with diagional covariance. Arguments \u00b6 Identical to torch.distribution.Normal . Example \u00b6 normal = Normal ( torch . zeros ( bsz , action_size ), torch . ones ( bsz , action_size )) actions = normal . sample () log_probs = normal . log_prob ( actions ) # shape: bsz x 1 entropies = normal . entropy () # shape: bsz x 1 deterministic_action = action . mode () mode ( self ) \u00b6 Description \u00b6 Returns the model of normal distribution (ie, its mean). cherry.distributions.TanhNormal \u00b6 [Source] Description \u00b6 Implements a Normal distribution followed by a Tanh, often used with the Soft Actor-Critic. This implementation also exposes sample_and_log_prob and rsample_and_log_prob , which returns both samples and log-densities. The log-densities are computed using the pre-activation values for numerical stability. References \u00b6 Haarnoja et al. 2018. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv [cs.LG]. Haarnoja et al. 2018. \u201cSoft Actor-Critic Algorithms and Applications.\u201d arXiv [cs.LG]. Vitchyr Pong's RLkit . Example mean = th . zeros ( 5 ) std = th . ones ( 5 ) dist = TanhNormal ( mean , std ) samples = dist . rsample () logprobs = dist . log_prob ( samples ) # Numerically unstable :( samples , logprobs = dist . rsample_and_log_prob () # Stable :) __init__ ( self , normal_mean , normal_std ) special \u00b6 Arguments \u00b6 normal_mean (tensor) - Mean of the Normal distribution. normal_std (tensor) - Standard deviation of the Normal distribution. mean ( self ) \u00b6 Description \u00b6 Returns the mean of the TanhDistribution (ie, tan(normal.mean)). mode ( self ) \u00b6 Description \u00b6 Returns the mode of the TanhDistribution (ie, its mean). rsample_and_log_prob ( self ) \u00b6 Description \u00b6 Similar to sample_and_log_prob but with reparameterized samples. sample_and_log_prob ( self ) \u00b6 Description \u00b6 Samples from the TanhNormal and computes the log-density of the samples in a numerically stable way. Returns \u00b6 value (tensor) - samples from the TanhNormal. log_prob (tensor) - log-probabilities of the samples. Example \u00b6 tanh_normal = TanhNormal ( torch . zeros ( bsz , action_size ), torch . ones ( bsz , action_size )) actions , log_probs = tanh_normal . sample_and_log_prob () cherry.distributions.Reparameterization \u00b6 [Source] Description \u00b6 Unifies interface for distributions that support rsample and those that do not. When calling sample() , this class checks whether density has a rsample() member, and defaults to call sample() if it does not. References \u00b6 Kingma and Welling. 2013. \u201cAuto-Encoding Variational Bayes.\u201d arXiv [stat.ML]. Example \u00b6 density = Normal ( mean , std ) reparam = Reparameterization ( density ) sample = reparam . sample () # Uses Normal.rsample() __init__ ( self , density ) special \u00b6 Arguments \u00b6 density (Distribution) - The distribution to wrap. cherry.distributions.ActionDistribution \u00b6 [Source] Description \u00b6 A helper module to automatically choose the proper policy distribution, based on the Gym environment action_space . For Discrete action spaces, it uses a Categorical distribution, otherwise it uses a Normal which uses a diagonal covariance matrix. This class enables to write single version policy body that will be compatible with a variety of environments. Example \u00b6 env = gym . make ( 'CartPole-v1' ) action_dist = ActionDistribution ( env ) __init__ ( self , env , logstd = None , use_probs = False , reparam = False ) special \u00b6 Arguments \u00b6 env (Environment) - Gym environment for which actions will be sampled. logstd (float/tensor, optional , default=0) - The log standard deviation for the Normal distribution. use_probs (bool, optional , default=False) - Whether to use probabilities or logits for the Categorical case. reparam (bool, optional , default=False) - Whether to use reparameterization in the Normal case.","title":"cherry.distributions"},{"location":"api/cherry.distributions/#cherrydistributions","text":"","title":"cherry.distributions"},{"location":"api/cherry.distributions/#cherry.distributions.Categorical","text":"[Source]","title":"Categorical"},{"location":"api/cherry.distributions/#cherry.distributions.Categorical.mode","text":"","title":"mode()"},{"location":"api/cherry.distributions/#cherry.distributions.Normal","text":"[Source]","title":"Normal"},{"location":"api/cherry.distributions/#cherry.distributions.Normal.mode","text":"","title":"mode()"},{"location":"api/cherry.distributions/#cherry.distributions.TanhNormal","text":"[Source]","title":"TanhNormal"},{"location":"api/cherry.distributions/#cherry.distributions.TanhNormal.__init__","text":"","title":"__init__()"},{"location":"api/cherry.distributions/#cherry.distributions.TanhNormal.mean","text":"","title":"mean()"},{"location":"api/cherry.distributions/#cherry.distributions.TanhNormal.mode","text":"","title":"mode()"},{"location":"api/cherry.distributions/#cherry.distributions.TanhNormal.rsample_and_log_prob","text":"","title":"rsample_and_log_prob()"},{"location":"api/cherry.distributions/#cherry.distributions.TanhNormal.sample_and_log_prob","text":"","title":"sample_and_log_prob()"},{"location":"api/cherry.distributions/#cherry.distributions.Reparameterization","text":"[Source]","title":"Reparameterization"},{"location":"api/cherry.distributions/#cherry.distributions.Reparameterization.__init__","text":"","title":"__init__()"},{"location":"api/cherry.distributions/#cherry.distributions.ActionDistribution","text":"[Source]","title":"ActionDistribution"},{"location":"api/cherry.distributions/#cherry.distributions.ActionDistribution.__init__","text":"","title":"__init__()"},{"location":"api/cherry.envs/","text":"cherry.envs \u00b6 cherry.envs.utils \u00b6 cherry . envs . utils . get_space_dimension ( space , vectorized_dims = False ) \u00b6 [Source] Description \u00b6 Returns the number of elements of a space sample, when unrolled. Arguments \u00b6 space - The space. vectorized_dims - Whether to return the full dimension for vectorized environments (True) or just the dimension for the underlying environment (False). cherry . envs . utils . is_discrete ( space , vectorized = False ) \u00b6 [Source] Description \u00b6 Returns whether a space is discrete. Arguments \u00b6 space - The space. vectorized - Whether to return the discreteness for the vectorized environments (True) or just the discreteness of the underlying environment (False). cherry . envs . utils . is_vectorized ( env ) \u00b6 [Source] Description \u00b6 Heuristic that returns whether an environment is vectorized or not. Warning: functionality is experimental, mostly tested with gym.vector . cherry . envs . utils . num_envs ( env ) \u00b6 [Source] Description \u00b6 Heuristic that returns the number of parallel workers in a vectorized environment. Warning: functionality is experimental, mostly tested with gym.vector .","title":"cherry.envs"},{"location":"api/cherry.envs/#cherryenvs","text":"","title":"cherry.envs"},{"location":"api/cherry.envs/#cherryenvsutils","text":"","title":"cherry.envs.utils"},{"location":"api/cherry.envs/#cherry.envs.utils.get_space_dimension","text":"[Source]","title":"get_space_dimension()"},{"location":"api/cherry.envs/#cherry.envs.utils.is_discrete","text":"[Source]","title":"is_discrete()"},{"location":"api/cherry.envs/#cherry.envs.utils.is_vectorized","text":"[Source]","title":"is_vectorized()"},{"location":"api/cherry.envs/#cherry.envs.utils.num_envs","text":"[Source]","title":"num_envs()"},{"location":"api/cherry/","text":"cherry \u00b6 cherry.experience_replay.Transition \u00b6 [Source] Description \u00b6 Represents a (s, a, r, s', d) tuple. All attributes (including the ones in infos) are accessible via transition.name_of_attr . (e.g. transition.log_prob if log_prob is in infos .) Example \u00b6 for transition in replay : print ( transition . state ) __init__ ( self , state , action , reward , next_state , done , device = None , ** infos ) special \u00b6 Arguments \u00b6 state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. to ( self , * args , ** kwargs ) \u00b6 Description Moves the constituents of the transition to the desired device, and casts them to the desired format. Note: This is done in-place and doesn't create a new transition. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example sars = Transition ( state , action , reward , next_state ) sars . to ( 'cuda' ) cherry.experience_replay.ExperienceReplay \u00b6 [Source] Description \u00b6 Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. References \u00b6 Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example \u00b6 replay = ch . ExperienceReplay () # Instanciate a new replay replay . append ( state , # Add experience to the replay action , reward , next_state , done , density : action_density , log_prob : action_density . log_prob ( action ), ) replay . state () # Tensor of states replay . action () # Tensor of actions replay . density () # list of action_density replay . log_prob () # Tensor of log_probabilities new_replay = replay [ - 10 :] # Last 10 transitions in new_replay #Sample some previous experience batch = replay . sample ( 32 , contiguous = True ) __init__ ( self , storage = None , device = None , vectorized = False ) special \u00b6 Arguments \u00b6 storage (list, optional , default=None) - A list of Transitions. device (torch.device, optional , default=None) - The device of the replay. vectorized (bool, optional , default=False) - Whether the transitions are vectorized or not. append ( self , state = None , action = None , reward = None , next_state = None , done = None , ** infos ) \u00b6 Description \u00b6 Appends new data to the list ExperienceReplay. Arguments \u00b6 state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? infos` (dict, optional , default=None) - Additional information on the transition. Example \u00b6 replay . append ( state , action , reward , next_state , done , info = { 'density' : density , 'log_prob' : density . log_prob ( action ), }) empty ( self ) \u00b6 Description \u00b6 Removes all data from an ExperienceReplay. Example \u00b6 replay . empty () flatten ( self ) \u00b6 Description \u00b6 Returns a \"flattened\" version of the replay, where each transition only contains one timestep. Assuming the original replay has N transitions each with M timesteps -- i.e. sars.state with shapes (M, state_size) -- this method returns a new replay with N M transitions (and the states have shape (*state_size)). Note: This method breaks the timestep orders, so transitions are not consecutive anymore. Note: No-op if not vectorized. Example \u00b6 flat_replay = replay . flatten () load ( self , path ) \u00b6 Description \u00b6 Loads data from a serialized ExperienceReplay. Arguments \u00b6 path (str) - File path of serialized ExperienceReplay. Example \u00b6 replay . load ( 'my_replay_file.pt' ) sample ( self , size = 1 , contiguous = False , episodes = False , nsteps = 1 , discount = 1.0 ) \u00b6 Samples from the Experience replay. Arguments \u00b6 size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. nsteps (int, optional , default=1) - Steps to compute the n-steps returns. discount (float, optional , default=1.0) - Discount for n-steps returns. Returns \u00b6 ExperienceReplay - New ExperienceReplay containing the sampled transitions. save ( self , path ) \u00b6 Description \u00b6 Serializes and saves the ExperienceReplay into the given path. Arguments \u00b6 path (str) - File path. Example \u00b6 replay . save ( 'my_replay_file.pt' ) to ( self , * args , ** kwargs ) \u00b6 Description \u00b6 Calls .to() on all transitions of the experience replay, moving them to the desired device and casting the to the desired format. Note: This return a new experience replay, but the transitions are modified in-place. Arguments \u00b6 device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example \u00b6 replay . to ( 'cuda:1' ) policy . to ( 'cuda:1' ) for sars in replay : cuda_action = policy ( sars . state ) . sample () cherry . _torch . totensor ( array , dtype = None ) \u00b6 [Source] Description Converts the argument array to a torch.tensor 1xN, regardless of its type or dimension. Arguments array (int, float, ndarray, tensor) - Data to be converted to array. dtype (dtype, optional , default=None) - Data type to use for representation. By default, uses torch.get_default_dtype() . Returns Tensor of shape 1xN with the appropriate data type. Example array = [ 5 , 6 , 7.0 ] tensor = cherry . totensor ( array , dtype = th . float32 ) array = np . array ( array , dtype = np . float64 ) tensor = cherry . totensor ( array , dtype = th . float16 ) cherry . _torch . normalize ( tensor , epsilon = 1e-08 ) \u00b6 [Source] Description Normalizes a tensor to have zero mean and unit standard deviation values. Arguments tensor (tensor) - The tensor to normalize. epsilon (float, optional , default=1e-8) - Numerical stability constant for normalization. Returns A new tensor, containing the normalized values. Example tensor = torch . arange ( 23 ) / 255.0 tensor = cherry . normalize ( tensor , epsilon = 1e-3 )","title":"cherry"},{"location":"api/cherry/#cherry","text":"","title":"cherry"},{"location":"api/cherry/#cherry.experience_replay.Transition","text":"[Source]","title":"Transition"},{"location":"api/cherry/#cherry.experience_replay.Transition.__init__","text":"","title":"__init__()"},{"location":"api/cherry/#cherry.experience_replay.Transition.to","text":"Description Moves the constituents of the transition to the desired device, and casts them to the desired format. Note: This is done in-place and doesn't create a new transition. Arguments device (device, optional , default=None) - The device to move the data to. dtype (dtype, optional , default=None) - The torch.dtype format to cast to. non_blocking (bool, optional , default=False) - Whether to perform the move asynchronously. Example sars = Transition ( state , action , reward , next_state ) sars . to ( 'cuda' )","title":"to()"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay","text":"[Source]","title":"ExperienceReplay"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay.__init__","text":"","title":"__init__()"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay.append","text":"","title":"append()"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay.empty","text":"","title":"empty()"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay.flatten","text":"","title":"flatten()"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay.load","text":"","title":"load()"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay.sample","text":"Samples from the Experience replay.","title":"sample()"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay.save","text":"","title":"save()"},{"location":"api/cherry/#cherry.experience_replay.ExperienceReplay.to","text":"","title":"to()"},{"location":"api/cherry/#cherry._torch.totensor","text":"[Source] Description Converts the argument array to a torch.tensor 1xN, regardless of its type or dimension. Arguments array (int, float, ndarray, tensor) - Data to be converted to array. dtype (dtype, optional , default=None) - Data type to use for representation. By default, uses torch.get_default_dtype() . Returns Tensor of shape 1xN with the appropriate data type. Example array = [ 5 , 6 , 7.0 ] tensor = cherry . totensor ( array , dtype = th . float32 ) array = np . array ( array , dtype = np . float64 ) tensor = cherry . totensor ( array , dtype = th . float16 )","title":"totensor()"},{"location":"api/cherry/#cherry._torch.normalize","text":"[Source] Description Normalizes a tensor to have zero mean and unit standard deviation values. Arguments tensor (tensor) - The tensor to normalize. epsilon (float, optional , default=1e-8) - Numerical stability constant for normalization. Returns A new tensor, containing the normalized values. Example tensor = torch . arange ( 23 ) / 255.0 tensor = cherry . normalize ( tensor , epsilon = 1e-3 )","title":"normalize()"},{"location":"api/cherry.models/","text":"cherry.models \u00b6 cherry . models . utils . polyak_average ( source , target , alpha ) \u00b6 [Source] Description \u00b6 Shifts the parameters of source towards those of target. Note: the parameter alpha indicates the convex combination weight of the source. (i.e. the old parameters are kept at a rate of alpha .) References \u00b6 Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments \u00b6 source (Module) - The module to be shifted. target (Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example \u00b6 target_qf = nn . Linear ( 23 , 34 ) qf = nn . Linear ( 23 , 34 ) ch . models . polyak_average ( target_qf , qf , alpha = 0.9 ) cherry.models.utils.RandomPolicy \u00b6 [Source] Description \u00b6 Policy that randomly samples actions from the environment action space. Example \u00b6 policy = ch . models . RandomPolicy ( env ) env = envs . Runner ( env ) replay = env . run ( policy , steps = 2048 ) __init__ ( self , env , * args , ** kwargs ) special \u00b6 Arguments \u00b6 env (Environment) - Environment from which to sample actions. cherry.models.atari \u00b6 cherry.models.atari.NatureFeatures \u00b6 [Source] Description \u00b6 The convolutional body of the DQN architecture. References \u00b6 Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit \u00b6 Adapted from Ilya Kostrikov's implementation. __init__ ( self , input_size = 4 , output_size = 512 , hidden_size = 3136 ) special \u00b6 Arguments \u00b6 input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation. hidden_size (int, optional , default=1568) - Size of the representation after the convolutional layers forward ( self , input ) inherited \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. cherry.models.atari.NatureActor \u00b6 [Source] Description \u00b6 The actor head of the A3C architecture. References \u00b6 Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit \u00b6 Adapted from Ilya Kostrikov's implementation. Arguments \u00b6 input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space. __init__ ( self , input_size , output_size ) special \u00b6 forward ( self , input : Tensor ) -> Tensor inherited \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. cherry.models.atari.NatureCritic \u00b6 [Source] Description \u00b6 The critic head of the A3C architecture. References \u00b6 Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit \u00b6 Adapted from Ilya Kostrikov's implementation. __init__ ( self , input_size , output_size = 1 ) special \u00b6 Arguments \u00b6 input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value. forward ( self , input : Tensor ) -> Tensor inherited \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. cherry.models.robotics \u00b6 cherry.models.robotics.RoboticsMLP \u00b6 [Source] Description \u00b6 A multi-layer perceptron with proper initialization for robotic control. Credit \u00b6 Adapted from Ilya Kostrikov's implementation. Example \u00b6 target_qf = ch . models . robotics . RoboticsMLP ( 23 , 34 , layer_sizes = [ 32 , 32 ]) __init__ ( self , input_size , output_size , layer_sizes = None ) special \u00b6 Arguments \u00b6 inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) forward ( self , x ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. cherry.models.robotics.RoboticsActor \u00b6 [Source] Description \u00b6 A multi-layer perceptron with initialization designed for choosing actions in continuous robotic environments. Credit \u00b6 Adapted from Ilya Kostrikov's implementation. Example \u00b6 policy_mean = ch . models . robotics . Actor ( 28 , 8 , layer_sizes = [ 64 , 32 , 16 ]) __init__ ( self , input_size , output_size , layer_sizes = None ) special \u00b6 Arguments \u00b6 inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) forward ( self , x ) inherited \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. cherry.models.robotics.LinearValue \u00b6 [Source] Description \u00b6 A linear state-value function, whose parameters are found by minimizing least-squares. Credit \u00b6 Adapted from Tristan Deleu's implementation. References \u00b6 Duan et al. 2016. \u201cBenchmarking Deep Reinforcement Learning for Continuous Control.\u201d https://github.com/tristandeleu/pytorch-maml-rl Example \u00b6 states = replay . state () rewards = replay . reward () dones = replay . done () returns = ch . td . discount ( gamma , rewards , dones ) baseline = LinearValue ( input_size ) baseline . fit ( states , returns ) next_values = baseline ( replay . next_states ()) __init__ ( self , input_size , reg = 1e-05 ) special \u00b6 Arguments \u00b6 inputs_size (int) - Size of input. reg (float, optional , default=1e-5) - Regularization coefficient. fit ( self , states , returns ) \u00b6 Description \u00b6 Fits the parameters of the linear model by the method of least-squares. Arguments \u00b6 states (tensor) - States collected with the policy to evaluate. returns (tensor) - Returns associated with those states (ie, discounted rewards). forward ( self , state ) \u00b6 Description \u00b6 Computes the value of a state using the linear function approximator. Arguments \u00b6 state (Tensor) - The state to evaluate. cherry.models.tabular \u00b6 cherry.models.tabular.StateValueFunction \u00b6 [Source] Description \u00b6 Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. References \u00b6 Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example \u00b6 vf = StateValueFunction ( env . state_size ) state = env . reset () state = ch . onehot ( state , env . state_size ) state_value = vf ( state ) __init__ ( self , state_size , init = None ) special \u00b6 Arguments \u00b6 state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) forward ( self , state ) \u00b6 Description \u00b6 Returns the state value of a one-hot encoded state. Arguments \u00b6 state (Tensor) - State to be evaluated. cherry.models.tabular.ActionValueFunction \u00b6 [Source] Description \u00b6 Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. References \u00b6 Richard Sutton and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example \u00b6 qf = ActionValueFunction ( env . state_size , env . action_size ) state = env . reset () state = ch . onehot ( state , env . state_size ) all_action_values = qf ( state ) action = ch . onehot ( 0 , env . action_size ) action_value = qf ( state , action ) __init__ ( self , state_size , action_size , init = None ) special \u00b6 Arguments \u00b6 state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) forward ( self , state , action = None ) \u00b6 Description \u00b6 Returns the action value of a one-hot encoded state and one-hot encoded action. Arguments \u00b6 state (Tensor) - State to be evaluated. action (Tensor) - Action to be evaluated.","title":"cherry.models"},{"location":"api/cherry.models/#cherrymodels","text":"","title":"cherry.models"},{"location":"api/cherry.models/#cherry.models.utils.polyak_average","text":"[Source]","title":"polyak_average()"},{"location":"api/cherry.models/#cherry.models.utils.RandomPolicy","text":"[Source]","title":"RandomPolicy"},{"location":"api/cherry.models/#cherry.models.utils.RandomPolicy.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherrymodelsatari","text":"","title":"cherry.models.atari"},{"location":"api/cherry.models/#cherry.models.atari.NatureFeatures","text":"[Source]","title":"NatureFeatures"},{"location":"api/cherry.models/#cherry.models.atari.NatureFeatures.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherry.models.atari.NatureFeatures.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward()"},{"location":"api/cherry.models/#cherry.models.atari.NatureActor","text":"[Source]","title":"NatureActor"},{"location":"api/cherry.models/#cherry.models.atari.NatureActor.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherry.models.atari.NatureActor.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward()"},{"location":"api/cherry.models/#cherry.models.atari.NatureCritic","text":"[Source]","title":"NatureCritic"},{"location":"api/cherry.models/#cherry.models.atari.NatureCritic.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherry.models.atari.NatureCritic.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward()"},{"location":"api/cherry.models/#cherrymodelsrobotics","text":"","title":"cherry.models.robotics"},{"location":"api/cherry.models/#cherry.models.robotics.RoboticsMLP","text":"[Source]","title":"RoboticsMLP"},{"location":"api/cherry.models/#cherry.models.robotics.RoboticsMLP.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherry.models.robotics.RoboticsMLP.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward()"},{"location":"api/cherry.models/#cherry.models.robotics.RoboticsActor","text":"[Source]","title":"RoboticsActor"},{"location":"api/cherry.models/#cherry.models.robotics.RoboticsActor.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherry.models.robotics.RoboticsActor.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward()"},{"location":"api/cherry.models/#cherry.models.robotics.LinearValue","text":"[Source]","title":"LinearValue"},{"location":"api/cherry.models/#cherry.models.robotics.LinearValue.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherry.models.robotics.LinearValue.fit","text":"","title":"fit()"},{"location":"api/cherry.models/#cherry.models.robotics.LinearValue.forward","text":"","title":"forward()"},{"location":"api/cherry.models/#cherrymodelstabular","text":"","title":"cherry.models.tabular"},{"location":"api/cherry.models/#cherry.models.tabular.StateValueFunction","text":"[Source]","title":"StateValueFunction"},{"location":"api/cherry.models/#cherry.models.tabular.StateValueFunction.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherry.models.tabular.StateValueFunction.forward","text":"","title":"forward()"},{"location":"api/cherry.models/#cherry.models.tabular.ActionValueFunction","text":"[Source]","title":"ActionValueFunction"},{"location":"api/cherry.models/#cherry.models.tabular.ActionValueFunction.__init__","text":"","title":"__init__()"},{"location":"api/cherry.models/#cherry.models.tabular.ActionValueFunction.forward","text":"","title":"forward()"},{"location":"api/cherry.nn.init/","text":"cherry.nn.init \u00b6 cherry . nn . init . robotics_init_ ( module , gain = None ) \u00b6 [Source] Description \u00b6 Default initialization for robotic control. Credit \u00b6 Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments \u00b6 module (nn.Module) - Module to initialize. gain (float, optional , default=sqrt(2.0)) - Gain of orthogonal initialization. Returns \u00b6 Module, whose weight and bias have been modified in-place. Example \u00b6 linear = nn . Linear ( 23 , 5 ) kostrikov_robotics_ ( linear ) cherry . nn . init . atari_init_ ( module , gain = None ) \u00b6 [Source] Description \u00b6 Default initialization for Atari environments. Credit \u00b6 Adapted from Ilya Kostrikov's implementation, itself inspired from OpenAI Baslines. Arguments \u00b6 module (nn.Module) - Module to initialize. gain (float, optional , default=None) - Gain of orthogonal initialization. Default is computed for ReLU activation with torch.nn.init.calculate_gain('relu') . Returns \u00b6 Module, whose weight and bias have been modified in-place. Example \u00b6 linear = nn . Linear ( 23 , 5 ) atari_init_ ( linear )","title":"cherry.nn.init"},{"location":"api/cherry.nn.init/#cherrynninit","text":"","title":"cherry.nn.init"},{"location":"api/cherry.nn.init/#cherry.nn.init.robotics_init_","text":"[Source]","title":"robotics_init_()"},{"location":"api/cherry.nn.init/#cherry.nn.init.atari_init_","text":"[Source]","title":"atari_init_()"},{"location":"api/cherry.nn/","text":"cherry.nn \u00b6 cherry.nn.policy.Policy \u00b6 [Source] Abstract Module to represent policies. Subclassing this module helps retain a unified API across codebases, and also automatically defines some helper functions (you only need that forward returns a Distribution instance). Example \u00b6 class RandomPolicy ( Policy ): def __init__ ( self , num_actions = 5 ): self . num_actions = num_actions def forward ( self , state ): # must return a density probs = torch . ones ( self . num_actions ) / self . num_actions density = cherry . distributions . Categorical ( probs = probs ) return density # We can now use some predefined functions: random_policy = RandomPolicy () actions = random_policy . act ( states , deterministic = True ) log_probs = random_policy . log_probs ( states , actions ) __init__ ( self ) -> None inherited special \u00b6 act ( self , state , deterministic = False ) \u00b6 Description \u00b6 Given a state, samples an action from the policy. If deterministic=True , the action is the model of the policy distribution. Arguments \u00b6 state (Tensor) - State to take an action in. deterministic (bool, optional , default=False) - Where the action is sampled ( False ) or the mode of the policy ( True ). forward ( self , state ) \u00b6 Description \u00b6 Should return a Distribution instance correspoinding to the policy density for state . Arguments \u00b6 state (Tensor) - State where the policy should be computed. log_prob ( self , state , action ) \u00b6 Description \u00b6 Computes the log probability of action given state , according to the policy. Arguments \u00b6 state (Tensor) - A tensor of states. action (Tensor) - The actions of which to compute the log probability. cherry.nn.action_value.ActionValue \u00b6 [Source] Description \u00b6 Abstract Module to represent Q-value functions. Example \u00b6 class QValue ( ActionValue ): def __init__ ( self , state_size , action_size ): super ( QValue , self ) . __init__ () self . mlp = MLP ( state_size + action_size , 1 , [ 1024 , 1024 ]) def forward ( self , state , action ): return self . mlp ( torch . cat ([ state , action ], dim = 1 )) qf = QValue ( 128 , 5 ) qvalue = qf ( state , action ) forward ( self , state , action = None ) \u00b6 Description \u00b6 Returns the scalar value for taking action action in state state . If action is not given, should return the value for all actions (useful for DQN-like architectures). Arguments \u00b6 state (Tensor) - State to be evaluated. action (Tensor, optional , default=None) - Action to be evaluated. Returns \u00b6 value (Tensor) - Value of taking action in state . Shape: (batch_size, 1) cherry.nn.action_value.Twin \u00b6 [Source] Description \u00b6 Helper class to implement Twin action-value functions as described in [1]. References \u00b6 Fujimoto et al., \"Addressing Function Approximation Error in Actor-Critic Methods\". ICML 2018. Example \u00b6 qvalue = Twin ( QValue (), QValue ()) values = qvalue ( states , actions ) values1 , values1 = qvalue . twin ( states , actions ) __init__ ( self , * action_values ) special \u00b6 Arguments \u00b6 qvalue1, qvalue2, ... (ActionValue) - Action value functions. forward ( self , state , action ) \u00b6 Description \u00b6 Returns the minimum value computed by the individual value functions wrapped by this class. Arguments \u00b6 state (Tensor) - The state to evaluate. action (Tensor) - The action to evaluate. twin ( self , state , action ) \u00b6 Description \u00b6 Returns the values of each individual value function wrapped by this class. Arguments \u00b6 state (Tensor) - State to be evaluated. action (Tensor) - Action to be evaluated. cherry.nn.robotics_layers.RoboticsLinear \u00b6 [Source] Description \u00b6 Akin to nn.Linear , but with proper initialization for robotic control. Credit \u00b6 Adapted from Ilya Kostrikov's implementation. Example \u00b6 linear = ch . nn . Linear ( 23 , 5 , bias = True ) action_mean = linear ( state ) __init__ ( self , * args , ** kwargs ) special \u00b6 Arguments \u00b6 gain (float, optional ) - Gain factor passed to robotics_init_ initialization. This class extends nn.Linear and supports all of its arguments. cherry.nn.epsilon_greedy.EpsilonGreedy \u00b6 [Source] Description \u00b6 Samples actions from a uniform distribution with probability epsilon or the one maximizing the input with probability 1 - epsilon . References \u00b6 Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example \u00b6 egreedy = EpsilonGreedy () q_values = q_value ( state ) # NxM tensor actions = egreedy ( q_values ) # Nx1 tensor of longs __init__ ( self , epsilon = 0.05 , learnable = False ) special \u00b6 Arguments \u00b6 epsilon (float, optional , default=0.05) - The epsilon factor. learnable (bool, optional , default=False) - Whether the epsilon factor is a learnable parameter or not. cherry.nn.mlp.MLP \u00b6 [Source] Description \u00b6 Implements a simple multi-layer perceptron. Example \u00b6 net = MLP ( 128 , 1 , [ 1024 , 1024 ], activation = torch . nn . GELU ) __init__ ( self , input_size , output_size , hidden_sizes , activation = None , bias = True ) special \u00b6 Arguments \u00b6 input_size (int) - Input size of the MLP. output_size (int) - Number of output units. hidden_sizes (list of int) - Each int is the number of hidden units of a layer. activation (callable) - Activation function to use for the MLP. bias (bool, optional , default=True) - Whether the MLP uses bias terms. cherry.nn.misc.Lambda \u00b6 [Source] Description \u00b6 Turns any function into a PyTorch Module. Example \u00b6 double = Lambda ( lambda x : 2 * x ) out = double ( tensor ([ 23 ])) # out == 46 __init__ ( self , fn ) special \u00b6 Description \u00b6 fn (callable) - Function to turn into a Module.","title":"cherry.nn"},{"location":"api/cherry.nn/#cherrynn","text":"","title":"cherry.nn"},{"location":"api/cherry.nn/#cherry.nn.policy.Policy","text":"[Source] Abstract Module to represent policies. Subclassing this module helps retain a unified API across codebases, and also automatically defines some helper functions (you only need that forward returns a Distribution instance).","title":"Policy"},{"location":"api/cherry.nn/#cherry.nn.policy.Policy.__init__","text":"","title":"__init__()"},{"location":"api/cherry.nn/#cherry.nn.policy.Policy.act","text":"","title":"act()"},{"location":"api/cherry.nn/#cherry.nn.policy.Policy.forward","text":"","title":"forward()"},{"location":"api/cherry.nn/#cherry.nn.policy.Policy.log_prob","text":"","title":"log_prob()"},{"location":"api/cherry.nn/#cherry.nn.action_value.ActionValue","text":"[Source]","title":"ActionValue"},{"location":"api/cherry.nn/#cherry.nn.action_value.ActionValue.forward","text":"","title":"forward()"},{"location":"api/cherry.nn/#cherry.nn.action_value.Twin","text":"[Source]","title":"Twin"},{"location":"api/cherry.nn/#cherry.nn.action_value.Twin.__init__","text":"","title":"__init__()"},{"location":"api/cherry.nn/#cherry.nn.action_value.Twin.forward","text":"","title":"forward()"},{"location":"api/cherry.nn/#cherry.nn.action_value.Twin.twin","text":"","title":"twin()"},{"location":"api/cherry.nn/#cherry.nn.robotics_layers.RoboticsLinear","text":"[Source]","title":"RoboticsLinear"},{"location":"api/cherry.nn/#cherry.nn.robotics_layers.RoboticsLinear.__init__","text":"","title":"__init__()"},{"location":"api/cherry.nn/#cherry.nn.epsilon_greedy.EpsilonGreedy","text":"[Source]","title":"EpsilonGreedy"},{"location":"api/cherry.nn/#cherry.nn.epsilon_greedy.EpsilonGreedy.__init__","text":"","title":"__init__()"},{"location":"api/cherry.nn/#cherry.nn.mlp.MLP","text":"[Source]","title":"MLP"},{"location":"api/cherry.nn/#cherry.nn.mlp.MLP.__init__","text":"","title":"__init__()"},{"location":"api/cherry.nn/#cherry.nn.misc.Lambda","text":"[Source]","title":"Lambda"},{"location":"api/cherry.nn/#cherry.nn.misc.Lambda.__init__","text":"","title":"__init__()"},{"location":"api/cherry.optim/","text":"cherry.optim \u00b6 cherry.optim.Distributed \u00b6 [Source] Description \u00b6 Synchronizes the gradients of a model across replicas. At every step, Distributed averages the gradient across all replicas before calling the wrapped optimizer. The sync parameters determines how frequently the parameters are synchronized between replicas, to minimize numerical divergences. This is done by calling the sync_parameters() method. If sync is None , this never happens except upon initialization of the class. References \u00b6 Zinkevich et al. 2010. \u201cParallelized Stochastic Gradient Descent.\u201d Example \u00b6 opt = optim . Adam ( model . parameters ()) opt = Distributed ( model . parameters (), opt , sync = 1 ) opt . step () opt . sync_parameters () __init__ ( self , params , opt , sync = None ) special \u00b6 Arguments \u00b6 params (iterable) - Iterable of parameters. opt (Optimizer) - The optimizer to wrap and synchronize. sync (int, optional , default=None) - Parameter synchronization frequency. sync_parameters ( self , root = 0 ) \u00b6 Description \u00b6 Broadcasts all parameters of root to all other replicas. Arguments \u00b6 root (int, optional , default=0) - Rank of root replica.","title":"cherry.optim"},{"location":"api/cherry.optim/#cherryoptim","text":"","title":"cherry.optim"},{"location":"api/cherry.optim/#cherry.optim.Distributed","text":"[Source]","title":"Distributed"},{"location":"api/cherry.optim/#cherry.optim.Distributed.__init__","text":"","title":"__init__()"},{"location":"api/cherry.optim/#cherry.optim.Distributed.sync_parameters","text":"","title":"sync_parameters()"},{"location":"api/cherry.pg/","text":"cherry.pg \u00b6 cherry . pg . generalized_advantage ( gamma , tau , rewards , dones , values , next_value ) \u00b6 Description \u00b6 Computes the generalized advantage estimator. (GAE) References \u00b6 Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments \u00b6 gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns \u00b6 tensor - Tensor of advantages. Example \u00b6 mass , next_value = policy ( replay [ - 1 ] . next_state ) advantages = generalized_advantage ( 0.99 , 0.95 , replay . reward (), replay . value (), replay . done (), next_value )","title":"cherry.pg"},{"location":"api/cherry.pg/#cherrypg","text":"","title":"cherry.pg"},{"location":"api/cherry.pg/#cherry.pg.generalized_advantage","text":"","title":"generalized_advantage()"},{"location":"api/cherry.plot/","text":"cherry.plot \u00b6 cherry . plot . ci95 ( values ) \u00b6 [Source] Description \u00b6 Computes the 95% confidence interval around the given values. Arguments \u00b6 values (list) - List of values for which to compute the 95% confidence interval. Returns \u00b6 (float, float) The lower and upper bounds of the confidence interval. Example \u00b6 from statistics import mean smoothed = [] for replay in replays : rewards = replay . rewards . view ( - 1 ) . tolist () y_smoothed = ch . plot . smooth ( rewards ) smoothed . append ( y_smoothed ) means = [ mean ( r ) for r in zip ( * smoothed )] confidences = [ ch . plot . ci95 ( r ) for r in zip ( * smoothed )] lower_bound = [ conf [ 0 ] for conf in confidences ] upper_bound = [ conf [ 1 ] for conf in confidences ] cherry . plot . exponential_smoothing ( x , y = None , temperature = 1.0 ) \u00b6 [Source] Decription \u00b6 Two-sided exponential moving average for smoothing a curve. It performs regular exponential moving average twice from two different sides and then combines the results together. Credit \u00b6 Adapted from OpenAI's baselines implementation. Arguments \u00b6 x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return \u00b6 x_smoothed (ndarray) - x values after resampling. y_smoothed (ndarray) - y values after smoothing. Example \u00b6 from cherry.plot import exponential_smoothing x_smoothed , y_smoothed , _ = exponential_smoothing ( x_original , y_original , temperature = 3. )","title":"cherry.plot"},{"location":"api/cherry.plot/#cherryplot","text":"","title":"cherry.plot"},{"location":"api/cherry.plot/#cherry.plot.ci95","text":"[Source]","title":"ci95()"},{"location":"api/cherry.plot/#cherry.plot.exponential_smoothing","text":"[Source]","title":"exponential_smoothing()"},{"location":"api/cherry.td/","text":"cherry.td \u00b6 cherry . td . discount ( gamma , rewards , dones , bootstrap = 0.0 ) \u00b6 Description \u00b6 Discounts rewards at an rate of gamma. References \u00b6 Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments \u00b6 gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns \u00b6 tensor - Tensor of discounted rewards. Example \u00b6 rewards = th . ones ( 23 , 1 ) * 8 dones = th . zeros_like ( rewards ) dones [ - 1 ] += 1.0 discounted = ch . rl . discount ( 0.99 , rewards , dones , bootstrap = 1.0 ) cherry . td . temporal_difference ( gamma , rewards , dones , values , next_values ) \u00b6 Description \u00b6 Returns the temporal difference residual. Reference \u00b6 Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments \u00b6 gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_values (tensor) - Values of the state obtained after the transition from the state used to compute the last value in values . Example \u00b6 values = vf ( replay . states ()) next_values = vf ( replay . next_states ()) td_errors = temporal_difference ( 0.99 , replay . reward (), replay . done (), values , next_values )","title":"cherry.td"},{"location":"api/cherry.td/#cherrytd","text":"","title":"cherry.td"},{"location":"api/cherry.td/#cherry.td.discount","text":"","title":"discount()"},{"location":"api/cherry.td/#cherry.td.temporal_difference","text":"","title":"temporal_difference()"},{"location":"api/cherry.wrappers/","text":"cherry.wrappers \u00b6 cherry.wrappers.base_wrapper.Wrapper \u00b6 [Source] Description \u00b6 This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example \u00b6 env = gym . make ( 'MyEnv-v0' ) env = cherry . wrappers . Logger ( env ) env = cherry . wrappers . Runner ( env ) env . log ( 'asdf' , 23 ) # Uses log() method from cherry.wrappers.Logger. action_size property readonly \u00b6 Description \u00b6 The number of dimensions of a single action. discrete_action property readonly \u00b6 Description \u00b6 Returns whether the env is vectorized or not. discrete_state property readonly \u00b6 Description \u00b6 Returns whether the env is vectorized or not. is_vectorized property readonly \u00b6 Description \u00b6 Returns whether the env is vectorized or not. state_size property readonly \u00b6 Description \u00b6 The (flattened) size of a single state. cherry.wrappers.runner_wrapper.Runner \u00b6 [Source] Description \u00b6 Helps collect transitions, given a get_action function. Example \u00b6 env = MyEnv () env = Runner ( env ) replay = env . run ( lambda x : policy ( x ), steps = 100 ) # or replay = env . run ( lambda x : policy ( x ), episodes = 5 ) run ( self , get_action , steps = None , episodes = None , render = False ) \u00b6 Description \u00b6 Runner wrapper's run method. Info Either use the steps OR the episodes argument. Arguments \u00b6 get_action (function) - Given a state, returns the action to be taken. steps (int, optional , default=None) - The number of steps to be collected. episodes (int, optional , default=None) - The number of episodes to be collected. cherry.wrappers.torch_wrapper.Torch \u00b6 This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Examples: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action) __init__ ( self , env , device = None , env_device = None ) special \u00b6 cherry.wrappers.reward_clipper_wrapper.RewardClipper \u00b6 __init__ ( self , env ) special \u00b6 cherry.wrappers.timestep_wrapper.AddTimestep \u00b6 Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/ __init__ ( self , env = None ) special \u00b6 cherry.wrappers.action_space_scaler_wrapper.ActionSpaceScaler \u00b6 Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41 __init__ ( self , env , clip = 1.0 ) special \u00b6 Soon Deprecated \u00b6 Info The following wrappers will soon be deprecated because they are available in gym . cherry.wrappers.logger_wrapper.Logger \u00b6 Tracks and prints some common statistics about the environment. __init__ ( self , env , interval = 1000 , episode_interval = 10 , title = None , logger = None ) special \u00b6","title":"cherry.wrappers"},{"location":"api/cherry.wrappers/#cherrywrappers","text":"","title":"cherry.wrappers"},{"location":"api/cherry.wrappers/#cherry.wrappers.base_wrapper.Wrapper","text":"[Source]","title":"Wrapper"},{"location":"api/cherry.wrappers/#cherry.wrappers.base_wrapper.Wrapper.action_size","text":"","title":"action_size"},{"location":"api/cherry.wrappers/#cherry.wrappers.base_wrapper.Wrapper.discrete_action","text":"","title":"discrete_action"},{"location":"api/cherry.wrappers/#cherry.wrappers.base_wrapper.Wrapper.discrete_state","text":"","title":"discrete_state"},{"location":"api/cherry.wrappers/#cherry.wrappers.base_wrapper.Wrapper.is_vectorized","text":"","title":"is_vectorized"},{"location":"api/cherry.wrappers/#cherry.wrappers.base_wrapper.Wrapper.state_size","text":"","title":"state_size"},{"location":"api/cherry.wrappers/#cherry.wrappers.runner_wrapper.Runner","text":"[Source]","title":"Runner"},{"location":"api/cherry.wrappers/#cherry.wrappers.runner_wrapper.Runner.run","text":"","title":"run()"},{"location":"api/cherry.wrappers/#cherry.wrappers.torch_wrapper.Torch","text":"This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Examples: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action)","title":"Torch"},{"location":"api/cherry.wrappers/#cherry.wrappers.torch_wrapper.Torch.__init__","text":"","title":"__init__()"},{"location":"api/cherry.wrappers/#cherry.wrappers.reward_clipper_wrapper.RewardClipper","text":"","title":"RewardClipper"},{"location":"api/cherry.wrappers/#cherry.wrappers.reward_clipper_wrapper.RewardClipper.__init__","text":"","title":"__init__()"},{"location":"api/cherry.wrappers/#cherry.wrappers.timestep_wrapper.AddTimestep","text":"Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/","title":"AddTimestep"},{"location":"api/cherry.wrappers/#cherry.wrappers.timestep_wrapper.AddTimestep.__init__","text":"","title":"__init__()"},{"location":"api/cherry.wrappers/#cherry.wrappers.action_space_scaler_wrapper.ActionSpaceScaler","text":"Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"ActionSpaceScaler"},{"location":"api/cherry.wrappers/#cherry.wrappers.action_space_scaler_wrapper.ActionSpaceScaler.__init__","text":"","title":"__init__()"},{"location":"api/cherry.wrappers/#soon-deprecated","text":"Info The following wrappers will soon be deprecated because they are available in gym .","title":"Soon Deprecated"},{"location":"api/cherry.wrappers/#cherry.wrappers.logger_wrapper.Logger","text":"Tracks and prints some common statistics about the environment.","title":"Logger"},{"location":"api/cherry.wrappers/#cherry.wrappers.logger_wrapper.Logger.__init__","text":"","title":"__init__()"},{"location":"tutorials/debugging_rl/","text":"Debugging Reinforcement Learning \u00b6 Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper, Explain how to use the debug mode, (post-mortem, logger, warning from functions) Maybe showcase all of the above with an example that seems right but doesn't work, Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/debugging_rl/#debugging-reinforcement-learning","text":"Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper, Explain how to use the debug mode, (post-mortem, logger, warning from functions) Maybe showcase all of the above with an example that seems right but doesn't work, Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/distributed_ppo/","text":"Distributed Training with PPO \u00b6 Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/distributed_ppo/#distributed-training-with-ppo","text":"Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/getting_started/","text":"Getting Started with Cherry \u00b6 This document provides an overview of the philosophy behind cherry, the tools it provides, and a small illustrative example. By the end of this tutorial, you should be well-equiped to incorporate cherry in your research workflow. We assume that you are already familiar with reinforcement learning. If, instead, you're looking for an introduction to the field we recommend looking at Josh Achiam's Spinning Up in Deep RL . Installation \u00b6 The first step in getting started with cherry is to install it. You can easily do that by typing the following command in your favorite shell. pip install cherry-rl By default cherry only has two dependencies: torch and gym . However, more dependencies might be required if you plan to use some specific functionalities. For example, the OpenAIAtari wrapper requires OpenCV ( pip install opencv-python ) and the VisdomLogger requires visdom ( pip install visdom ). Note While cherry depends on Gym for its environment wrappers, it doesn't restrict you to Gym environments. For instance, check the examples using simple_rl and pycolab environments for Gym-free usage of cherry. Overview \u00b6 Why do we need cherry ? There are many reinforcement learning libraries, many of which feature high-quality implementations. However, few of them provide the kind of low-level utilities useful to researchers. Cherry aims to alleviate this issue. Instead of an interface akin to PPO(env_name).train(1000) , it provides researchers with a set of tools they can use to write readable, replicable, and flexible implementations. Cherry prioritizes time-to-correct-implementation over time-to-run, by explicitely helping you check, debug, and reliably report your results. How to use cherry ? Our goal is to make cherry a natural extension to PyTorch, with reinforcement learning in mind. To this end, we closely follow the package structure of PyTorch while providing additional utilities where we see fit. So if your goal is to implement a novel distributed off-policy policy gradient algorithm, you can count on cherry to provide you experience replays, policy gradient losses, discounting/advantage functions, and distributed optimizers. Those functions not only reduce the time spent writing code, they also check that your implementation is sane. (e.g. do the log probabilities and rewards have identical shapes?) Moreover, cherry provides the implementation details necessary to make deep reinforcement learning work. (e.g. initializations, modules, and wrappers commonly used in robotic or Atari benchmarks.) Importantly, it includes built-in debugging functionalities: cherry can help you visualize what is happening under the hood of your algorithm to help you find bugs faster. What's the future of cherry ? Reinforcement learning is a fast moving field, and it is difficult to predict which advances are safe bets for the future. Our long-term development strategy can be summarized as follows. Have as many recent and high-quality examples as possible. Merge advances that turn up to be fundamental in theory or practice into the core library. We hope to combat the reproducibility crisis by extensively testing and benchmarking our implementations. Note Cherry is in its early days and is still missing some of the well-established methods from the past 60 years. Those ones are being implemented as fast as we can :) Core Features \u00b6 The following features are fundamental components of cherry. Transitions and Experience Replay \u00b6 A majority of algorithms needs to store, retrieve, and sample past experience. To that end, you can use cherry's ExperienceReplay . An experience replay is implemented as a wrapper around a standard Python list. The major difference is that the append() method expects arguments used to create a Transition . In addition to behaving like a list, it exposes methods that act on this list, such as to(device) (moves the replay to a device), sample() (randomly samples some experience), or load() / save() (for convenient serialization). An ExperienceReplay contains Transition s, which are akin to ( state , action , reward , next_state , done ) named tuples with possibly additional custom fields. Those fields are easily accessible directly from the replay by accessing the method named after them. For example, calling replay.action() will fetch the action field from every transition stored in replay , stack them along the first dimension, and return that large tensor. The same is true for custom fields; if all transitions have a logprob field, replay.logprob() will return the result of stacking them. Temporal Difference and Policy Gradients \u00b6 Many low-level utilities used to implement temporal difference and policy gradient algorithms are available in the cherry.td and cherry.pg modules, respectively. Those modules include classical methods such as discounting rewards or computing the temporal difference , as well as more recent advances such as the generalized advantage estimator . We tried our best to avoid philosophical dissonance when a method belonged to both families of algorithms. Models and PyTorch \u00b6 Similar to PyTorch, we provide differentiable modules in cherry.nn , domain-specific initialization schemes in cherry.nn.init , and optimization utilities in cherry.optim . In addition, popular higher-level models are available in cherry.models ; for instance, those include tabular modules , the Atari CNN features extractor , and a Multi-Layer Perceptron for continuous control. Gym Wrappers \u00b6 Given the popularity of OpenAI Gym environment in modern reinforcement learning benchmarks, cherry includes convenient wrappers in the cherry.envs package. Examples include normalization of states and actions , Atari frames pre-processing, customizable state / action processing, and automatic collection of experience in a replay. Plots \u00b6 Reporting comparable results has become a central problem in modern reinforcement learning. In order to alleviate this issue, cherry provides utilities to smooth and compute confidence intervals over lists of rewards. Those are available in the cherry.plot submodule. Implementing Policy Gradient \u00b6 As an introductory example let us dissect the following snippet, which demonstrates how to implement the policy gradient theorem using cherry. import cherry as ch env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) env = ch . envs . Runner ( env ) env . seed ( 42 ) policy = PolicyNet () optimizer = optim . Adam ( policy . parameters (), lr = 1e-2 ) action_dist = ch . distributions . ActionDistribution ( env ) def get_action ( state ): mass = action_dist ( policy ( state )) action = mass . sample () log_prob = mass . log_prob ( action ) return action , { 'log_prob' : log_prob } for step in range ( 1000 ): replay = env . run ( get_action , episodes = 1 ) rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () After importing cherry, the first step is to instanciate, wrap, and seed the desired gym environment. env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) env = ch . envs . Runner ( env ) env . seed ( 42 ) The Logger , Torch , and Runner classes are Gym environment wrappers that systematically modify the behaviour of an environment: Logger keeps track of metrics and prints them at a given interval. Torch converts Gym states into PyTorch tensors, and action tensors into numpy arrays. Runner implements a run() method which allows to easily gather transitions for a number of steps or episodes. One particularity of wrappers is that they automatically expose methods of the wrapped environment: env.seed(42) calls the seed() method from the CartPole-v0 environment. Second, we instanciate the policy, optimizer, as well as the action distribution. The action distribution is created with action_dist = ch . distributions . ActionDistribution ( env ) which will automatically choose a diagonal Gaussian for continuous action-spaces and a categorical distribution for discrete ones. Next, we define get_action() which specifies how to get an action from our agent and will be used in conjuction to env.run() to quickly collect experience data: replay = env . run ( get_action , episodes = 1 ) env.run() assumes that the first returned value by get_action is the action to be passed to the environment and the second, optional, returned value is a dictionary to be saved into the experience replay. Under the hood, env.run() creates a new ExperienceReplay and fills it with the desired number of transitions; instead of episodes=1 we could have passed steps=100 . Finally, we discount and normalize the rewards and take an optimization step on the policy gradient loss. rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () When calling replay.reward() , replay.done() , or replay.log_prob() , the experience replay will concatenate the corresponding attribute across all of its transitions and return a new tensor . This means that this operation is rather expensive (you should cache it when possible) and that modifying this tensor does not modify the corresponding transitions in replay . Note that in this case log_prob is a custom attribute which is not declared in the original implementation of ExperienceReplay , and we could have given it any name by changing the dictionary key in get_action() . Conclusion \u00b6 You should now be able to use cherry in your own work. For more information, have a look at the documentation , the other tutorials , or the numerous examples . Since one of the characteristics of cherry is to avoid providing \"pre-baked\" algorithms, we tried our best to heavily document its usage.","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#getting-started-with-cherry","text":"This document provides an overview of the philosophy behind cherry, the tools it provides, and a small illustrative example. By the end of this tutorial, you should be well-equiped to incorporate cherry in your research workflow. We assume that you are already familiar with reinforcement learning. If, instead, you're looking for an introduction to the field we recommend looking at Josh Achiam's Spinning Up in Deep RL .","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#installation","text":"The first step in getting started with cherry is to install it. You can easily do that by typing the following command in your favorite shell. pip install cherry-rl By default cherry only has two dependencies: torch and gym . However, more dependencies might be required if you plan to use some specific functionalities. For example, the OpenAIAtari wrapper requires OpenCV ( pip install opencv-python ) and the VisdomLogger requires visdom ( pip install visdom ). Note While cherry depends on Gym for its environment wrappers, it doesn't restrict you to Gym environments. For instance, check the examples using simple_rl and pycolab environments for Gym-free usage of cherry.","title":"Installation"},{"location":"tutorials/getting_started/#overview","text":"Why do we need cherry ? There are many reinforcement learning libraries, many of which feature high-quality implementations. However, few of them provide the kind of low-level utilities useful to researchers. Cherry aims to alleviate this issue. Instead of an interface akin to PPO(env_name).train(1000) , it provides researchers with a set of tools they can use to write readable, replicable, and flexible implementations. Cherry prioritizes time-to-correct-implementation over time-to-run, by explicitely helping you check, debug, and reliably report your results. How to use cherry ? Our goal is to make cherry a natural extension to PyTorch, with reinforcement learning in mind. To this end, we closely follow the package structure of PyTorch while providing additional utilities where we see fit. So if your goal is to implement a novel distributed off-policy policy gradient algorithm, you can count on cherry to provide you experience replays, policy gradient losses, discounting/advantage functions, and distributed optimizers. Those functions not only reduce the time spent writing code, they also check that your implementation is sane. (e.g. do the log probabilities and rewards have identical shapes?) Moreover, cherry provides the implementation details necessary to make deep reinforcement learning work. (e.g. initializations, modules, and wrappers commonly used in robotic or Atari benchmarks.) Importantly, it includes built-in debugging functionalities: cherry can help you visualize what is happening under the hood of your algorithm to help you find bugs faster. What's the future of cherry ? Reinforcement learning is a fast moving field, and it is difficult to predict which advances are safe bets for the future. Our long-term development strategy can be summarized as follows. Have as many recent and high-quality examples as possible. Merge advances that turn up to be fundamental in theory or practice into the core library. We hope to combat the reproducibility crisis by extensively testing and benchmarking our implementations. Note Cherry is in its early days and is still missing some of the well-established methods from the past 60 years. Those ones are being implemented as fast as we can :)","title":"Overview"},{"location":"tutorials/getting_started/#core-features","text":"The following features are fundamental components of cherry.","title":"Core Features"},{"location":"tutorials/getting_started/#transitions-and-experience-replay","text":"A majority of algorithms needs to store, retrieve, and sample past experience. To that end, you can use cherry's ExperienceReplay . An experience replay is implemented as a wrapper around a standard Python list. The major difference is that the append() method expects arguments used to create a Transition . In addition to behaving like a list, it exposes methods that act on this list, such as to(device) (moves the replay to a device), sample() (randomly samples some experience), or load() / save() (for convenient serialization). An ExperienceReplay contains Transition s, which are akin to ( state , action , reward , next_state , done ) named tuples with possibly additional custom fields. Those fields are easily accessible directly from the replay by accessing the method named after them. For example, calling replay.action() will fetch the action field from every transition stored in replay , stack them along the first dimension, and return that large tensor. The same is true for custom fields; if all transitions have a logprob field, replay.logprob() will return the result of stacking them.","title":"Transitions and Experience Replay"},{"location":"tutorials/getting_started/#temporal-difference-and-policy-gradients","text":"Many low-level utilities used to implement temporal difference and policy gradient algorithms are available in the cherry.td and cherry.pg modules, respectively. Those modules include classical methods such as discounting rewards or computing the temporal difference , as well as more recent advances such as the generalized advantage estimator . We tried our best to avoid philosophical dissonance when a method belonged to both families of algorithms.","title":"Temporal Difference and Policy Gradients"},{"location":"tutorials/getting_started/#models-and-pytorch","text":"Similar to PyTorch, we provide differentiable modules in cherry.nn , domain-specific initialization schemes in cherry.nn.init , and optimization utilities in cherry.optim . In addition, popular higher-level models are available in cherry.models ; for instance, those include tabular modules , the Atari CNN features extractor , and a Multi-Layer Perceptron for continuous control.","title":"Models and PyTorch"},{"location":"tutorials/getting_started/#gym-wrappers","text":"Given the popularity of OpenAI Gym environment in modern reinforcement learning benchmarks, cherry includes convenient wrappers in the cherry.envs package. Examples include normalization of states and actions , Atari frames pre-processing, customizable state / action processing, and automatic collection of experience in a replay.","title":"Gym Wrappers"},{"location":"tutorials/getting_started/#plots","text":"Reporting comparable results has become a central problem in modern reinforcement learning. In order to alleviate this issue, cherry provides utilities to smooth and compute confidence intervals over lists of rewards. Those are available in the cherry.plot submodule.","title":"Plots"},{"location":"tutorials/getting_started/#implementing-policy-gradient","text":"As an introductory example let us dissect the following snippet, which demonstrates how to implement the policy gradient theorem using cherry. import cherry as ch env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) env = ch . envs . Runner ( env ) env . seed ( 42 ) policy = PolicyNet () optimizer = optim . Adam ( policy . parameters (), lr = 1e-2 ) action_dist = ch . distributions . ActionDistribution ( env ) def get_action ( state ): mass = action_dist ( policy ( state )) action = mass . sample () log_prob = mass . log_prob ( action ) return action , { 'log_prob' : log_prob } for step in range ( 1000 ): replay = env . run ( get_action , episodes = 1 ) rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () After importing cherry, the first step is to instanciate, wrap, and seed the desired gym environment. env = gym . make ( 'CartPole-v0' ) env = ch . envs . Logger ( env , interval = 1000 ) env = ch . envs . Torch ( env ) env = ch . envs . Runner ( env ) env . seed ( 42 ) The Logger , Torch , and Runner classes are Gym environment wrappers that systematically modify the behaviour of an environment: Logger keeps track of metrics and prints them at a given interval. Torch converts Gym states into PyTorch tensors, and action tensors into numpy arrays. Runner implements a run() method which allows to easily gather transitions for a number of steps or episodes. One particularity of wrappers is that they automatically expose methods of the wrapped environment: env.seed(42) calls the seed() method from the CartPole-v0 environment. Second, we instanciate the policy, optimizer, as well as the action distribution. The action distribution is created with action_dist = ch . distributions . ActionDistribution ( env ) which will automatically choose a diagonal Gaussian for continuous action-spaces and a categorical distribution for discrete ones. Next, we define get_action() which specifies how to get an action from our agent and will be used in conjuction to env.run() to quickly collect experience data: replay = env . run ( get_action , episodes = 1 ) env.run() assumes that the first returned value by get_action is the action to be passed to the environment and the second, optional, returned value is a dictionary to be saved into the experience replay. Under the hood, env.run() creates a new ExperienceReplay and fills it with the desired number of transitions; instead of episodes=1 we could have passed steps=100 . Finally, we discount and normalize the rewards and take an optimization step on the policy gradient loss. rewards = ch . td . discount ( 0.99 , replay . reward (), replay . done ()) rewards = ch . normalize ( rewards ) loss = - th . sum ( replay . log_prob () * rewards ) optimizer . zero_grad () loss . backward () optimizer . step () When calling replay.reward() , replay.done() , or replay.log_prob() , the experience replay will concatenate the corresponding attribute across all of its transitions and return a new tensor . This means that this operation is rather expensive (you should cache it when possible) and that modifying this tensor does not modify the corresponding transitions in replay . Note that in this case log_prob is a custom attribute which is not declared in the original implementation of ExperienceReplay , and we could have given it any name by changing the dictionary key in get_action() .","title":"Implementing Policy Gradient"},{"location":"tutorials/getting_started/#conclusion","text":"You should now be able to use cherry in your own work. For more information, have a look at the documentation , the other tutorials , or the numerous examples . Since one of the characteristics of cherry is to avoid providing \"pre-baked\" algorithms, we tried our best to heavily document its usage.","title":"Conclusion"},{"location":"tutorials/recurrent_a2c/","text":"Recurrent Policy Gradients with A2C \u00b6 Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"},{"location":"tutorials/recurrent_a2c/#recurrent-policy-gradients-with-a2c","text":"Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"}]}